{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Food Serving Sizes Through Nutrition Profiles\n",
    "\n",
    "### Notebook 3 - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Importing Packages](#Importing-Packages)\n",
    "2. [Reading Data](#Reading-Data)\n",
    "3. [Feature Engineering](#Feature-Engineering)\n",
    "4. [Preprocessing](#Preprocessing)\n",
    "5. [Modeling](#Modeling)\n",
    "    1. [Baseline Model](#Baseline-Model)\n",
    "    2. [Linear Regression](#Linear-Regression)\n",
    "    3. [Ridge Regression](#Ridge-Regression)\n",
    "    4. [LASSO Regression](#LASSO-Regression)\n",
    "    5. [Neural Network](#Neural-Network)\n",
    "6. [Model Evaluations](#Model-Evaluations)\n",
    "    1. [R<sup>2</sup> Score](#R2-Score)\n",
    "    2. [Mean Squared Error](#Mean-Squared-Error)\n",
    "    3. [Visualizations](#Visualizations)\n",
    "7. [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "    1. [Conclusions](#Conclusions)\n",
    "    2. [Recommendations](#Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# general tools/visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import set_random_seed\n",
    "import pickle\n",
    "\n",
    "# imports for NLP\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# imports for modeling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "# making the magic happen for plots\n",
    "%matplotlib inline\n",
    "\n",
    "# setting options for better viewing\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "# setting global random seeds for numpy and tensorflow\n",
    "np.random.seed(42)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.read_csv(\"../datasets/clean_foods_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>brand_owner</th>\n",
       "      <th>branded_food_category</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>energy</th>\n",
       "      <th>fat_total</th>\n",
       "      <th>fat_sat</th>\n",
       "      <th>fat_trans</th>\n",
       "      <th>chol</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fiber</th>\n",
       "      <th>sugars</th>\n",
       "      <th>sodium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356425</td>\n",
       "      <td>G. T. Japan, Inc.</td>\n",
       "      <td>Ice Cream &amp; Frozen Yogurt</td>\n",
       "      <td>MOCHI ICE CREAM BONBONS</td>\n",
       "      <td>ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1 PIECE</td>\n",
       "      <td>200.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356426</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>CHIPOTLE BARBECUE SAUCE</td>\n",
       "      <td>WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356427</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>HOT &amp; SPICY BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.29</td>\n",
       "      <td>676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356428</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.57</td>\n",
       "      <td>971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>356429</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>757.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fdc_id        brand_owner                 branded_food_category  \\\n",
       "0  356425  G. T. Japan, Inc.             Ice Cream & Frozen Yogurt   \n",
       "1  356426       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "2  356427       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "3  356428       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "4  356429       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "\n",
       "                  description  \\\n",
       "0     MOCHI ICE CREAM BONBONS   \n",
       "1     CHIPOTLE BARBECUE SAUCE   \n",
       "2  HOT & SPICY BARBECUE SAUCE   \n",
       "3              BARBECUE SAUCE   \n",
       "4              BARBECUE SAUCE   \n",
       "\n",
       "                                         ingredients  serving_size  \\\n",
       "0  ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...          40.0   \n",
       "1  WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...          37.0   \n",
       "2  SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...          34.0   \n",
       "3  TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...          35.0   \n",
       "4  SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...          37.0   \n",
       "\n",
       "  household_serving_fulltext  energy  fat_total  fat_sat  fat_trans  chol  \\\n",
       "0                    1 PIECE   200.0       6.25     3.75        0.0  25.0   \n",
       "1                     2 Tbsp   162.0       0.00     0.00        0.0   0.0   \n",
       "2                     2 Tbsp   176.0       0.00     0.00        0.0   0.0   \n",
       "3                     2 Tbsp   143.0       0.00     0.00        0.0   0.0   \n",
       "4                     2 Tbsp   189.0       0.00     0.00        0.0   0.0   \n",
       "\n",
       "   protein  carbs  fiber  sugars  sodium  \n",
       "0      2.5  35.00    0.0   30.00    75.0  \n",
       "1      0.0  43.24    0.0   37.84   703.0  \n",
       "2      0.0  41.18    0.0   35.29   676.0  \n",
       "3      0.0  34.29    0.0   28.57   971.0  \n",
       "4      0.0  45.95    0.0   43.24   757.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the modeling process begins, we need to fully prepare the dataset. In the previous notebook we had seen the tailed distribution of the serving size target, so we will make a new column here to use as the target for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods[\"log_serv\"] = np.log(foods[\"serving_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use the category of the food product as a feature, we will have to turn them into dummy columns first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.get_dummies(data=foods,\n",
    "                       columns=[\"branded_food_category\"],\n",
    "                       prefix=\"cat\",\n",
    "                       drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>brand_owner</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>energy</th>\n",
       "      <th>fat_total</th>\n",
       "      <th>fat_sat</th>\n",
       "      <th>fat_trans</th>\n",
       "      <th>chol</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fiber</th>\n",
       "      <th>sugars</th>\n",
       "      <th>sodium</th>\n",
       "      <th>log_serv</th>\n",
       "      <th>cat_All Noodles</th>\n",
       "      <th>cat_Bacon, Sausages &amp; Ribs</th>\n",
       "      <th>cat_Baking</th>\n",
       "      <th>cat_Baking Accessories</th>\n",
       "      <th>cat_Baking Additives &amp; Extracts</th>\n",
       "      <th>cat_Baking Decorations &amp; Dessert Toppings</th>\n",
       "      <th>cat_Baking/Cooking Mixes (Perishable)</th>\n",
       "      <th>cat_Baking/Cooking Mixes (Shelf Stable)</th>\n",
       "      <th>cat_Baking/Cooking Mixes/Supplies Variety Packs</th>\n",
       "      <th>cat_Baking/Cooking Supplies (Shelf Stable)</th>\n",
       "      <th>cat_Beef - Prepared/Processed</th>\n",
       "      <th>cat_Biscuits/Cookies (Shelf Stable)</th>\n",
       "      <th>cat_Bread &amp; Muffin Mixes</th>\n",
       "      <th>cat_Breads &amp; Buns</th>\n",
       "      <th>cat_Breakfast Drinks</th>\n",
       "      <th>cat_Breakfast Foods</th>\n",
       "      <th>cat_Breakfast Sandwiches, Biscuits &amp; Meals</th>\n",
       "      <th>cat_Butter &amp; Spread</th>\n",
       "      <th>cat_Cake, Cookie &amp; Cupcake Mixes</th>\n",
       "      <th>cat_Cakes - Sweet (Frozen)</th>\n",
       "      <th>cat_Cakes - Sweet (Shelf Stable)</th>\n",
       "      <th>cat_Cakes, Cupcakes, Snack Cakes</th>\n",
       "      <th>cat_Candy</th>\n",
       "      <th>cat_Canned &amp; Bottled Beans</th>\n",
       "      <th>cat_Canned Condensed Soup</th>\n",
       "      <th>cat_Canned Fruit</th>\n",
       "      <th>cat_Canned Meat</th>\n",
       "      <th>cat_Canned Seafood</th>\n",
       "      <th>cat_Canned Soup</th>\n",
       "      <th>cat_Canned Tuna</th>\n",
       "      <th>cat_Canned Vegetables</th>\n",
       "      <th>cat_Cereal</th>\n",
       "      <th>cat_Cereal/Muesli Bars</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_Pancakes, Waffles, French Toast &amp; Crepes</th>\n",
       "      <th>cat_Pasta Dinners</th>\n",
       "      <th>cat_Pasta by Shape &amp; Type</th>\n",
       "      <th>cat_Pasta/Noodles - Not Ready to Eat (Frozen)</th>\n",
       "      <th>cat_Pastry Shells &amp; Fillings</th>\n",
       "      <th>cat_Pepperoni, Salami &amp; Cold Cuts</th>\n",
       "      <th>cat_Pickles, Olives, Peppers &amp; Relishes</th>\n",
       "      <th>cat_Pies/Pastries - Sweet (Shelf Stable)</th>\n",
       "      <th>cat_Pies/Pastries/Pizzas/Quiches - Savoury (Frozen)</th>\n",
       "      <th>cat_Pizza</th>\n",
       "      <th>cat_Pizza Mixes &amp; Other Dry Dinners</th>\n",
       "      <th>cat_Plant Based Milk</th>\n",
       "      <th>cat_Plant Based Water</th>\n",
       "      <th>cat_Popcorn (Shelf Stable)</th>\n",
       "      <th>cat_Popcorn, Peanuts, Seeds &amp; Related Snacks</th>\n",
       "      <th>cat_Pork Sausages - Prepared/Processed</th>\n",
       "      <th>cat_Poultry, Chicken &amp; Turkey</th>\n",
       "      <th>cat_Powdered Drinks</th>\n",
       "      <th>cat_Pre-Packaged Fruit &amp; Vegetables</th>\n",
       "      <th>cat_Prepared Pasta &amp; Pizza Sauces</th>\n",
       "      <th>cat_Prepared Subs &amp; Sandwiches</th>\n",
       "      <th>cat_Prepared Wraps and Burittos</th>\n",
       "      <th>cat_Processed Cheese &amp; Cheese Novelties</th>\n",
       "      <th>cat_Puddings &amp; Custards</th>\n",
       "      <th>cat_Rice</th>\n",
       "      <th>cat_Salad Dressing &amp; Mayonnaise</th>\n",
       "      <th>cat_Sausages, Hotdogs &amp; Brats</th>\n",
       "      <th>cat_Seasoning Mixes, Salts, Marinades &amp; Tenderizers</th>\n",
       "      <th>cat_Snack, Energy &amp; Granola Bars</th>\n",
       "      <th>cat_Soda</th>\n",
       "      <th>cat_Soups - Prepared (Shelf Stable)</th>\n",
       "      <th>cat_Specialty Formula Supplements</th>\n",
       "      <th>cat_Sport Drinks</th>\n",
       "      <th>cat_Stuffing</th>\n",
       "      <th>cat_Sushi</th>\n",
       "      <th>cat_Syrups &amp; Molasses</th>\n",
       "      <th>cat_Tea Bags</th>\n",
       "      <th>cat_Tomatoes</th>\n",
       "      <th>cat_Vegetable &amp; Cooking Oils</th>\n",
       "      <th>cat_Vegetable Based Products / Meals - Not Ready to Eat (Frozen)</th>\n",
       "      <th>cat_Vegetable and Lentil Mixes</th>\n",
       "      <th>cat_Vegetables - Prepared/Processed (Frozen)</th>\n",
       "      <th>cat_Vegetables - Prepared/Processed (Shelf Stable)</th>\n",
       "      <th>cat_Vegetarian Frozen Meats</th>\n",
       "      <th>cat_Vitamins</th>\n",
       "      <th>cat_Water</th>\n",
       "      <th>cat_Weight Control</th>\n",
       "      <th>cat_Wholesome Snacks</th>\n",
       "      <th>cat_Yogurt</th>\n",
       "      <th>cat_Yogurt/Yogurt Substitutes (Perishable)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356425</td>\n",
       "      <td>G. T. Japan, Inc.</td>\n",
       "      <td>MOCHI ICE CREAM BONBONS</td>\n",
       "      <td>ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1 PIECE</td>\n",
       "      <td>200.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356426</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>CHIPOTLE BARBECUE SAUCE</td>\n",
       "      <td>WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>703.0</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356427</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>HOT &amp; SPICY BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.29</td>\n",
       "      <td>676.0</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356428</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.57</td>\n",
       "      <td>971.0</td>\n",
       "      <td>3.555348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>356429</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>757.0</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fdc_id        brand_owner                 description  \\\n",
       "0  356425  G. T. Japan, Inc.     MOCHI ICE CREAM BONBONS   \n",
       "1  356426       FRESH & EASY     CHIPOTLE BARBECUE SAUCE   \n",
       "2  356427       FRESH & EASY  HOT & SPICY BARBECUE SAUCE   \n",
       "3  356428       FRESH & EASY              BARBECUE SAUCE   \n",
       "4  356429       FRESH & EASY              BARBECUE SAUCE   \n",
       "\n",
       "                                         ingredients  serving_size  \\\n",
       "0  ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...          40.0   \n",
       "1  WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...          37.0   \n",
       "2  SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...          34.0   \n",
       "3  TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...          35.0   \n",
       "4  SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...          37.0   \n",
       "\n",
       "  household_serving_fulltext  energy  fat_total  fat_sat  fat_trans  chol  \\\n",
       "0                    1 PIECE   200.0       6.25     3.75        0.0  25.0   \n",
       "1                     2 Tbsp   162.0       0.00     0.00        0.0   0.0   \n",
       "2                     2 Tbsp   176.0       0.00     0.00        0.0   0.0   \n",
       "3                     2 Tbsp   143.0       0.00     0.00        0.0   0.0   \n",
       "4                     2 Tbsp   189.0       0.00     0.00        0.0   0.0   \n",
       "\n",
       "   protein  carbs  fiber  sugars  sodium  log_serv  cat_All Noodles  \\\n",
       "0      2.5  35.00    0.0   30.00    75.0  3.688879                0   \n",
       "1      0.0  43.24    0.0   37.84   703.0  3.610918                0   \n",
       "2      0.0  41.18    0.0   35.29   676.0  3.526361                0   \n",
       "3      0.0  34.29    0.0   28.57   971.0  3.555348                0   \n",
       "4      0.0  45.95    0.0   43.24   757.0  3.610918                0   \n",
       "\n",
       "   cat_Bacon, Sausages & Ribs  cat_Baking  cat_Baking Accessories  \\\n",
       "0                           0           0                       0   \n",
       "1                           0           0                       0   \n",
       "2                           0           0                       0   \n",
       "3                           0           0                       0   \n",
       "4                           0           0                       0   \n",
       "\n",
       "   cat_Baking Additives & Extracts  cat_Baking Decorations & Dessert Toppings  \\\n",
       "0                                0                                          0   \n",
       "1                                0                                          0   \n",
       "2                                0                                          0   \n",
       "3                                0                                          0   \n",
       "4                                0                                          0   \n",
       "\n",
       "   cat_Baking/Cooking Mixes (Perishable)  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   cat_Baking/Cooking Mixes (Shelf Stable)  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   cat_Baking/Cooking Mixes/Supplies Variety Packs  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   cat_Baking/Cooking Supplies (Shelf Stable)  cat_Beef - Prepared/Processed  \\\n",
       "0                                           0                              0   \n",
       "1                                           0                              0   \n",
       "2                                           0                              0   \n",
       "3                                           0                              0   \n",
       "4                                           0                              0   \n",
       "\n",
       "   cat_Biscuits/Cookies (Shelf Stable)  cat_Bread & Muffin Mixes  \\\n",
       "0                                    0                         0   \n",
       "1                                    0                         0   \n",
       "2                                    0                         0   \n",
       "3                                    0                         0   \n",
       "4                                    0                         0   \n",
       "\n",
       "   cat_Breads & Buns  cat_Breakfast Drinks  cat_Breakfast Foods  \\\n",
       "0                  0                     0                    0   \n",
       "1                  0                     0                    0   \n",
       "2                  0                     0                    0   \n",
       "3                  0                     0                    0   \n",
       "4                  0                     0                    0   \n",
       "\n",
       "   cat_Breakfast Sandwiches, Biscuits & Meals  cat_Butter & Spread  \\\n",
       "0                                           0                    0   \n",
       "1                                           0                    0   \n",
       "2                                           0                    0   \n",
       "3                                           0                    0   \n",
       "4                                           0                    0   \n",
       "\n",
       "   cat_Cake, Cookie & Cupcake Mixes  cat_Cakes - Sweet (Frozen)  \\\n",
       "0                                 0                           0   \n",
       "1                                 0                           0   \n",
       "2                                 0                           0   \n",
       "3                                 0                           0   \n",
       "4                                 0                           0   \n",
       "\n",
       "   cat_Cakes - Sweet (Shelf Stable)  cat_Cakes, Cupcakes, Snack Cakes  \\\n",
       "0                                 0                                 0   \n",
       "1                                 0                                 0   \n",
       "2                                 0                                 0   \n",
       "3                                 0                                 0   \n",
       "4                                 0                                 0   \n",
       "\n",
       "   cat_Candy  cat_Canned & Bottled Beans  cat_Canned Condensed Soup  \\\n",
       "0          0                           0                          0   \n",
       "1          0                           0                          0   \n",
       "2          0                           0                          0   \n",
       "3          0                           0                          0   \n",
       "4          0                           0                          0   \n",
       "\n",
       "   cat_Canned Fruit  cat_Canned Meat  cat_Canned Seafood  cat_Canned Soup  \\\n",
       "0                 0                0                   0                0   \n",
       "1                 0                0                   0                0   \n",
       "2                 0                0                   0                0   \n",
       "3                 0                0                   0                0   \n",
       "4                 0                0                   0                0   \n",
       "\n",
       "   cat_Canned Tuna  cat_Canned Vegetables  cat_Cereal  cat_Cereal/Muesli Bars  \\\n",
       "0                0                      0           0                       0   \n",
       "1                0                      0           0                       0   \n",
       "2                0                      0           0                       0   \n",
       "3                0                      0           0                       0   \n",
       "4                0                      0           0                       0   \n",
       "\n",
       "   ...  cat_Pancakes, Waffles, French Toast & Crepes  cat_Pasta Dinners  \\\n",
       "0  ...                                             0                  0   \n",
       "1  ...                                             0                  0   \n",
       "2  ...                                             0                  0   \n",
       "3  ...                                             0                  0   \n",
       "4  ...                                             0                  0   \n",
       "\n",
       "   cat_Pasta by Shape & Type  cat_Pasta/Noodles - Not Ready to Eat (Frozen)  \\\n",
       "0                          0                                              0   \n",
       "1                          0                                              0   \n",
       "2                          0                                              0   \n",
       "3                          0                                              0   \n",
       "4                          0                                              0   \n",
       "\n",
       "   cat_Pastry Shells & Fillings  cat_Pepperoni, Salami & Cold Cuts  \\\n",
       "0                             0                                  0   \n",
       "1                             0                                  0   \n",
       "2                             0                                  0   \n",
       "3                             0                                  0   \n",
       "4                             0                                  0   \n",
       "\n",
       "   cat_Pickles, Olives, Peppers & Relishes  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   cat_Pies/Pastries - Sweet (Shelf Stable)  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "\n",
       "   cat_Pies/Pastries/Pizzas/Quiches - Savoury (Frozen)  cat_Pizza  \\\n",
       "0                                                  0            0   \n",
       "1                                                  0            0   \n",
       "2                                                  0            0   \n",
       "3                                                  0            0   \n",
       "4                                                  0            0   \n",
       "\n",
       "   cat_Pizza Mixes & Other Dry Dinners  cat_Plant Based Milk  \\\n",
       "0                                    0                     0   \n",
       "1                                    0                     0   \n",
       "2                                    0                     0   \n",
       "3                                    0                     0   \n",
       "4                                    0                     0   \n",
       "\n",
       "   cat_Plant Based Water  cat_Popcorn (Shelf Stable)  \\\n",
       "0                      0                           0   \n",
       "1                      0                           0   \n",
       "2                      0                           0   \n",
       "3                      0                           0   \n",
       "4                      0                           0   \n",
       "\n",
       "   cat_Popcorn, Peanuts, Seeds & Related Snacks  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   cat_Pork Sausages - Prepared/Processed  cat_Poultry, Chicken & Turkey  \\\n",
       "0                                       0                              0   \n",
       "1                                       0                              0   \n",
       "2                                       0                              0   \n",
       "3                                       0                              0   \n",
       "4                                       0                              0   \n",
       "\n",
       "   cat_Powdered Drinks  cat_Pre-Packaged Fruit & Vegetables  \\\n",
       "0                    0                                    0   \n",
       "1                    0                                    0   \n",
       "2                    0                                    0   \n",
       "3                    0                                    0   \n",
       "4                    0                                    0   \n",
       "\n",
       "   cat_Prepared Pasta & Pizza Sauces  cat_Prepared Subs & Sandwiches  \\\n",
       "0                                  0                               0   \n",
       "1                                  0                               0   \n",
       "2                                  0                               0   \n",
       "3                                  0                               0   \n",
       "4                                  0                               0   \n",
       "\n",
       "   cat_Prepared Wraps and Burittos  cat_Processed Cheese & Cheese Novelties  \\\n",
       "0                                0                                        0   \n",
       "1                                0                                        0   \n",
       "2                                0                                        0   \n",
       "3                                0                                        0   \n",
       "4                                0                                        0   \n",
       "\n",
       "   cat_Puddings & Custards  cat_Rice  cat_Salad Dressing & Mayonnaise  \\\n",
       "0                        0         0                                0   \n",
       "1                        0         0                                0   \n",
       "2                        0         0                                0   \n",
       "3                        0         0                                0   \n",
       "4                        0         0                                0   \n",
       "\n",
       "   cat_Sausages, Hotdogs & Brats  \\\n",
       "0                              0   \n",
       "1                              0   \n",
       "2                              0   \n",
       "3                              0   \n",
       "4                              0   \n",
       "\n",
       "   cat_Seasoning Mixes, Salts, Marinades & Tenderizers  \\\n",
       "0                                                  0     \n",
       "1                                                  0     \n",
       "2                                                  0     \n",
       "3                                                  0     \n",
       "4                                                  0     \n",
       "\n",
       "   cat_Snack, Energy & Granola Bars  cat_Soda  \\\n",
       "0                                 0         0   \n",
       "1                                 0         0   \n",
       "2                                 0         0   \n",
       "3                                 0         0   \n",
       "4                                 0         0   \n",
       "\n",
       "   cat_Soups - Prepared (Shelf Stable)  cat_Specialty Formula Supplements  \\\n",
       "0                                    0                                  0   \n",
       "1                                    0                                  0   \n",
       "2                                    0                                  0   \n",
       "3                                    0                                  0   \n",
       "4                                    0                                  0   \n",
       "\n",
       "   cat_Sport Drinks  cat_Stuffing  cat_Sushi  cat_Syrups & Molasses  \\\n",
       "0                 0             0          0                      0   \n",
       "1                 0             0          0                      0   \n",
       "2                 0             0          0                      0   \n",
       "3                 0             0          0                      0   \n",
       "4                 0             0          0                      0   \n",
       "\n",
       "   cat_Tea Bags  cat_Tomatoes  cat_Vegetable & Cooking Oils  \\\n",
       "0             0             0                             0   \n",
       "1             0             0                             0   \n",
       "2             0             0                             0   \n",
       "3             0             0                             0   \n",
       "4             0             0                             0   \n",
       "\n",
       "   cat_Vegetable Based Products / Meals - Not Ready to Eat (Frozen)  \\\n",
       "0                                                  0                  \n",
       "1                                                  0                  \n",
       "2                                                  0                  \n",
       "3                                                  0                  \n",
       "4                                                  0                  \n",
       "\n",
       "   cat_Vegetable and Lentil Mixes  \\\n",
       "0                               0   \n",
       "1                               0   \n",
       "2                               0   \n",
       "3                               0   \n",
       "4                               0   \n",
       "\n",
       "   cat_Vegetables - Prepared/Processed (Frozen)  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   cat_Vegetables - Prepared/Processed (Shelf Stable)  \\\n",
       "0                                                  0    \n",
       "1                                                  0    \n",
       "2                                                  0    \n",
       "3                                                  0    \n",
       "4                                                  0    \n",
       "\n",
       "   cat_Vegetarian Frozen Meats  cat_Vitamins  cat_Water  cat_Weight Control  \\\n",
       "0                            0             0          0                   0   \n",
       "1                            0             0          0                   0   \n",
       "2                            0             0          0                   0   \n",
       "3                            0             0          0                   0   \n",
       "4                            0             0          0                   0   \n",
       "\n",
       "   cat_Wholesome Snacks  cat_Yogurt  \\\n",
       "0                     0           0   \n",
       "1                     0           0   \n",
       "2                     0           0   \n",
       "3                     0           0   \n",
       "4                     0           0   \n",
       "\n",
       "   cat_Yogurt/Yogurt Substitutes (Perishable)  \n",
       "0                                           0  \n",
       "1                                           0  \n",
       "2                                           0  \n",
       "3                                           0  \n",
       "4                                           0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for new cols\n",
    "foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessing steps here are to split our dataset into the X features and y target, and then scale the features so they can be flexibly used across different model types. The predicting features are going to include the full nutritional profile (minus the serving size), and the branded food category. We do not need the ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y vars\n",
    "X = foods.drop(columns=[\"fdc_id\", \n",
    "                        \"brand_owner\", \n",
    "                        \"description\", \n",
    "                        \"ingredients\", \n",
    "                        \"household_serving_fulltext\",\n",
    "                        \"serving_size\",\n",
    "                       \"log_serv\"])\n",
    "y = foods[\"log_serv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# instantiating scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# scaling X data\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been fully prepared, we can beign the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way of predicting serving size without using any machine learning, by simply using the mean of the target. For this model, we will measure the MSE, and will use the default scoring of R<sup>2</sup> for the remainder of the models. Both metrics will be evaluated for all models after the modeling is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Train MSE Score for our Base Model is: 3685.336822662155\n",
      "Our Test MSE Score for our Base Model is: 3833.116046199187\n"
     ]
    }
   ],
   "source": [
    "# building a base model to compare results against\n",
    "# using code from Boom's Local Session\n",
    "\n",
    "# Instantiate: creates a dummy regression that always predicts the mean of the target\n",
    "base_mean = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Fit the \"model\"\n",
    "base_mean_model = base_mean.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions for our testing set (not kaggle testing set)\n",
    "y_hat_base_train = base_mean.predict(X_train)\n",
    "y_hat_base_test = base_mean.predict(X_test)\n",
    "\n",
    "# Get R2\n",
    "print(\"Our Train MSE Score for our Base Model is:\", metrics.mean_squared_error(np.exp(y_train),np.exp(y_hat_base_train)))\n",
    "print(\"Our Test MSE Score for our Base Model is:\",  metrics.mean_squared_error(np.exp(y_test),np.exp(y_hat_base_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The help find the best possible model parameters, we should build a function to perform gridsearches for us. We can then implement this function on any model type and set of parameters to optimize each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in func from Reddit NLP project\n",
    "def grid_searcher(pipe, params):\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=params, cv=3, verbose=1, n_jobs=3)\n",
    "    gs.fit(X_train_sc, y_train)\n",
    "    print(f'CrossVal Score: {gs.best_score_}')\n",
    "    print(f'Training Score: {gs.score(X_train_sc, y_train)}')\n",
    "    print(f'Testing Score: {gs.score(X_test_sc, y_test)}')\n",
    "    print(gs.best_params_)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As linear regression is a very simple model, there is really no hyperparameter searching that needs to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg_model = linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train R2 score is: 0.7353463856717573.\n",
      "The test R2 score is: 0.7406407151042635.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The train R2 score is: {linreg_model.score(X_train, y_train)}.\")\n",
    "print(f\"The test R2 score is: {linreg_model.score(X_test, y_test)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic linear regression model is not scoring very high, although it is also showing a fairly low variance. Since we did see some multicolinearity in a few of the features during the EDA, it may be worth looking at a LASSO or ridge regression for some improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `LassoCV` model already iterates through a list of 100 alphas, we do not need to run the gridsearching function on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jondov/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV()\n",
    "lasso_model = lasso.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train R2 score is: 0.7351460362337019.\n",
      "The test R2 score is: 0.7403776480182412.\n",
      "the var is -0.005231611784539347\n",
      "the adjusted test score is 0.7456092598027806\n"
     ]
    }
   ],
   "source": [
    "train_r2 = lasso_model.score(X_train_sc, y_train)\n",
    "test_r2 = lasso_model.score(X_test_sc, y_test)\n",
    "\n",
    "print(f\"The train R2 score is: {train_r2}.\")\n",
    "print(f\"The test R2 score is: {test_r2}.\")\n",
    "print(f\"the adjusted test score is {test_r2 - var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While There is also a `RidgeCV` model that will test different alpha values and cross-validate, it only tests three values, which may not truly be optimizing that hyperparameter. We will employ the custom gridsearching function to expand on these values, and be able to hone in on a more precise alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe = Pipeline([(\"ridge\", Ridge())])\n",
    "\n",
    "ridge_params = {\n",
    "    \"ridge__alpha\": [11, 12, 13]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.7338202830606281\n",
      "Training Score: 0.7353441884095184\n",
      "Testing Score: 0.7406257916854004\n",
      "{'ridge__alpha': 12}\n"
     ]
    }
   ],
   "source": [
    "ridge_model = grid_searcher(ridge_pipe, ridge_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of these regularization methods had any large impact on the R<sup>2</sup> scores. These model are typically most helpful to regularize overfit linear regressions, and as our initial model was not very overfit, they may not just not be enough of an improvement. We may need to utilize a more advanced regression method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As using a Neural Network was showing promise as the best model type to use, we should have a way of effectively optimizing it. While a Keras Sequential model cannot be passed through Scikit-learn's `GridSearchCV`, we can make our own custom function to essentially perform the same type of task. To that end, the following functions were created, which were adapted from code written by Mahdi Shadkam-Farrokhi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function to be made is for establishing the permutations of parameters. This fucntion will take in a dictionary of all the layer parameters, and unpacks each combination into a distinct set of single parameters that a model can be built on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_params(grid_params):\n",
    "    # returns a list of all combinations of unique parameters from the given dictionary\n",
    "    out = [{}]\n",
    "    for param_name, param_list in grid_params.items():\n",
    "        if len(param_list) == 1:\n",
    "            for item in out:\n",
    "                item[param_name] = param_list[0]\n",
    "        else:\n",
    "            temp_out = []\n",
    "            for param_val in param_list:\n",
    "                for item in out:\n",
    "                    cloned_item = item.copy()\n",
    "                    cloned_item[param_name] = param_val\n",
    "                    temp_out.append(cloned_item)\n",
    "            out = temp_out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece is to take a dictionary of parameters and build a functioning model out of it. We will make a model building fucntion to perfrom this action. It will take our train/test split data, and a dictionary of the parameters that has the specified keys. The function will reach into each of the keys for the appropriate attribute, and then apply that to the corresponding layer. To increase flexibility, there will be default settings for each of the parameters, so that the dictioanry does not have to contain entries for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params_dict, X_train, X_test, y_train, y_test):\n",
    "    # defining params\n",
    "    first_layer_nodes = params_dict.get(\"first_layer_nodes\") or 16            # default low nodes\n",
    "    first_dropout_rate = params_dict.get(\"first_dropout_rate\") or 0.0         # default no dropout\n",
    "    \n",
    "    second_layer_nodes = params_dict.get(\"second_layer_nodes\") or 16          # default low nodes\n",
    "    second_dropout_rate = params_dict.get(\"second_dropout_rate\") or 0.0       # default no dropout\n",
    "    \n",
    "    third_layer_nodes = params_dict.get(\"third_layer_nodes\") or 16            # default low nodes\n",
    "    third_dropout_rate = params_dict.get(\"third_dropout_rate\") or 0.0         # default no dropout  \n",
    "    \n",
    "    reg = params_dict.get(\"reg\") or 0                                         # default no reg\n",
    "    \n",
    "    epochs = params_dict.get(\"epochs\") or 10                                  # default low epochs\n",
    "    batch_size = params_dict.get(\"batch_size\") or 1024                        # default large batch\n",
    "    early_stop = params_dict.get(\"early_stop\") or EarlyStopping(monitor=\"val_loss\",\n",
    "                                                                min_delta=0.000000001,  # small delta\n",
    "                                                                patience=100)           # large patience\n",
    "    \n",
    "    # instantiating model\n",
    "    model = Sequential()\n",
    "\n",
    "    # adding layers according to inputs\n",
    "    # first layer(s)\n",
    "    model.add(Dense(first_layer_nodes,\n",
    "                   activation=\"relu\",\n",
    "                   input_shape=(X_train.shape[1],),\n",
    "                   kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(first_dropout_rate))\n",
    "    \n",
    "    # second layer(s)\n",
    "    model.add(Dense(second_layer_nodes,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(second_dropout_rate))\n",
    "    \n",
    "    # third layer(s)\n",
    "    model.add(Dense(third_layer_nodes,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(third_dropout_rate))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compiling model\n",
    "    model.compile(loss=\"mean_squared_error\",\n",
    "             optimizer=\"adam\")\n",
    "    \n",
    "    # fitting model according to inputs\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                       callbacks=[early_stop])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece is to combine the two above functions into one functioan that will take and X and y, along with the parameter dictionary, and do the rest of the work for us. This function performs a train/test split, scales the data, and the runs the data through each of the models within the parameter dictionary. Each iteration will check the determined metric, and updates it if it has been improved. At the end of the function, the model that gave the best score for the metric is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_grid_search(\n",
    "    X,\n",
    "    y,\n",
    "    grid_params,\n",
    "    random_state=42\n",
    "):\n",
    "    ### this will make a series of FFNN models \n",
    "    ### and return the one with the best score as set below\n",
    "    ### currently set to test r2 score\n",
    "    \n",
    "    # list of all parameter combinations\n",
    "    all_params = permutate_params(grid_params)\n",
    "    \n",
    "    # creating vars with to update each iter\n",
    "    best_model = None\n",
    "    best_score = 0.0 # no accuracy to start\n",
    "    best_params = None\n",
    "    best_history = None\n",
    "    \n",
    "    # train/test split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    # scaling data\n",
    "    ss = StandardScaler()\n",
    "    X_train_sc = ss.fit_transform(X_train)\n",
    "    X_test_sc = ss.transform(X_test)\n",
    "    \n",
    "    # looping through the unpacked parameter list\n",
    "    for i, params in enumerate(all_params):\n",
    "        \n",
    "        # keeping track of which model we're running\n",
    "        print(f\"Building model {i + 1} of {len(all_params)}\")\n",
    "        \n",
    "        # bulding the model\n",
    "        model, history = build_model(\n",
    "            params_dict = params,\n",
    "            X_train = X_train_sc, \n",
    "            X_test = X_test_sc, \n",
    "            y_train = y_train, \n",
    "            y_test = y_test\n",
    "        )\n",
    "        \n",
    "        # making preds and scoring\n",
    "        test_preds = model.predict(X_test_sc)\n",
    "        score = metrics.r2_score(y_test, test_preds)\n",
    "        \n",
    "        # checking if the score beats the current best\n",
    "        # updates vars if true\n",
    "        if score > best_score:\n",
    "            print(\"***Good R2 found: {:.2%}***\".format(score))\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "            best_history = history\n",
    "    \n",
    "    # loop is done, return the best model\n",
    "    return {\n",
    "        \"best_model\"   : best_model,\n",
    "        \"best_score\"   : best_score,\n",
    "        \"best_params\"  : best_params,\n",
    "        \"best_history\" : best_history,\n",
    "        \"test_preds\"   : test_preds\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions built, we can run the search. We will start with finding the best number of nodes out of three hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting params\n",
    "node_params = {\n",
    "    \"first_layer_nodes\": [256, 128, 64],\n",
    "    \"second_layer_nodes\": [128, 64, 32],\n",
    "    \"third_layer_nodes\": [64, 32, 16],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model 1 of 27\n",
      "WARNING:tensorflow:From /Users/jondov/anaconda3/envs/dsi/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/jondov/anaconda3/envs/dsi/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 1.2851 - val_loss: 0.2221\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.2287 - val_loss: 0.3477\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.1855 - val_loss: 0.1662\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1632 - val_loss: 0.1578\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1515 - val_loss: 0.1494\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1448 - val_loss: 0.1445\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1420 - val_loss: 0.1450\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1384 - val_loss: 0.1378\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1340 - val_loss: 0.1370\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1315 - val_loss: 0.1337\n",
      "***Good R2 found: 86.89%***\n",
      "Building model 2 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 1.4214 - val_loss: 0.2129\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2024 - val_loss: 0.1813\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1859 - val_loss: 0.1682\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1632 - val_loss: 0.1566\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1525 - val_loss: 0.1523\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1489 - val_loss: 0.1503\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1443 - val_loss: 0.1452\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1413 - val_loss: 0.1411\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1381 - val_loss: 0.1373\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1365 - val_loss: 0.1351\n",
      "Building model 3 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 1.9569 - val_loss: 0.2484\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.2306 - val_loss: 0.1928\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1813 - val_loss: 0.1768\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1697 - val_loss: 0.1621\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1594 - val_loss: 0.1542\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1516 - val_loss: 0.1490\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1479 - val_loss: 0.1474\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1482 - val_loss: 0.1511\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1501 - val_loss: 0.1551\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1506 - val_loss: 0.1509\n",
      "Building model 4 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.6273 - val_loss: 0.2266\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.2084 - val_loss: 0.1814\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.1768 - val_loss: 0.1671\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1631 - val_loss: 0.1602\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.1561 - val_loss: 0.1550\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1497 - val_loss: 0.1490\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1461 - val_loss: 0.1447\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1414 - val_loss: 0.1412\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1387 - val_loss: 0.1370\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1354 - val_loss: 0.1357\n",
      "Building model 5 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 1.8449 - val_loss: 0.2322\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.2095 - val_loss: 0.1885\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1805 - val_loss: 0.1716\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1718 - val_loss: 0.1643\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1602 - val_loss: 0.1616\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1576 - val_loss: 0.1591\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1612 - val_loss: 0.1535\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1517 - val_loss: 0.1486\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1449 - val_loss: 0.1438\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1416 - val_loss: 0.1426\n",
      "Building model 6 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 1.9813 - val_loss: 0.3277\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2494 - val_loss: 0.2107\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1911 - val_loss: 0.1808\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1722 - val_loss: 0.1664\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1632 - val_loss: 0.1608\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1566 - val_loss: 0.1576\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1527 - val_loss: 0.1529\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1509 - val_loss: 0.1500\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1474 - val_loss: 0.1535\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1460 - val_loss: 0.1469\n",
      "Building model 7 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 1.5990 - val_loss: 0.2249\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1976 - val_loss: 0.1785\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1709 - val_loss: 0.1635\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1590 - val_loss: 0.1558\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1520 - val_loss: 0.1516\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.1477 - val_loss: 0.1469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1439 - val_loss: 0.1432\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1401 - val_loss: 0.1416\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1366 - val_loss: 0.1384\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1345 - val_loss: 0.1378\n",
      "Building model 8 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 1.7635 - val_loss: 0.2302\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.2100 - val_loss: 0.1829\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1756 - val_loss: 0.1690\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.1645 - val_loss: 0.1606\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1615 - val_loss: 0.1596\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1557 - val_loss: 0.1480\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.1480 - val_loss: 0.1473\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1451 - val_loss: 0.1420\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1413 - val_loss: 0.1424\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1391 - val_loss: 0.1403\n",
      "Building model 9 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 1.9479 - val_loss: 0.2908\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.2414 - val_loss: 0.2009\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1909 - val_loss: 0.1773\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1731 - val_loss: 0.1667\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1637 - val_loss: 0.1607\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1584 - val_loss: 0.1558\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1539 - val_loss: 0.1532\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1503 - val_loss: 0.1487\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1457 - val_loss: 0.1469\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1438 - val_loss: 0.1476\n",
      "Building model 10 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 1.3801 - val_loss: 0.2153\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1939 - val_loss: 0.1756\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1754 - val_loss: 0.1663\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.1613 - val_loss: 0.1601\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1533 - val_loss: 0.1515\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1473 - val_loss: 0.1452\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1437 - val_loss: 0.1390\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1388 - val_loss: 0.1389\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1378 - val_loss: 0.1368\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1338 - val_loss: 0.1339\n",
      "Building model 11 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.8140 - val_loss: 0.2371\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.2125 - val_loss: 0.1893\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1792 - val_loss: 0.1711\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1659 - val_loss: 0.1607\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1587 - val_loss: 0.1538\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1515 - val_loss: 0.1499\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1507 - val_loss: 0.1499\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1439 - val_loss: 0.1426\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1398 - val_loss: 0.1384\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1360 - val_loss: 0.1357\n",
      "Building model 12 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 1.8475 - val_loss: 0.2855\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.2278 - val_loss: 0.2069\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1891 - val_loss: 0.1843\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1734 - val_loss: 0.1729\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1662 - val_loss: 0.1641\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1602 - val_loss: 0.1643\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1552 - val_loss: 0.1641\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1529 - val_loss: 0.1732\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1516 - val_loss: 0.1504\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1450 - val_loss: 0.1461\n",
      "Building model 13 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 1.5479 - val_loss: 0.2267\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2107 - val_loss: 0.1828\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1776 - val_loss: 0.1658\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1643 - val_loss: 0.1550\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1555 - val_loss: 0.1521\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1504 - val_loss: 0.1483\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1451 - val_loss: 0.1434\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1422 - val_loss: 0.1476\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1397 - val_loss: 0.1430\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1369 - val_loss: 0.1386\n",
      "Building model 14 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 1.8050 - val_loss: 0.2277\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.2036 - val_loss: 0.1796\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1752 - val_loss: 0.1677\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1638 - val_loss: 0.1575\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1604 - val_loss: 0.1542\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1513 - val_loss: 0.1483\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1493 - val_loss: 0.1469\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1482 - val_loss: 0.1450\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1438 - val_loss: 0.1386\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1374 - val_loss: 0.1364\n",
      "Building model 15 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 2.1271 - val_loss: 0.2548\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2266 - val_loss: 0.1943\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1877 - val_loss: 0.1747\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1721 - val_loss: 0.1650\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1641 - val_loss: 0.1596\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1577 - val_loss: 0.1555\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1554 - val_loss: 0.1517\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1521 - val_loss: 0.1522\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1511 - val_loss: 0.1545\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1505 - val_loss: 0.1485\n",
      "Building model 16 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.7004 - val_loss: 0.2390\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.2234 - val_loss: 0.1909\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1774 - val_loss: 0.1669\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1621 - val_loss: 0.1570\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1524 - val_loss: 0.1517\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1469 - val_loss: 0.1458\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1427 - val_loss: 0.1433\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1400 - val_loss: 0.1441\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1387 - val_loss: 0.1374\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1335 - val_loss: 0.1357\n",
      "Building model 17 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 1.4177 - val_loss: 0.2180\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.2182 - val_loss: 0.1805\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1765 - val_loss: 0.1655\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1613 - val_loss: 0.1590\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.1543 - val_loss: 0.1532\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1492 - val_loss: 0.1480\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1451 - val_loss: 0.1428\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1420 - val_loss: 0.1422\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1396 - val_loss: 0.1401\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1380 - val_loss: 0.1391\n",
      "Building model 18 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 2.2580 - val_loss: 0.3508\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2647 - val_loss: 0.2014\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1918 - val_loss: 0.1771\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1725 - val_loss: 0.1663\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1637 - val_loss: 0.1598\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1573 - val_loss: 0.1547\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1539 - val_loss: 0.1507\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1498 - val_loss: 0.1475\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1457 - val_loss: 0.1439\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1427 - val_loss: 0.1423\n",
      "Building model 19 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.8776 - val_loss: 0.2229\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.2028 - val_loss: 0.1904\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1742 - val_loss: 0.1642\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.1590 - val_loss: 0.1528\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1487 - val_loss: 0.1478\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1424 - val_loss: 0.1409\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1406 - val_loss: 0.1415\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1362 - val_loss: 0.1378\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1351 - val_loss: 0.1347\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1338 - val_loss: 0.1344\n",
      "Building model 20 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 2.1224 - val_loss: 0.2438\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.2126 - val_loss: 0.1916\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1826 - val_loss: 0.1770\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1709 - val_loss: 0.1716\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1655 - val_loss: 0.1677\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1611 - val_loss: 0.1661\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1611 - val_loss: 0.1550\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1526 - val_loss: 0.1548\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1491 - val_loss: 0.1519\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1450 - val_loss: 0.1467\n",
      "Building model 21 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 2.3222 - val_loss: 0.2539\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.2361 - val_loss: 0.1955\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1905 - val_loss: 0.1770\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1731 - val_loss: 0.1668\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1642 - val_loss: 0.1605\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1589 - val_loss: 0.1571\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.1547 - val_loss: 0.1533\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1512 - val_loss: 0.1515\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1482 - val_loss: 0.1495\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1476 - val_loss: 0.1469\n",
      "Building model 22 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 6s 42us/step - loss: 1.1204 - val_loss: 0.2240\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2049 - val_loss: 0.1812\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1734 - val_loss: 0.1645\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1610 - val_loss: 0.1582\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1542 - val_loss: 0.1531\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1481 - val_loss: 0.1485\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1458 - val_loss: 0.1500\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1431 - val_loss: 0.1415\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1386 - val_loss: 0.1389\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1357 - val_loss: 0.1345\n",
      "Building model 23 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 1.5546 - val_loss: 0.2329\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.2036 - val_loss: 0.1815\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1730 - val_loss: 0.1649\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1612 - val_loss: 0.1576\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1550 - val_loss: 0.1528\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1496 - val_loss: 0.1506\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1467 - val_loss: 0.1487\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1436 - val_loss: 0.1460\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1413 - val_loss: 0.1421\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1391 - val_loss: 0.1425\n",
      "Building model 24 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 2.6120 - val_loss: 0.2630\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.2322 - val_loss: 0.2023\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1934 - val_loss: 0.1839\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1791 - val_loss: 0.1746\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1704 - val_loss: 0.1714\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1654 - val_loss: 0.1653\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1603 - val_loss: 0.1582\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1544 - val_loss: 0.1520\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1507 - val_loss: 0.1482\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1484 - val_loss: 0.1509\n",
      "Building model 25 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.7166 - val_loss: 0.2296\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.2134 - val_loss: 0.1942\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1858 - val_loss: 0.1794\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1725 - val_loss: 0.1610\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1615 - val_loss: 0.1553\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1541 - val_loss: 0.1515\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1475 - val_loss: 0.1473\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1433 - val_loss: 0.1424\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1407 - val_loss: 0.1398\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1370 - val_loss: 0.1378\n",
      "Building model 26 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.5951 - val_loss: 0.2866\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.2296 - val_loss: 0.1850\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1877 - val_loss: 0.1738\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1740 - val_loss: 0.1704\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1631 - val_loss: 0.1624\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1559 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1518 - val_loss: 0.1536\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1494 - val_loss: 0.1454\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1500 - val_loss: 0.1468\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1484 - val_loss: 0.1425\n",
      "Building model 27 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 2.5544 - val_loss: 0.2744\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.2425 - val_loss: 0.2090\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1980 - val_loss: 0.1921\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1847 - val_loss: 0.1803\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1786 - val_loss: 0.1778\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1788 - val_loss: 0.1924\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1739 - val_loss: 0.1656\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1630 - val_loss: 0.1594\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1611 - val_loss: 0.1611\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1627 - val_loss: 0.1591\n"
     ]
    }
   ],
   "source": [
    "# best_model_dict = nn_grid_search(X, y, node_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the grid search has completed, we have a list of dictionaries that holds the model that gave use the highest R<sup>2</sup> score, along with the parameters of that model, the score, and the history.\n",
    "\n",
    "In order to not have to run this cell multiple times, but be able to keep the model object, we are going to pickle the result. This process will store the completed model as its specific byte stream, which can then be loaded back exactly as it was, without needing to fit each of the models over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_model': <keras.engine.sequential.Sequential at 0x1a38161d10>,\n",
       " 'best_score': 0.8689408311089408,\n",
       " 'best_params': {'first_layer_nodes': 256,\n",
       "  'second_layer_nodes': 128,\n",
       "  'third_layer_nodes': 64},\n",
       " 'best_history': <keras.callbacks.History at 0x1a390f8890>,\n",
       " 'test_preds': array([[5.5052342],\n",
       "        [4.5292025],\n",
       "        [3.9009528],\n",
       "        ...,\n",
       "        [5.1670656],\n",
       "        [4.783846 ],\n",
       "        [3.5345993]], dtype=float32)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train R2 score is: 0.8730327919420489.\n",
      "The test R2 score is: 0.84398686878084.\n"
     ]
    }
   ],
   "source": [
    "# making predictions\n",
    "nn_train_preds = best_model_dict[\"best_model\"].predict(X_train_sc)\n",
    "nn_test_preds = best_model_dict[\"test_preds\"]\n",
    "\n",
    "print(f\"The train R2 score is: {metrics.r2_score(y_train, nn_train_preds)}.\")\n",
    "print(f\"The test R2 score is: {metrics.r2_score(y_test, nn_test_preds)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling tutorial found on https://www.datacamp.com/community/tutorials/pickle-python-tutorial\n",
    "\n",
    "# making new file\n",
    "outfile = open(\"nn_gs_pickle\", \"wb\")\n",
    "\n",
    "# dumping model to pickle\n",
    "pickle.dump(best_model_dict, outfile)\n",
    "\n",
    "# closing new file\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the model that had the best promise, and use those parameters in a new model to find the best amount of epochs for it. After some trials, 75 epochs seemed to be the best number to use, though we were left with some slight overfitting, so an additional search will be performed on using different regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making new param dict with regularization\n",
    "pdict = {\n",
    "    \"first_layer_nodes\": [256],\n",
    "    \"first_dropout_rate\": [0.25, 0.5],\n",
    "    \"second_layer_nodes\": [128],\n",
    "    \"second_dropout_rate\": [0.25, 0.5],\n",
    "    \"third_layer_nodes\": [16],\n",
    "    \"third_dropout_rate\": [0.25, 0.5],\n",
    "    \"reg\": [0.001, 0.01, 0.1],\n",
    "    \"epochs\": [75]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_nn_model_dict = nn_grid_search(X, y, grid_params=pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the grid search has completed, we want to once again pickle the resulting model so we don't have to run the cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making new file\n",
    "outfile = open(\"final_nn_pickle\", \"wb\")\n",
    "\n",
    "# dumping model to pickle\n",
    "pickle.dump(final_nn_model_dict, outfile)\n",
    "\n",
    "# closing new file\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening pickle file\n",
    "infile = open(\"final_nn_pickle\", \"rb\")\n",
    "\n",
    "# unpickling back into model object\n",
    "final_nn_model_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nn_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final model as it's own object\n",
    "final_nn_model = final_nn_model_dict[\"best_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "nn_train_preds = final_nn_model.predict(X_train_sc)\n",
    "nn_test_preds = final_nn_model_dict[\"test_preds\"]\n",
    "\n",
    "print(f\"The train R2 score is: {metrics.r2_score(y_train, nn_train_preds)}.\")\n",
    "print(f\"The train R2 score is: {metrics.r2_score(y_test, nn_test_preds)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_train, nn_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, nn_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help automate the eavluations process, we are going to compile the train and test predictions for each of our models, and from there we will be able to easily evaluate the desired metrics from one spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making train pred df\n",
    "train_pred_df = pd.DataFrame(columns=[\"y_train\",\n",
    "                                      \"base\",\n",
    "                                      \"linear\",\n",
    "                                      \"lasso\",\n",
    "                                      \"ridge\",\n",
    "                                      \"neural_net\"])\n",
    "\n",
    "# making test pred df\n",
    "test_pred_df = pd.DataFrame(columns=[\"y_test\",\n",
    "                                     \"base\",\n",
    "                                     \"linear\",\n",
    "                                     \"lasso\",\n",
    "                                     \"ridge\",\n",
    "                                     \"neural_net\"])\n",
    "# filling both y_true cols\n",
    "train_pred_df[\"y_train\"] = np.exp(y_train)\n",
    "test_pred_df[\"y_test\"] = np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dicts of our models\n",
    "# one for unscaled Xs\n",
    "unsc_models = {\"base\": base_mean,\n",
    "               \"linear\": linreg_model}\n",
    "\n",
    "# one for scaled Xs\n",
    "sc_models = {\"lasso\": lasso_model,\n",
    "             \"ridge\": ridge_model,\n",
    "             \"neural_net\": final_nn_model}\n",
    "\n",
    "\n",
    "# looping through unsc_models to input preds into df\n",
    "for model in unsc_models.keys():\n",
    "    train_pred_df[model] = np.exp(unsc_models[model].predict(X_train))\n",
    "    test_pred_df[model] = np.exp(unsc_models[model].predict(X_test))\n",
    "\n",
    "# looping through sc_models to input preds into df\n",
    "for model in sc_models.keys():\n",
    "    train_pred_df[model] = np.exp(sc_models[model].predict(X_train_sc))\n",
    "    test_pred_df[model] = np.exp(sc_models[model].predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R<sup>2</sup> Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate both the normal R<sup>2</sup>, and adjusted R<sup>2</sup> scores for each of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making r2 dataframe\n",
    "r2_df = pd.DataFrame(index=train_pred_df.columns[1:], columns=[\"train_r2\", \"test_r2\"])\n",
    "\n",
    "# looping through models\n",
    "for model in train_pred_df.columns[1:]:\n",
    "    \n",
    "    # setting the scores for the model row\n",
    "    r2_df.loc[model] = [\n",
    "        metrics.r2_score(train_pred_df[\"y_train\"], train_pred_df[model]),\n",
    "        metrics.r2_score(test_pred_df[\"y_test\"], test_pred_df[model])\n",
    "    ]\n",
    "    \n",
    "# making variance column\n",
    "r2_df[\"var\"] = abs(r2_df[\"train_r2\"] - r2_df[\"test_r2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling up final df\n",
    "r2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in custom adjusted r2 function from 3.01 Linear Reg Lab\n",
    "def r2_adj():\n",
    "    # seting some base variables\n",
    "    n = len(y)\n",
    "    k = len(X.columns)\n",
    "    r_sq = model.score(X, y)\n",
    "    \n",
    "    # using the vars as inputs for the adjusted r2 equation\n",
    "    r_sq_adj = 1 - (((1 - r_sq) * (n - 1)) / (n - k - 1))\n",
    "    return r_sq_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 scores are above in the modeling phase, just need to compile them and compare against adjusted r2 down here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making r2 dataframe\n",
    "mse_df = pd.DataFrame(index=train_pred_df.columns[1:], columns=[\"train_mse\", \"test_mse\"])\n",
    "\n",
    "# looping through models\n",
    "for model in train_pred_df.columns[1:]:\n",
    "    \n",
    "    # setting the scores for the model row\n",
    "    mse_df.loc[model] = [\n",
    "        metrics.mean_squared_error(train_pred_df[\"y_train\"], train_pred_df[model]),\n",
    "        metrics.mean_squared_error(test_pred_df[\"y_test\"], test_pred_df[model])\n",
    "    ]\n",
    "\n",
    "# making variance column\n",
    "mse_df[\"var\"] = abs(mse_df[\"train_mse\"] - mse_df[\"test_mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling up final df\n",
    "mse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these numbers that the neural network had by far the lowest MSE scores, though it was somewhat more overfit than the others, showing the highest variance number. However, due to how much better of a performance it was able to give us, we are going to select that as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert bar charts of mse and r2 here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = final_model[\"best_history\"].history[\"loss\"]\n",
    "test_loss = final_model[\"best_history\"].history[\"val_loss\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.title(\"Loss Curve of Best Neural Network\", size=20)\n",
    "plt.xlabel(\"Epoch\", size=15)\n",
    "plt.ylabel(\"Loss\", size=15)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line graph shows us how the loss of our final model changed as it ran through 75 epochs. There does not appear to be a lot of variation in the curves, and we have avoided divergence of the losses between the train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this comparison of predicted and actual serving sizes that our model has a clear tendency to underpredict servings as the real serving size increases. There could be several reasons behind why this is happening, though one clear and logical answer may be that some of those foods with larger serving sizes have larger portions of the food that are inedible. This could mean a food that has shells or pits, or something else that is counted in the total weight, but is not eaten. This is most likely where the outliers are arising from. Unfortunately, these aspects were not accounted for in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding residuals\n",
    "resids = np.exp(y_test) - test_pred_df[\"neural_net\"]\n",
    "\n",
    "# making plot of resids vs. preds\n",
    "sns.scatterplot(test_pred_df[\"neural_net\"], resids)\n",
    "plt.title(\"Homoskedasticity of Residals\", size=20)\n",
    "plt.xlabel(\"Predicted Serving Size (g)\", size=14)\n",
    "plt.ylabel(\"Residual from True Serving Size (g)\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that the residuals from the model are fairly homoscedastic, with only a few outliers. There could be several reasons behind the placement of these outliers, though one clear and logical answer may be that some of those foods with larger serving sizes have larger portions of the food that are inedible. This could mean a food that has shells or pits, or something else that is counted in the total weight, but is not eaten. This is most likely where the outliers are arising from. Unfortunately, these aspects were not accounted for in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(resids, bins=20, kde=False) # WHY THE WEIRD Y-TICK NUMBERS???\n",
    "# plt.hist(resids, bins=5)\n",
    "plt.title(\"Normal Distribution of Residuals\", size=20)\n",
    "plt.xlabel(\"Deviation from True Sale Price ($)\", size=14)\n",
    "plt.ylabel(\"Frequency of Distance\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this histogram that the residuals form a normally shaped distrubition, albeit with a long right tail. Coupled with the scatterplot above, We can conclude that we have made a well fit model with no clear trends in the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing a neural network gave use the best results overall. It was able to increase the R<sup>2</sup> score above 90%, and still limit variance to less than 2%. It was also able to minimize MSE fra better than the other models that were made. However, this model was still not perfect, and was subject to significant underpredcition as the true serving size increased. \n",
    "\n",
    "As discussed above, this may be influence by factors unseen by the model, such as the percentage of the food item that is not typically eaten. In addition, this model only takes certain attributes into account, and there are many other factors that can have an impact on serving size, such as package size, sale price, or targeted demographic. Many of these factors can be proprietary information to the manufacturing company, and are unavailable to the public, making a deeper analysis difficult. This could, however, be adapted within a specific company to account for some of these features, or to be trained on a more specific set of foods products.\n",
    "\n",
    "In the end, this model should still be fairly useful to a project team that is working to develop a new food product. If the standard nutrition facts and category can be enterered, the resulting serving size should be fairly close to similar products on the market in that category, and can be useful in the development process. This way, researchers have a good idea of what their final servings size might be, and can have a head start on things like branding and marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main recommendations for this project would be to conduct further analysis of the vectorized ingredients. Utilizing the ingredients as features did not appear to help our models in the current scope, though with more time to optimize, they may be able to improve the model further. A more complex method of cleaning and vectorizing this feature may allow it to be more effectively be incorporated into the model. Further use of NLP could be use to analyze the food descriptions as well.\n",
    "\n",
    "Another interesting step to take would be to perform PCA on the nutrient features. This would be able to definitively eliminate any possibility of multicolinearity in the X variables. It would also ensure that only the most valuable elements of each of the features were being effectively used to make the prediction.\n",
    "\n",
    "Further utilization of the neural network grid search function. The project timeline and available computing power were severe limits on fully fleshing out the neural network. Of course the possibilities are truly endless, but even within the scope of layers, nodes, and regularization that were searched through here, there is most likely further optimization that could be done.\n",
    "\n",
    "Another modeling option could be to use a support vector machine (SVM), as this may be a more fitting model for this type of data. Unfortunately, due to the large amount of data and features being used, there was once again a computing power issue preventing this type of model from being run.\n",
    "\n",
    "Overall, the computational issues in this project could possibly be overcome by utilizing cloud computing services, such as those available on Amazon Web Services, or perhaps even just using a more powerful home computer.\n",
    "\n",
    "The last recommendation would be to continually maintain the dataset, as the USDA periodically updates the information stored in it. This could entail downloading the zip file when a new version comes out, or it could be linked to the API access port to continually update."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
