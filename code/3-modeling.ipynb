{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Food Serving Sizes Through Nutrition Profiles\n",
    "\n",
    "### Notebook 3 - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Importing Packages](#Importing-Packages)\n",
    "2. [Reading Data](#Reading-Data)\n",
    "3. [Feature Engineering](#Feature-Engineering)\n",
    "4. [Preprocessing](#Preprocessing)\n",
    "5. [Modeling](#Modeling)\n",
    "    1. [Baseline Model](#Baseline-Model)\n",
    "    2. [Linear Regression](#Linear-Regression)\n",
    "    3. [Ridge Regression](#Ridge-Regression)\n",
    "    4. [LASSO Regression](#LASSO-Regression)\n",
    "    5. [Neural Network](#Neural-Network)\n",
    "6. [Model Evaluations](#Model-Evaluations)\n",
    "    1. [R<sup>2</sup> Score](#R2-Score)\n",
    "    2. [Mean Squared Error](#Mean-Squared-Error)\n",
    "    3. [Visualizations](#Visualizations)\n",
    "7. [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "    1. [Conclusions](#Conclusions)\n",
    "    2. [Recommendations](#Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools/visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# imports for NLP\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# imports for modeling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "# making the magic happen for plots\n",
    "%matplotlib inline\n",
    "\n",
    "# setting options for better viewing\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "# setting global random seeds for numpy and tensorflow\n",
    "# np.random.seed(42)\n",
    "# set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.read_csv(\"../datasets/clean_foods_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>brand_owner</th>\n",
       "      <th>branded_food_category</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>energy</th>\n",
       "      <th>fat_total</th>\n",
       "      <th>fat_sat</th>\n",
       "      <th>fat_trans</th>\n",
       "      <th>chol</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fiber</th>\n",
       "      <th>sugars</th>\n",
       "      <th>sodium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356425</td>\n",
       "      <td>G. T. Japan, Inc.</td>\n",
       "      <td>Ice Cream &amp; Frozen Yogurt</td>\n",
       "      <td>MOCHI ICE CREAM BONBONS</td>\n",
       "      <td>ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1 PIECE</td>\n",
       "      <td>200.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356426</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>CHIPOTLE BARBECUE SAUCE</td>\n",
       "      <td>WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356427</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>HOT &amp; SPICY BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.29</td>\n",
       "      <td>676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356428</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.57</td>\n",
       "      <td>971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>356429</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>Ketchup, Mustard, BBQ &amp; Cheese Sauce</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>757.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fdc_id        brand_owner                 branded_food_category  \\\n",
       "0  356425  G. T. Japan, Inc.             Ice Cream & Frozen Yogurt   \n",
       "1  356426       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "2  356427       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "3  356428       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "4  356429       FRESH & EASY  Ketchup, Mustard, BBQ & Cheese Sauce   \n",
       "\n",
       "                  description  \\\n",
       "0     MOCHI ICE CREAM BONBONS   \n",
       "1     CHIPOTLE BARBECUE SAUCE   \n",
       "2  HOT & SPICY BARBECUE SAUCE   \n",
       "3              BARBECUE SAUCE   \n",
       "4              BARBECUE SAUCE   \n",
       "\n",
       "                                         ingredients  serving_size  \\\n",
       "0  ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...          40.0   \n",
       "1  WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...          37.0   \n",
       "2  SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...          34.0   \n",
       "3  TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...          35.0   \n",
       "4  SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...          37.0   \n",
       "\n",
       "  household_serving_fulltext  energy  fat_total  fat_sat  fat_trans  chol  \\\n",
       "0                    1 PIECE   200.0       6.25     3.75        0.0  25.0   \n",
       "1                     2 Tbsp   162.0       0.00     0.00        0.0   0.0   \n",
       "2                     2 Tbsp   176.0       0.00     0.00        0.0   0.0   \n",
       "3                     2 Tbsp   143.0       0.00     0.00        0.0   0.0   \n",
       "4                     2 Tbsp   189.0       0.00     0.00        0.0   0.0   \n",
       "\n",
       "   protein  carbs  fiber  sugars  sodium  \n",
       "0      2.5  35.00    0.0   30.00    75.0  \n",
       "1      0.0  43.24    0.0   37.84   703.0  \n",
       "2      0.0  41.18    0.0   35.29   676.0  \n",
       "3      0.0  34.29    0.0   28.57   971.0  \n",
       "4      0.0  45.95    0.0   43.24   757.0  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the modeling process begins, we need to fully prepare the dataset. In the previous notebook we had seen the tailed distribution of the serving size target, so we will make a new column here to use as the target for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods[\"log_serv\"] = np.log(foods[\"serving_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use the category of the food product as a feature, we will have to turn them into dummy columns first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.get_dummies(data=foods,\n",
    "                       columns=[\"branded_food_category\"],\n",
    "                       prefix=\"cat\",\n",
    "                       drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>brand_owner</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>energy</th>\n",
       "      <th>fat_total</th>\n",
       "      <th>fat_sat</th>\n",
       "      <th>fat_trans</th>\n",
       "      <th>chol</th>\n",
       "      <th>protein</th>\n",
       "      <th>carbs</th>\n",
       "      <th>fiber</th>\n",
       "      <th>sugars</th>\n",
       "      <th>sodium</th>\n",
       "      <th>log_serv</th>\n",
       "      <th>cat_All Noodles</th>\n",
       "      <th>cat_Bacon, Sausages &amp; Ribs</th>\n",
       "      <th>cat_Baking</th>\n",
       "      <th>cat_Baking Accessories</th>\n",
       "      <th>cat_Baking Additives &amp; Extracts</th>\n",
       "      <th>cat_Baking Decorations &amp; Dessert Toppings</th>\n",
       "      <th>cat_Baking/Cooking Mixes (Perishable)</th>\n",
       "      <th>cat_Baking/Cooking Mixes (Shelf Stable)</th>\n",
       "      <th>cat_Baking/Cooking Mixes/Supplies Variety Packs</th>\n",
       "      <th>cat_Baking/Cooking Supplies (Shelf Stable)</th>\n",
       "      <th>cat_Beef - Prepared/Processed</th>\n",
       "      <th>cat_Biscuits/Cookies (Shelf Stable)</th>\n",
       "      <th>cat_Bread &amp; Muffin Mixes</th>\n",
       "      <th>cat_Breads &amp; Buns</th>\n",
       "      <th>cat_Breakfast Drinks</th>\n",
       "      <th>cat_Breakfast Foods</th>\n",
       "      <th>cat_Breakfast Sandwiches, Biscuits &amp; Meals</th>\n",
       "      <th>cat_Butter &amp; Spread</th>\n",
       "      <th>cat_Cake, Cookie &amp; Cupcake Mixes</th>\n",
       "      <th>cat_Cakes - Sweet (Frozen)</th>\n",
       "      <th>cat_Cakes - Sweet (Shelf Stable)</th>\n",
       "      <th>cat_Cakes, Cupcakes, Snack Cakes</th>\n",
       "      <th>cat_Candy</th>\n",
       "      <th>cat_Canned &amp; Bottled Beans</th>\n",
       "      <th>cat_Canned Condensed Soup</th>\n",
       "      <th>cat_Canned Fruit</th>\n",
       "      <th>cat_Canned Meat</th>\n",
       "      <th>cat_Canned Seafood</th>\n",
       "      <th>cat_Canned Soup</th>\n",
       "      <th>cat_Canned Tuna</th>\n",
       "      <th>cat_Canned Vegetables</th>\n",
       "      <th>cat_Cereal</th>\n",
       "      <th>cat_Cereal/Muesli Bars</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_Pancakes, Waffles, French Toast &amp; Crepes</th>\n",
       "      <th>cat_Pasta Dinners</th>\n",
       "      <th>cat_Pasta by Shape &amp; Type</th>\n",
       "      <th>cat_Pasta/Noodles - Not Ready to Eat (Frozen)</th>\n",
       "      <th>cat_Pastry Shells &amp; Fillings</th>\n",
       "      <th>cat_Pepperoni, Salami &amp; Cold Cuts</th>\n",
       "      <th>cat_Pickles, Olives, Peppers &amp; Relishes</th>\n",
       "      <th>cat_Pies/Pastries - Sweet (Shelf Stable)</th>\n",
       "      <th>cat_Pies/Pastries/Pizzas/Quiches - Savoury (Frozen)</th>\n",
       "      <th>cat_Pizza</th>\n",
       "      <th>cat_Pizza Mixes &amp; Other Dry Dinners</th>\n",
       "      <th>cat_Plant Based Milk</th>\n",
       "      <th>cat_Plant Based Water</th>\n",
       "      <th>cat_Popcorn (Shelf Stable)</th>\n",
       "      <th>cat_Popcorn, Peanuts, Seeds &amp; Related Snacks</th>\n",
       "      <th>cat_Pork Sausages - Prepared/Processed</th>\n",
       "      <th>cat_Poultry, Chicken &amp; Turkey</th>\n",
       "      <th>cat_Powdered Drinks</th>\n",
       "      <th>cat_Pre-Packaged Fruit &amp; Vegetables</th>\n",
       "      <th>cat_Prepared Pasta &amp; Pizza Sauces</th>\n",
       "      <th>cat_Prepared Subs &amp; Sandwiches</th>\n",
       "      <th>cat_Prepared Wraps and Burittos</th>\n",
       "      <th>cat_Processed Cheese &amp; Cheese Novelties</th>\n",
       "      <th>cat_Puddings &amp; Custards</th>\n",
       "      <th>cat_Rice</th>\n",
       "      <th>cat_Salad Dressing &amp; Mayonnaise</th>\n",
       "      <th>cat_Sausages, Hotdogs &amp; Brats</th>\n",
       "      <th>cat_Seasoning Mixes, Salts, Marinades &amp; Tenderizers</th>\n",
       "      <th>cat_Snack, Energy &amp; Granola Bars</th>\n",
       "      <th>cat_Soda</th>\n",
       "      <th>cat_Soups - Prepared (Shelf Stable)</th>\n",
       "      <th>cat_Specialty Formula Supplements</th>\n",
       "      <th>cat_Sport Drinks</th>\n",
       "      <th>cat_Stuffing</th>\n",
       "      <th>cat_Sushi</th>\n",
       "      <th>cat_Syrups &amp; Molasses</th>\n",
       "      <th>cat_Tea Bags</th>\n",
       "      <th>cat_Tomatoes</th>\n",
       "      <th>cat_Vegetable &amp; Cooking Oils</th>\n",
       "      <th>cat_Vegetable Based Products / Meals - Not Ready to Eat (Frozen)</th>\n",
       "      <th>cat_Vegetable and Lentil Mixes</th>\n",
       "      <th>cat_Vegetables - Prepared/Processed (Frozen)</th>\n",
       "      <th>cat_Vegetables - Prepared/Processed (Shelf Stable)</th>\n",
       "      <th>cat_Vegetarian Frozen Meats</th>\n",
       "      <th>cat_Vitamins</th>\n",
       "      <th>cat_Water</th>\n",
       "      <th>cat_Weight Control</th>\n",
       "      <th>cat_Wholesome Snacks</th>\n",
       "      <th>cat_Yogurt</th>\n",
       "      <th>cat_Yogurt/Yogurt Substitutes (Perishable)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356425</td>\n",
       "      <td>G. T. Japan, Inc.</td>\n",
       "      <td>MOCHI ICE CREAM BONBONS</td>\n",
       "      <td>ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1 PIECE</td>\n",
       "      <td>200.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356426</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>CHIPOTLE BARBECUE SAUCE</td>\n",
       "      <td>WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.84</td>\n",
       "      <td>703.0</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356427</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>HOT &amp; SPICY BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.29</td>\n",
       "      <td>676.0</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356428</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.57</td>\n",
       "      <td>971.0</td>\n",
       "      <td>3.555348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>356429</td>\n",
       "      <td>FRESH &amp; EASY</td>\n",
       "      <td>BARBECUE SAUCE</td>\n",
       "      <td>SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2 Tbsp</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>757.0</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fdc_id        brand_owner                 description  \\\n",
       "0  356425  G. T. Japan, Inc.     MOCHI ICE CREAM BONBONS   \n",
       "1  356426       FRESH & EASY     CHIPOTLE BARBECUE SAUCE   \n",
       "2  356427       FRESH & EASY  HOT & SPICY BARBECUE SAUCE   \n",
       "3  356428       FRESH & EASY              BARBECUE SAUCE   \n",
       "4  356429       FRESH & EASY              BARBECUE SAUCE   \n",
       "\n",
       "                                         ingredients  serving_size  \\\n",
       "0  ICE CREAM INGREDIENTS: MILK, CREAM, SUGAR, STR...          40.0   \n",
       "1  WATER, SUGAR, TOMATO PASTE, MOLASSES, DISTILLE...          37.0   \n",
       "2  SUGAR, WATER, DISTILLED VINEGAR, TOMATO PASTE,...          34.0   \n",
       "3  TOMATO PUREE (WATER, TOMATO PASTE), SUGAR, DIS...          35.0   \n",
       "4  SUGAR, DISTILLED VINEGAR, WATER, TOMATO PASTE,...          37.0   \n",
       "\n",
       "  household_serving_fulltext  energy  fat_total  fat_sat  fat_trans  chol  \\\n",
       "0                    1 PIECE   200.0       6.25     3.75        0.0  25.0   \n",
       "1                     2 Tbsp   162.0       0.00     0.00        0.0   0.0   \n",
       "2                     2 Tbsp   176.0       0.00     0.00        0.0   0.0   \n",
       "3                     2 Tbsp   143.0       0.00     0.00        0.0   0.0   \n",
       "4                     2 Tbsp   189.0       0.00     0.00        0.0   0.0   \n",
       "\n",
       "   protein  carbs  fiber  sugars  sodium  log_serv  cat_All Noodles  \\\n",
       "0      2.5  35.00    0.0   30.00    75.0  3.688879                0   \n",
       "1      0.0  43.24    0.0   37.84   703.0  3.610918                0   \n",
       "2      0.0  41.18    0.0   35.29   676.0  3.526361                0   \n",
       "3      0.0  34.29    0.0   28.57   971.0  3.555348                0   \n",
       "4      0.0  45.95    0.0   43.24   757.0  3.610918                0   \n",
       "\n",
       "   cat_Bacon, Sausages & Ribs  cat_Baking  cat_Baking Accessories  \\\n",
       "0                           0           0                       0   \n",
       "1                           0           0                       0   \n",
       "2                           0           0                       0   \n",
       "3                           0           0                       0   \n",
       "4                           0           0                       0   \n",
       "\n",
       "   cat_Baking Additives & Extracts  cat_Baking Decorations & Dessert Toppings  \\\n",
       "0                                0                                          0   \n",
       "1                                0                                          0   \n",
       "2                                0                                          0   \n",
       "3                                0                                          0   \n",
       "4                                0                                          0   \n",
       "\n",
       "   cat_Baking/Cooking Mixes (Perishable)  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   cat_Baking/Cooking Mixes (Shelf Stable)  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   cat_Baking/Cooking Mixes/Supplies Variety Packs  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   cat_Baking/Cooking Supplies (Shelf Stable)  cat_Beef - Prepared/Processed  \\\n",
       "0                                           0                              0   \n",
       "1                                           0                              0   \n",
       "2                                           0                              0   \n",
       "3                                           0                              0   \n",
       "4                                           0                              0   \n",
       "\n",
       "   cat_Biscuits/Cookies (Shelf Stable)  cat_Bread & Muffin Mixes  \\\n",
       "0                                    0                         0   \n",
       "1                                    0                         0   \n",
       "2                                    0                         0   \n",
       "3                                    0                         0   \n",
       "4                                    0                         0   \n",
       "\n",
       "   cat_Breads & Buns  cat_Breakfast Drinks  cat_Breakfast Foods  \\\n",
       "0                  0                     0                    0   \n",
       "1                  0                     0                    0   \n",
       "2                  0                     0                    0   \n",
       "3                  0                     0                    0   \n",
       "4                  0                     0                    0   \n",
       "\n",
       "   cat_Breakfast Sandwiches, Biscuits & Meals  cat_Butter & Spread  \\\n",
       "0                                           0                    0   \n",
       "1                                           0                    0   \n",
       "2                                           0                    0   \n",
       "3                                           0                    0   \n",
       "4                                           0                    0   \n",
       "\n",
       "   cat_Cake, Cookie & Cupcake Mixes  cat_Cakes - Sweet (Frozen)  \\\n",
       "0                                 0                           0   \n",
       "1                                 0                           0   \n",
       "2                                 0                           0   \n",
       "3                                 0                           0   \n",
       "4                                 0                           0   \n",
       "\n",
       "   cat_Cakes - Sweet (Shelf Stable)  cat_Cakes, Cupcakes, Snack Cakes  \\\n",
       "0                                 0                                 0   \n",
       "1                                 0                                 0   \n",
       "2                                 0                                 0   \n",
       "3                                 0                                 0   \n",
       "4                                 0                                 0   \n",
       "\n",
       "   cat_Candy  cat_Canned & Bottled Beans  cat_Canned Condensed Soup  \\\n",
       "0          0                           0                          0   \n",
       "1          0                           0                          0   \n",
       "2          0                           0                          0   \n",
       "3          0                           0                          0   \n",
       "4          0                           0                          0   \n",
       "\n",
       "   cat_Canned Fruit  cat_Canned Meat  cat_Canned Seafood  cat_Canned Soup  \\\n",
       "0                 0                0                   0                0   \n",
       "1                 0                0                   0                0   \n",
       "2                 0                0                   0                0   \n",
       "3                 0                0                   0                0   \n",
       "4                 0                0                   0                0   \n",
       "\n",
       "   cat_Canned Tuna  cat_Canned Vegetables  cat_Cereal  cat_Cereal/Muesli Bars  \\\n",
       "0                0                      0           0                       0   \n",
       "1                0                      0           0                       0   \n",
       "2                0                      0           0                       0   \n",
       "3                0                      0           0                       0   \n",
       "4                0                      0           0                       0   \n",
       "\n",
       "   ...  cat_Pancakes, Waffles, French Toast & Crepes  cat_Pasta Dinners  \\\n",
       "0  ...                                             0                  0   \n",
       "1  ...                                             0                  0   \n",
       "2  ...                                             0                  0   \n",
       "3  ...                                             0                  0   \n",
       "4  ...                                             0                  0   \n",
       "\n",
       "   cat_Pasta by Shape & Type  cat_Pasta/Noodles - Not Ready to Eat (Frozen)  \\\n",
       "0                          0                                              0   \n",
       "1                          0                                              0   \n",
       "2                          0                                              0   \n",
       "3                          0                                              0   \n",
       "4                          0                                              0   \n",
       "\n",
       "   cat_Pastry Shells & Fillings  cat_Pepperoni, Salami & Cold Cuts  \\\n",
       "0                             0                                  0   \n",
       "1                             0                                  0   \n",
       "2                             0                                  0   \n",
       "3                             0                                  0   \n",
       "4                             0                                  0   \n",
       "\n",
       "   cat_Pickles, Olives, Peppers & Relishes  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   cat_Pies/Pastries - Sweet (Shelf Stable)  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "\n",
       "   cat_Pies/Pastries/Pizzas/Quiches - Savoury (Frozen)  cat_Pizza  \\\n",
       "0                                                  0            0   \n",
       "1                                                  0            0   \n",
       "2                                                  0            0   \n",
       "3                                                  0            0   \n",
       "4                                                  0            0   \n",
       "\n",
       "   cat_Pizza Mixes & Other Dry Dinners  cat_Plant Based Milk  \\\n",
       "0                                    0                     0   \n",
       "1                                    0                     0   \n",
       "2                                    0                     0   \n",
       "3                                    0                     0   \n",
       "4                                    0                     0   \n",
       "\n",
       "   cat_Plant Based Water  cat_Popcorn (Shelf Stable)  \\\n",
       "0                      0                           0   \n",
       "1                      0                           0   \n",
       "2                      0                           0   \n",
       "3                      0                           0   \n",
       "4                      0                           0   \n",
       "\n",
       "   cat_Popcorn, Peanuts, Seeds & Related Snacks  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   cat_Pork Sausages - Prepared/Processed  cat_Poultry, Chicken & Turkey  \\\n",
       "0                                       0                              0   \n",
       "1                                       0                              0   \n",
       "2                                       0                              0   \n",
       "3                                       0                              0   \n",
       "4                                       0                              0   \n",
       "\n",
       "   cat_Powdered Drinks  cat_Pre-Packaged Fruit & Vegetables  \\\n",
       "0                    0                                    0   \n",
       "1                    0                                    0   \n",
       "2                    0                                    0   \n",
       "3                    0                                    0   \n",
       "4                    0                                    0   \n",
       "\n",
       "   cat_Prepared Pasta & Pizza Sauces  cat_Prepared Subs & Sandwiches  \\\n",
       "0                                  0                               0   \n",
       "1                                  0                               0   \n",
       "2                                  0                               0   \n",
       "3                                  0                               0   \n",
       "4                                  0                               0   \n",
       "\n",
       "   cat_Prepared Wraps and Burittos  cat_Processed Cheese & Cheese Novelties  \\\n",
       "0                                0                                        0   \n",
       "1                                0                                        0   \n",
       "2                                0                                        0   \n",
       "3                                0                                        0   \n",
       "4                                0                                        0   \n",
       "\n",
       "   cat_Puddings & Custards  cat_Rice  cat_Salad Dressing & Mayonnaise  \\\n",
       "0                        0         0                                0   \n",
       "1                        0         0                                0   \n",
       "2                        0         0                                0   \n",
       "3                        0         0                                0   \n",
       "4                        0         0                                0   \n",
       "\n",
       "   cat_Sausages, Hotdogs & Brats  \\\n",
       "0                              0   \n",
       "1                              0   \n",
       "2                              0   \n",
       "3                              0   \n",
       "4                              0   \n",
       "\n",
       "   cat_Seasoning Mixes, Salts, Marinades & Tenderizers  \\\n",
       "0                                                  0     \n",
       "1                                                  0     \n",
       "2                                                  0     \n",
       "3                                                  0     \n",
       "4                                                  0     \n",
       "\n",
       "   cat_Snack, Energy & Granola Bars  cat_Soda  \\\n",
       "0                                 0         0   \n",
       "1                                 0         0   \n",
       "2                                 0         0   \n",
       "3                                 0         0   \n",
       "4                                 0         0   \n",
       "\n",
       "   cat_Soups - Prepared (Shelf Stable)  cat_Specialty Formula Supplements  \\\n",
       "0                                    0                                  0   \n",
       "1                                    0                                  0   \n",
       "2                                    0                                  0   \n",
       "3                                    0                                  0   \n",
       "4                                    0                                  0   \n",
       "\n",
       "   cat_Sport Drinks  cat_Stuffing  cat_Sushi  cat_Syrups & Molasses  \\\n",
       "0                 0             0          0                      0   \n",
       "1                 0             0          0                      0   \n",
       "2                 0             0          0                      0   \n",
       "3                 0             0          0                      0   \n",
       "4                 0             0          0                      0   \n",
       "\n",
       "   cat_Tea Bags  cat_Tomatoes  cat_Vegetable & Cooking Oils  \\\n",
       "0             0             0                             0   \n",
       "1             0             0                             0   \n",
       "2             0             0                             0   \n",
       "3             0             0                             0   \n",
       "4             0             0                             0   \n",
       "\n",
       "   cat_Vegetable Based Products / Meals - Not Ready to Eat (Frozen)  \\\n",
       "0                                                  0                  \n",
       "1                                                  0                  \n",
       "2                                                  0                  \n",
       "3                                                  0                  \n",
       "4                                                  0                  \n",
       "\n",
       "   cat_Vegetable and Lentil Mixes  \\\n",
       "0                               0   \n",
       "1                               0   \n",
       "2                               0   \n",
       "3                               0   \n",
       "4                               0   \n",
       "\n",
       "   cat_Vegetables - Prepared/Processed (Frozen)  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   cat_Vegetables - Prepared/Processed (Shelf Stable)  \\\n",
       "0                                                  0    \n",
       "1                                                  0    \n",
       "2                                                  0    \n",
       "3                                                  0    \n",
       "4                                                  0    \n",
       "\n",
       "   cat_Vegetarian Frozen Meats  cat_Vitamins  cat_Water  cat_Weight Control  \\\n",
       "0                            0             0          0                   0   \n",
       "1                            0             0          0                   0   \n",
       "2                            0             0          0                   0   \n",
       "3                            0             0          0                   0   \n",
       "4                            0             0          0                   0   \n",
       "\n",
       "   cat_Wholesome Snacks  cat_Yogurt  \\\n",
       "0                     0           0   \n",
       "1                     0           0   \n",
       "2                     0           0   \n",
       "3                     0           0   \n",
       "4                     0           0   \n",
       "\n",
       "   cat_Yogurt/Yogurt Substitutes (Perishable)  \n",
       "0                                           0  \n",
       "1                                           0  \n",
       "2                                           0  \n",
       "3                                           0  \n",
       "4                                           0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for new cols\n",
    "foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessing steps here are to split our dataset into the X features and y target, and then scale the features so they can be flexibly used across different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y vars\n",
    "X = foods.drop(columns=[\"fdc_id\", \n",
    "                        \"brand_owner\", \n",
    "                        \"description\", \n",
    "                        \"ingredients\", \n",
    "                        \"household_serving_fulltext\",\n",
    "                        \"serving_size\",\n",
    "                       \"log_serv\"])\n",
    "y = foods[\"log_serv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# instantiating scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# scaling X data\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been fully prepared, we can beign the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way of predicting serving size without using any machine learning, by simply using the mean of the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Train MSE Score for our Base Model is: 3685.336822662155\n",
      "Our Test MSE Score for our Base Model is: 3833.116046199187\n"
     ]
    }
   ],
   "source": [
    "# building a base model to compare results against\n",
    "# using code from Boom's Local Session\n",
    "\n",
    "# Instantiate: creates a dummy regression that always predicts the mean of the target\n",
    "base_mean = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Fit the \"model\"\n",
    "base_mean = base_mean.fit(X_train_sc, y_train)\n",
    "\n",
    "# Get predictions for our testing set (not kaggle testing set)\n",
    "y_hat_base_train = base_mean.predict(X_train_sc)\n",
    "y_hat_base_test = base_mean.predict(X_test_sc)\n",
    "\n",
    "# Get R2\n",
    "print(\"Our Train MSE Score for our Base Model is:\", metrics.mean_squared_error(np.exp(y_train),np.exp(y_hat_base_train)))\n",
    "print(\"Our Test MSE Score for our Base Model is:\",  metrics.mean_squared_error(np.exp(y_test),np.exp(y_hat_base_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The help find the best possible model parameters, we should build a function to perform gridsearches for us. We can then implement this function on any model type and set of parameters to optimize each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in func from Reddit NLP project\n",
    "def grid_searcher(pipe, params):\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=params, cv=3, verbose=1, n_jobs=3)\n",
    "    gs.fit(X_train_sc, y_train)\n",
    "    print(f'CrossVal Score: {gs.best_score_}')\n",
    "    print(f'Training Score: {gs.score(X_train_sc, y_train)}')\n",
    "    print(f'Testing Score: {gs.score(X_test_sc, y_test)}')\n",
    "    print(gs.best_params_)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As linear regression is a very simple model, there is really no hyperparameter searching that needs to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg_model = linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train R2 score is: 0.7353463856717573.\n",
      "The test R2 score is: 0.7406407151042635.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The train R2 score is: {linreg_model.score(X_train, y_train)}.\")\n",
    "print(f\"The test R2 score is: {linreg_model.score(X_test, y_test)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic linear regression model is not scoring very high, although it is also showing a fairly low variance. Since we did see some multicolinearity in a few of the features during the EDA, it may be worth looking at a LASSO or ridge regression for some improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `LassoCV` model already iterates through a list of 100 alphas, we do not need torun the gridsearching function on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jondov/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV()\n",
    "lasso_model = lasso.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train R2 score is: 0.7351460362337019.\n",
      "The test R2 score is: 0.7403776480182412.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The train R2 score is: {lasso_model.score(X_train_sc, y_train)}.\")\n",
    "print(f\"The test R2 score is: {lasso_model.score(X_test_sc, y_test)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe = Pipeline([(\"ridge\", Ridge())])\n",
    "\n",
    "ridge_params = {\n",
    "    \"ridge__alpha\": [11, 12, 13]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossVal Score: 0.7338202830606281\n",
      "Training Score: 0.7353441884095184\n",
      "Testing Score: 0.7406257916854004\n",
      "{'ridge__alpha': 12}\n"
     ]
    }
   ],
   "source": [
    "ridge_model = grid_searcher(ridge_pipe, ridge_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of these regularization methods had any large impact on the R<sup>2</sup> scores. These model are typically most helpful to regularize overfit linear regressions, and as our initial model was not very overfit, they may not just not be enough of an improvement. We may need to utilize a more advanced regression method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As using a Neural Network was showing promise as the best model type to use, we should have a way of effectively optimizing it. While a Keras Sequential model cannot be passed through Scikit-learn's `GridSearchCV`, we can make our own custom function to essentially perform the same type of task. To that end, the following functions were created, which were adapted from code written by Mahdi Shadkam-Farrokhi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function to be made is for establishing the permutations of parameters. This fucntion will take in a dictionary of all the layer parameters, and unpacks each combination into a distinct set of single parameters that a model can be built on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate_params(grid_params):\n",
    "    # returns a list of all combinations of unique parameters from the given dictionary\n",
    "    out = [{}]\n",
    "    for param_name, param_list in grid_params.items():\n",
    "        if len(param_list) == 1:\n",
    "            for item in out:\n",
    "                item[param_name] = param_list[0]\n",
    "        else:\n",
    "            temp_out = []\n",
    "            for param_val in param_list:\n",
    "                for item in out:\n",
    "                    cloned_item = item.copy()\n",
    "                    cloned_item[param_name] = param_val\n",
    "                    temp_out.append(cloned_item)\n",
    "            out = temp_out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece is to take a dictionary of parameters and build a functioning model out of it. We will make a model building fucntion to perfrom this action. It will take our train/test split data, and a dictionary of the parameters that has the specified keys. The function will reach into each of the keys for the appropriate attribute, and then apply that to the coresponding layer. To increase flexibility, there will be default settings for each of the parameters, so that the dictioanry does not have to contain entries for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params_dict, X_train, X_test, y_train, y_test):\n",
    "    # defining params\n",
    "    first_layer_nodes = params_dict.get(\"first_layer_nodes\") or 16            # default low nodes\n",
    "    first_dropout_rate = params_dict.get(\"first_dropout_rate\") or 0.0         # default no dropout\n",
    "    \n",
    "    second_layer_nodes = params_dict.get(\"second_layer_nodes\") or 16          # default low nodes\n",
    "    second_dropout_rate = params_dict.get(\"second_dropout_rate\") or 0.0       # default no dropout\n",
    "    \n",
    "    third_layer_nodes = params_dict.get(\"third_layer_nodes\") or 16            # default low nodes\n",
    "    third_dropout_rate = params_dict.get(\"third_dropout_rate\") or 0.0         # default no dropout  \n",
    "    \n",
    "    reg = params_dict.get(\"reg\") or 0                                         # default no reg\n",
    "    \n",
    "    epochs = params_dict.get(\"epochs\") or 10                                  # default low epochs\n",
    "    batch_size = params_dict.get(\"batch_size\") or 1024                        # default large batch\n",
    "    early_stop = params_dict.get(\"early_stop\") or EarlyStopping(monitor=\"val_loss\",\n",
    "                                                                min_delta=0.000000001,  # small delta\n",
    "                                                                patience=100)           # large patience\n",
    "    \n",
    "    # instantiating model\n",
    "    model = Sequential()\n",
    "\n",
    "    # adding layers according to inputs\n",
    "    # first layer(s)\n",
    "    model.add(Dense(first_layer_nodes,\n",
    "                   activation=\"relu\",\n",
    "                   input_shape=(X_train.shape[1],),\n",
    "                   kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(first_dropout_rate))\n",
    "    \n",
    "    # second layer(s)\n",
    "    model.add(Dense(second_layer_nodes,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(second_dropout_rate))\n",
    "    \n",
    "    # third layer(s)\n",
    "    model.add(Dense(third_layer_nodes,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(third_dropout_rate))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compiling model\n",
    "    model.compile(loss=\"mean_squared_error\",\n",
    "             optimizer=\"adam\")\n",
    "    \n",
    "    # fitting model according to inputs\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                       callbacks=[early_stop])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece is to combine the two above functions into one functioan that will take and X and y, along with the parameter dictionary, and do the rest of the work for us. This function performs a train/test split, scales the data, and the runs the data through each of the models fit within the parameter dictionary. Each iteration will check the determined metric, and updates it if it has been improved. At the end of the function, the model that gave the best score for the metric is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_grid_search(\n",
    "    X,\n",
    "    y,\n",
    "    grid_params,\n",
    "    random_state=42\n",
    "):\n",
    "    ### this will make a series of FFNN models \n",
    "    ### and return the one with the best score as set below\n",
    "    ### currently set to test r2 score\n",
    "    \n",
    "    # list of all parameter combinations\n",
    "    all_params = permutate_params(grid_params)\n",
    "    \n",
    "    # creating vars with to update each iter\n",
    "    best_model = None\n",
    "    best_score = 0.0 # no accuracy to start\n",
    "    best_params = None\n",
    "    best_history = None\n",
    "    \n",
    "    # train/test split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    # scaling data\n",
    "    ss = StandardScaler()\n",
    "    X_train_sc = ss.fit_transform(X_train)\n",
    "    X_test_sc = ss.transform(X_test)\n",
    "    \n",
    "    # looping through the unpacked parameter list\n",
    "    for i, params in enumerate(all_params):\n",
    "        \n",
    "        # keeping track of which model we're running\n",
    "        print(f\"Building model {i + 1} of {len(all_params)}\")\n",
    "        \n",
    "        # bulding the model\n",
    "        model, history = build_model(\n",
    "            params_dict = params,\n",
    "            X_train = X_train_sc, \n",
    "            X_test = X_test_sc, \n",
    "            y_train = y_train, \n",
    "            y_test = y_test\n",
    "        )\n",
    "        \n",
    "        # making preds and scoring\n",
    "        test_preds = model.predict(X_test_sc)\n",
    "        score = metrics.r2_score(y_test, test_preds)\n",
    "        \n",
    "        # checking if the score beats the current best\n",
    "        # updates vars if true\n",
    "        if score > best_score:\n",
    "            print(\"***Good R2 found: {:.2%}***\".format(score))\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "            best_history = history\n",
    "    \n",
    "    # loop is done, return the best model\n",
    "    return {\n",
    "        \"best_model\"   : best_model,\n",
    "        \"best_score\"   : best_score,\n",
    "        \"best_params\"  : best_params,\n",
    "        \"best_history\" : best_history,\n",
    "        \"test_preds\"   : test_preds\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions built, we can run the search. We will start with finding the best number of nodes out of three hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting params\n",
    "node_params = {\n",
    "    \"first_layer_nodes\": [256, 128, 64],\n",
    "    \"second_layer_nodes\": [128, 64, 32],\n",
    "    \"third_layer_nodes\": [64, 32, 16],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model 1 of 27\n",
      "WARNING:tensorflow:From /Users/jondov/anaconda3/envs/dsi/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/jondov/anaconda3/envs/dsi/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 1.8480 - val_loss: 0.2177\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.2511 - val_loss: 0.1866\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1780 - val_loss: 0.1720\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1655 - val_loss: 0.1588\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1560 - val_loss: 0.1517\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1503 - val_loss: 0.1508\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1458 - val_loss: 0.1423\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1418 - val_loss: 0.1390\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1377 - val_loss: 0.1373\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1360 - val_loss: 0.1363\n",
      "***Good R2 found: 86.64%***\n",
      "Building model 2 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 1.6750 - val_loss: 0.2131\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1966 - val_loss: 0.1820\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1701 - val_loss: 0.1625\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1596 - val_loss: 0.1563\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1536 - val_loss: 0.1516\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1500 - val_loss: 0.1506\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1465 - val_loss: 0.1455\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1414 - val_loss: 0.1404\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1385 - val_loss: 0.1381\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1358 - val_loss: 0.1349\n",
      "***Good R2 found: 86.77%***\n",
      "Building model 3 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 1.8887 - val_loss: 0.2434\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.2262 - val_loss: 0.1876\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1775 - val_loss: 0.1716\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1649 - val_loss: 0.1607\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1595 - val_loss: 0.1560\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1584 - val_loss: 0.1604\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1560 - val_loss: 0.1686\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1551 - val_loss: 0.1561\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1495 - val_loss: 0.1476\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1423 - val_loss: 0.1408\n",
      "Building model 4 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 1.1263 - val_loss: 0.2058\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1876 - val_loss: 0.1775\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1688 - val_loss: 0.1633\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1596 - val_loss: 0.1583\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1531 - val_loss: 0.1537\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1511 - val_loss: 0.1490\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1458 - val_loss: 0.1430\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1392 - val_loss: 0.1381\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.1371 - val_loss: 0.1364\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1352 - val_loss: 0.1347\n",
      "***Good R2 found: 86.79%***\n",
      "Building model 5 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 1.4174 - val_loss: 0.2229\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2002 - val_loss: 0.1801\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1747 - val_loss: 0.1670\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1618 - val_loss: 0.1564\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1543 - val_loss: 0.1570\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1522 - val_loss: 0.1539\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1494 - val_loss: 0.1484\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1466 - val_loss: 0.1495\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1467 - val_loss: 0.1412\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1405 - val_loss: 0.1439\n",
      "Building model 6 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 2.0023 - val_loss: 0.3035\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.2403 - val_loss: 0.1988\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1867 - val_loss: 0.1777\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1717 - val_loss: 0.1662\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1634 - val_loss: 0.1605\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1586 - val_loss: 0.1606\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1550 - val_loss: 0.1534\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1504 - val_loss: 0.1475\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1483 - val_loss: 0.1457\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1435 - val_loss: 0.1432\n",
      "Building model 7 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 2.0452 - val_loss: 0.2245\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.2066 - val_loss: 0.1819\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.1757 - val_loss: 0.1665\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 18us/step - loss: 0.1653 - val_loss: 0.1623\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1592 - val_loss: 0.1522\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1502 - val_loss: 0.1477\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1452 - val_loss: 0.1439\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1406 - val_loss: 0.1407\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1368 - val_loss: 0.1380\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1348 - val_loss: 0.1362\n",
      "Building model 8 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 1.3688 - val_loss: 0.2637\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.2189 - val_loss: 0.1904\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1802 - val_loss: 0.1726\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1657 - val_loss: 0.1593\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1578 - val_loss: 0.1545\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1533 - val_loss: 0.1490\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1492 - val_loss: 0.1507\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1463 - val_loss: 0.1430\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1429 - val_loss: 0.1455\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1418 - val_loss: 0.1407\n",
      "Building model 9 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 2.3154 - val_loss: 0.2661\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.2709 - val_loss: 0.1995\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1925 - val_loss: 0.1769\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1718 - val_loss: 0.1672\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1626 - val_loss: 0.1585\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1561 - val_loss: 0.1568\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1522 - val_loss: 0.1510\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1481 - val_loss: 0.1485\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1453 - val_loss: 0.1449\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1419 - val_loss: 0.1441\n",
      "Building model 10 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.9390 - val_loss: 0.2028\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1902 - val_loss: 0.1766\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1731 - val_loss: 0.1704\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1652 - val_loss: 0.1546\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1564 - val_loss: 0.1448\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1457 - val_loss: 0.1430\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1390 - val_loss: 0.1401\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1350 - val_loss: 0.1352\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1312 - val_loss: 0.1321\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1290 - val_loss: 0.1318\n",
      "***Good R2 found: 87.08%***\n",
      "Building model 11 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 1.4023 - val_loss: 0.2055\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1985 - val_loss: 0.1762\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1694 - val_loss: 0.1642\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1592 - val_loss: 0.1569\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1540 - val_loss: 0.1523\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1482 - val_loss: 0.1480\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1445 - val_loss: 0.1440\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1405 - val_loss: 0.1410\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1366 - val_loss: 0.1385\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1343 - val_loss: 0.1378\n",
      "Building model 12 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 1.5955 - val_loss: 0.3439\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.2564 - val_loss: 0.1961\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1840 - val_loss: 0.1755\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1703 - val_loss: 0.1636\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1632 - val_loss: 0.1614\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1608 - val_loss: 0.1592\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1561 - val_loss: 0.1609\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1531 - val_loss: 0.1468\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1470 - val_loss: 0.1490\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1451 - val_loss: 0.1436\n",
      "Building model 13 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.9593 - val_loss: 0.2080\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1920 - val_loss: 0.1777\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1833 - val_loss: 0.1642\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1673 - val_loss: 0.1530\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1594 - val_loss: 0.1512\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1503 - val_loss: 0.1466\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 22us/step - loss: 0.1416 - val_loss: 0.1405\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1365 - val_loss: 0.1391\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1343 - val_loss: 0.1399\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1323 - val_loss: 0.1331\n",
      "Building model 14 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.6498 - val_loss: 0.2255\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2128 - val_loss: 0.1855\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1786 - val_loss: 0.1697\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1663 - val_loss: 0.1644\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1611 - val_loss: 0.1592\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1570 - val_loss: 0.1519\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1522 - val_loss: 0.1479\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1476 - val_loss: 0.1437\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1417 - val_loss: 0.1404\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1365 - val_loss: 0.1378\n",
      "Building model 15 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 1.8320 - val_loss: 0.2708\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.2465 - val_loss: 0.1952\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1865 - val_loss: 0.1779\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1729 - val_loss: 0.1673\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1654 - val_loss: 0.1621\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1617 - val_loss: 0.1564\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1576 - val_loss: 0.1515\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1515 - val_loss: 0.1486\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1490 - val_loss: 0.1451\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1452 - val_loss: 0.1425\n",
      "Building model 16 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 2.1269 - val_loss: 0.2366\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2790 - val_loss: 0.1872\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1836 - val_loss: 0.1707\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1694 - val_loss: 0.1613\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1593 - val_loss: 0.1573\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1548 - val_loss: 0.1539\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1539 - val_loss: 0.1479\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1537 - val_loss: 0.1468\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1431 - val_loss: 0.1461\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1407 - val_loss: 0.1402\n",
      "Building model 17 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 1.8820 - val_loss: 0.2412\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.2116 - val_loss: 0.1860\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1779 - val_loss: 0.1711\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1676 - val_loss: 0.1645\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1597 - val_loss: 0.1583\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1550 - val_loss: 0.1535\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1513 - val_loss: 0.1504\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1474 - val_loss: 0.1500\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1451 - val_loss: 0.1509\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1441 - val_loss: 0.1488\n",
      "Building model 18 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 2.2923 - val_loss: 0.2695\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.2562 - val_loss: 0.1983\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1931 - val_loss: 0.1785\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1741 - val_loss: 0.1697\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1669 - val_loss: 0.1652\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1622 - val_loss: 0.1606\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1586 - val_loss: 0.1551\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1542 - val_loss: 0.1574\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1546 - val_loss: 0.1584\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1500 - val_loss: 0.1486\n",
      "Building model 19 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.3319 - val_loss: 0.2192\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1889 - val_loss: 0.1745\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1696 - val_loss: 0.1620\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1588 - val_loss: 0.1544\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1529 - val_loss: 0.1546\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1515 - val_loss: 0.1508\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1483 - val_loss: 0.1456\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1428 - val_loss: 0.1430\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1383 - val_loss: 0.1394\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.1365 - val_loss: 0.1413\n",
      "Building model 20 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 1.6037 - val_loss: 0.2206\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2216 - val_loss: 0.2699\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1984 - val_loss: 0.1723\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1689 - val_loss: 0.1615\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1585 - val_loss: 0.1543\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1536 - val_loss: 0.1506\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1491 - val_loss: 0.1483\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1448 - val_loss: 0.1455\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1428 - val_loss: 0.1431\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1406 - val_loss: 0.1500\n",
      "Building model 21 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 2.5000 - val_loss: 0.2576\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2320 - val_loss: 0.1971\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1888 - val_loss: 0.1778\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1740 - val_loss: 0.1683\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1660 - val_loss: 0.1623\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1611 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1558 - val_loss: 0.1564\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1515 - val_loss: 0.1495\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1484 - val_loss: 0.1475\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1458 - val_loss: 0.1455\n",
      "Building model 22 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.5932 - val_loss: 0.2246\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.2030 - val_loss: 0.1874\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1801 - val_loss: 0.1745\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1720 - val_loss: 0.1645\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.1644 - val_loss: 0.1574\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1549 - val_loss: 0.1536\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1493 - val_loss: 0.1465\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1455 - val_loss: 0.1441\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1429 - val_loss: 0.1406\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1400 - val_loss: 0.1413\n",
      "Building model 23 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 1.2414 - val_loss: 0.2272\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.2060 - val_loss: 0.1958\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1791 - val_loss: 0.1726\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1636 - val_loss: 0.1579\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1547 - val_loss: 0.1540\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1511 - val_loss: 0.1498\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 3s 21us/step - loss: 0.1463 - val_loss: 0.1441\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1433 - val_loss: 0.1448\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1458 - val_loss: 0.1416\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1364 - val_loss: 0.1404\n",
      "Building model 24 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 2.2707 - val_loss: 0.2776\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.2601 - val_loss: 0.2213\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1947 - val_loss: 0.1782\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1739 - val_loss: 0.1680\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1658 - val_loss: 0.1612\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1601 - val_loss: 0.1583\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1573 - val_loss: 0.1581\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1623 - val_loss: 0.1626\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1538 - val_loss: 0.1523\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1497 - val_loss: 0.1518\n",
      "Building model 25 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 1.4883 - val_loss: 0.3015\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 3s 20us/step - loss: 0.2344 - val_loss: 0.2097\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1785 - val_loss: 0.1678\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1649 - val_loss: 0.1590\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1580 - val_loss: 0.1541\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1510 - val_loss: 0.1490\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1467 - val_loss: 0.1477\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1455 - val_loss: 0.1432\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 17us/step - loss: 0.1404 - val_loss: 0.1399\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 3s 19us/step - loss: 0.1377 - val_loss: 0.1370\n",
      "Building model 26 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 2.2189 - val_loss: 0.2291\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.2075 - val_loss: 0.1898\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1800 - val_loss: 0.1743\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1732 - val_loss: 0.1742\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1668 - val_loss: 0.1631\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 3s 18us/step - loss: 0.1596 - val_loss: 0.1652\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 16us/step - loss: 0.1585 - val_loss: 0.1535\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1532 - val_loss: 0.1621\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 15us/step - loss: 0.1506 - val_loss: 0.1497\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 14us/step - loss: 0.1449 - val_loss: 0.1482\n",
      "Building model 27 of 27\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/10\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 2.7833 - val_loss: 0.4104\n",
      "Epoch 2/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.2693 - val_loss: 0.2190\n",
      "Epoch 3/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1992 - val_loss: 0.1837\n",
      "Epoch 4/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1787 - val_loss: 0.1730\n",
      "Epoch 5/10\n",
      "141859/141859 [==============================] - 2s 13us/step - loss: 0.1694 - val_loss: 0.1664\n",
      "Epoch 6/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1639 - val_loss: 0.1612\n",
      "Epoch 7/10\n",
      "141859/141859 [==============================] - 2s 12us/step - loss: 0.1587 - val_loss: 0.1560\n",
      "Epoch 8/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1546 - val_loss: 0.1531\n",
      "Epoch 9/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1511 - val_loss: 0.1510\n",
      "Epoch 10/10\n",
      "141859/141859 [==============================] - 2s 11us/step - loss: 0.1481 - val_loss: 0.1479\n"
     ]
    }
   ],
   "source": [
    "# best_model = nn_grid_search(X, y, node_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the grid search has completed, we have a list of dictionaries that holds the model that gave use the highest R<sup>2</sup> score, along with the parameters of that model, the score, and the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e83f62407969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take the model that had the best promise, and use those parameters in a new model to find the best amount of epochs for it. After some trials, 75 epochs seemed to be the best number to use, though we were left with some slight overfitting, so an additional search will be performed with some regularization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdict = {\n",
    "    \"first_layer_nodes\": [256],\n",
    "    \"first_dropout_rate\": [0.25, 0.5],\n",
    "    \"second_layer_nodes\": [128],\n",
    "    \"second_dropout_rate\": [0.25, 0.5],\n",
    "    \"third_layer_nodes\": [32],\n",
    "    \"third_dropout_rate\": [0.25, 0.5],\n",
    "    \"epochs\": [75]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model 1 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n",
      "141859/141859 [==============================] - 6s 39us/step - loss: 2.5056 - val_loss: 0.3672\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.9568 - val_loss: 0.2869\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.8180 - val_loss: 0.2617\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.7392 - val_loss: 0.2592\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.6748 - val_loss: 0.2313\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.6359 - val_loss: 0.2231\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.6047 - val_loss: 0.2506\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5729 - val_loss: 0.2350\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.5470 - val_loss: 0.2387\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.5205 - val_loss: 0.2069\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.4967 - val_loss: 0.2032\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.4751 - val_loss: 0.2079\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.4545 - val_loss: 0.1952\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.4353 - val_loss: 0.1961\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.4179 - val_loss: 0.1917\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.4011 - val_loss: 0.1736\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.3843 - val_loss: 0.1657\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.3689 - val_loss: 0.1692\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.3535 - val_loss: 0.1597\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.3412 - val_loss: 0.1582\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.3302 - val_loss: 0.1471\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.3174 - val_loss: 0.1435\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3009 - val_loss: 0.1428\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2931 - val_loss: 0.1458\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2837 - val_loss: 0.1469\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2732 - val_loss: 0.1382\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2641 - val_loss: 0.1343\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2547 - val_loss: 0.1320\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2440 - val_loss: 0.1295\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2376 - val_loss: 0.1331\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2293 - val_loss: 0.1346\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2208 - val_loss: 0.1302\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2146 - val_loss: 0.1284\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.2082 - val_loss: 0.1268\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2030 - val_loss: 0.1222\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1966 - val_loss: 0.1272\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1888 - val_loss: 0.1286\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1865 - val_loss: 0.1233\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1806 - val_loss: 0.1152\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1747 - val_loss: 0.1198\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1725 - val_loss: 0.1161\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1671 - val_loss: 0.1156\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1622 - val_loss: 0.1204\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1586 - val_loss: 0.1169\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1566 - val_loss: 0.1169\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1536 - val_loss: 0.1144\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1510 - val_loss: 0.1189\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1491 - val_loss: 0.1140\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1460 - val_loss: 0.1115\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1435 - val_loss: 0.1081\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1424 - val_loss: 0.1147\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1392 - val_loss: 0.1112\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1378 - val_loss: 0.1062\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1379 - val_loss: 0.1126\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1356 - val_loss: 0.1105\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1337 - val_loss: 0.1112\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1347 - val_loss: 0.1175\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1318 - val_loss: 0.1160\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1332 - val_loss: 0.1033\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1303 - val_loss: 0.1065\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1293 - val_loss: 0.1069\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1292 - val_loss: 0.1074\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1281 - val_loss: 0.1020\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1281 - val_loss: 0.0997\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1266 - val_loss: 0.1015\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1278 - val_loss: 0.1046\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1265 - val_loss: 0.1020\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1265 - val_loss: 0.1042\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1260 - val_loss: 0.1034\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1232 - val_loss: 0.1011\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1255 - val_loss: 0.0990\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1231 - val_loss: 0.1003\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1230 - val_loss: 0.1035\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1237 - val_loss: 0.0983\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1221 - val_loss: 0.1013\n",
      "***Good R2 found: 90.07%***\n",
      "Building model 2 of 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 3.0929 - val_loss: 0.7281\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 1.1999 - val_loss: 0.7055\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.9508 - val_loss: 0.7631\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.8411 - val_loss: 0.7650\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.7586 - val_loss: 0.7721\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.6958 - val_loss: 0.7211\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.6489 - val_loss: 0.7101\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.6075 - val_loss: 0.6782\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.5742 - val_loss: 0.5800\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.5408 - val_loss: 0.5121\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 844s 6ms/step - loss: 0.5122 - val_loss: 0.4474\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 6s 44us/step - loss: 0.4912 - val_loss: 0.3944\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 6s 42us/step - loss: 0.4735 - val_loss: 0.3482\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 6s 42us/step - loss: 0.4489 - val_loss: 0.3066\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 6s 41us/step - loss: 0.4308 - val_loss: 0.2881\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 6s 44us/step - loss: 0.4127 - val_loss: 0.2762\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 6s 42us/step - loss: 0.3996 - val_loss: 0.2583\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.3867 - val_loss: 0.2576\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.3676 - val_loss: 0.2184\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.3596 - val_loss: 0.2105\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.3455 - val_loss: 0.2083\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3389 - val_loss: 0.2085\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.3259 - val_loss: 0.1945\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3151 - val_loss: 0.1836\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.3041 - val_loss: 0.1746\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2953 - val_loss: 0.1842\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.2854 - val_loss: 0.1687\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.2767 - val_loss: 0.1758\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2682 - val_loss: 0.1597\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2602 - val_loss: 0.1580\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.2518 - val_loss: 0.1526\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2435 - val_loss: 0.1503\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.2382 - val_loss: 0.1480\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2311 - val_loss: 0.1487\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2242 - val_loss: 0.1481\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2191 - val_loss: 0.1388\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2135 - val_loss: 0.1429\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2071 - val_loss: 0.1389\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1986 - val_loss: 0.1405\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1943 - val_loss: 0.1387\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1918 - val_loss: 0.1373\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1862 - val_loss: 0.1359\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1837 - val_loss: 0.1328\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1797 - val_loss: 0.1268\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1776 - val_loss: 0.1294\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1735 - val_loss: 0.1308\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1704 - val_loss: 0.1254\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1671 - val_loss: 0.1270\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1643 - val_loss: 0.1252\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1642 - val_loss: 0.1205\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1598 - val_loss: 0.1241\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1605 - val_loss: 0.1280\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1580 - val_loss: 0.1241\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1555 - val_loss: 0.1208\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1555 - val_loss: 0.1202\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1519 - val_loss: 0.1170\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1495 - val_loss: 0.1213\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1502 - val_loss: 0.1161\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1490 - val_loss: 0.1182\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1482 - val_loss: 0.1193\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1461 - val_loss: 0.1192\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1454 - val_loss: 0.1175\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1468 - val_loss: 0.1180\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1446 - val_loss: 0.1146\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1442 - val_loss: 0.1123\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1428 - val_loss: 0.1109\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1435 - val_loss: 0.1087\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1417 - val_loss: 0.1135\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1424 - val_loss: 0.1137\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1407 - val_loss: 0.1094\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1409 - val_loss: 0.1076\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1404 - val_loss: 0.1112\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1396 - val_loss: 0.1084\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1394 - val_loss: 0.1147\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1383 - val_loss: 0.1080\n",
      "Building model 3 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 6s 41us/step - loss: 2.8603 - val_loss: 0.6142\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 1.1605 - val_loss: 0.5834\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.9461 - val_loss: 0.5282\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.8301 - val_loss: 0.4050\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.7499 - val_loss: 0.4065\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6863 - val_loss: 0.3309\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6370 - val_loss: 0.2939\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.5808 - val_loss: 0.2452\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.5311 - val_loss: 0.2333\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.4967 - val_loss: 0.2204\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.4620 - val_loss: 0.2274\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4378 - val_loss: 0.2079\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.4096 - val_loss: 0.1951\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3912 - val_loss: 0.1881\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.3786 - val_loss: 0.1849\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.3645 - val_loss: 0.1699\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.3483 - val_loss: 0.1713\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3393 - val_loss: 0.1770\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3289 - val_loss: 0.1620\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3201 - val_loss: 0.1635\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3112 - val_loss: 0.1558\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3024 - val_loss: 0.1519\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2912 - val_loss: 0.1509\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2850 - val_loss: 0.1542\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2750 - val_loss: 0.1480\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2683 - val_loss: 0.1442\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2607 - val_loss: 0.1505\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.2539 - val_loss: 0.1438\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.2460 - val_loss: 0.1420\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2390 - val_loss: 0.1476\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2331 - val_loss: 0.1450\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2272 - val_loss: 0.1461\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2203 - val_loss: 0.1429\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2157 - val_loss: 0.1487\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2105 - val_loss: 0.1393\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2040 - val_loss: 0.1444\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1989 - val_loss: 0.1423\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1951 - val_loss: 0.1396\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1901 - val_loss: 0.1477\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1867 - val_loss: 0.1448\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1833 - val_loss: 0.1449\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1809 - val_loss: 0.1466\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1777 - val_loss: 0.1473\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1736 - val_loss: 0.1510\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1722 - val_loss: 0.1374\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1691 - val_loss: 0.1471\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1659 - val_loss: 0.1549\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1660 - val_loss: 0.1378\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1630 - val_loss: 0.1394\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1609 - val_loss: 0.1437\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1589 - val_loss: 0.1431\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1575 - val_loss: 0.1459\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1568 - val_loss: 0.1385\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1548 - val_loss: 0.1370\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1523 - val_loss: 0.1343\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1507 - val_loss: 0.1371\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1503 - val_loss: 0.1379\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1492 - val_loss: 0.1348\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1494 - val_loss: 0.1333\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.1476 - val_loss: 0.1360\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1467 - val_loss: 0.1260\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1453 - val_loss: 0.1307\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1456 - val_loss: 0.1270\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1437 - val_loss: 0.1316\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1452 - val_loss: 0.1400\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1421 - val_loss: 0.1273\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1433 - val_loss: 0.1266\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1430 - val_loss: 0.1342\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1404 - val_loss: 0.1232\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1402 - val_loss: 0.1237\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.1416 - val_loss: 0.1156\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1393 - val_loss: 0.1220\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1394 - val_loss: 0.1270\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.1401 - val_loss: 0.1224\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1391 - val_loss: 0.1227\n",
      "Building model 4 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 6s 46us/step - loss: 3.7948 - val_loss: 0.8782\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 1.5692 - val_loss: 0.9334\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 1.2432 - val_loss: 0.8287\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 1.1003 - val_loss: 0.8062\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.9896 - val_loss: 0.6206\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.9175 - val_loss: 0.5072\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.8169 - val_loss: 0.4770\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.7470 - val_loss: 0.4420\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.6939 - val_loss: 0.4085\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.6394 - val_loss: 0.3496\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.6029 - val_loss: 0.2934\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.5690 - val_loss: 0.2858\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.5375 - val_loss: 0.2619\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.4970 - val_loss: 0.2465\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.4627 - val_loss: 0.2278\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.4486 - val_loss: 0.2141\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4271 - val_loss: 0.2132\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.4171 - val_loss: 0.2038\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3976 - val_loss: 0.1952\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.3861 - val_loss: 0.1880\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.3726 - val_loss: 0.1872\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 27us/step - loss: 0.3613 - val_loss: 0.1788\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.3466 - val_loss: 0.1773\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 4s 28us/step - loss: 0.3362 - val_loss: 0.1730\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.3255 - val_loss: 0.1722\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.3161 - val_loss: 0.1613\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 6s 42us/step - loss: 0.3064 - val_loss: 0.1592\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.2987 - val_loss: 0.1627\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.2880 - val_loss: 0.1604\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2797 - val_loss: 0.1585\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2719 - val_loss: 0.1573\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 3s 23us/step - loss: 0.2644 - val_loss: 0.1618\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2577 - val_loss: 0.1585\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2501 - val_loss: 0.1642\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.2429 - val_loss: 0.1616\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.2368 - val_loss: 0.1583\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2294 - val_loss: 0.1533\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2232 - val_loss: 0.1623\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2189 - val_loss: 0.1569\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2133 - val_loss: 0.1501\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.2102 - val_loss: 0.1587\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2049 - val_loss: 0.1529\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.2011 - val_loss: 0.1559\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1967 - val_loss: 0.1544\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1915 - val_loss: 0.1468\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1871 - val_loss: 0.1452\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 26us/step - loss: 0.1850 - val_loss: 0.1438\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1842 - val_loss: 0.1475\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.1801 - val_loss: 0.1449\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1767 - val_loss: 0.1375\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1753 - val_loss: 0.1427\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1721 - val_loss: 0.1369\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 3s 25us/step - loss: 0.1707 - val_loss: 0.1428\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1697 - val_loss: 0.1380\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1678 - val_loss: 0.1290\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1670 - val_loss: 0.1454\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1642 - val_loss: 0.1408\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1634 - val_loss: 0.1284\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1607 - val_loss: 0.1359\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1616 - val_loss: 0.1322\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1611 - val_loss: 0.1316\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1596 - val_loss: 0.1390\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1577 - val_loss: 0.1486\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 4s 25us/step - loss: 0.1566 - val_loss: 0.1353\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1561 - val_loss: 0.1340\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 3s 24us/step - loss: 0.1554 - val_loss: 0.1337\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1553 - val_loss: 0.1295\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1552 - val_loss: 0.1352\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1539 - val_loss: 0.1315\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1532 - val_loss: 0.1265\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1548 - val_loss: 0.1230\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1520 - val_loss: 0.1376\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1556 - val_loss: 0.1271\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1495 - val_loss: 0.1240\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1517 - val_loss: 0.1306\n",
      "Building model 5 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 6s 44us/step - loss: 3.3205 - val_loss: 0.5380\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 1.6474 - val_loss: 0.5033\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.4114 - val_loss: 0.4204\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.2826 - val_loss: 0.3725\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.2030 - val_loss: 0.3366\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.1310 - val_loss: 0.4062\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.0557 - val_loss: 0.3553\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.9931 - val_loss: 0.3347\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.9312 - val_loss: 0.3285\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.8801 - val_loss: 0.2833\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.8311 - val_loss: 0.2689\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 6s 42us/step - loss: 0.7848 - val_loss: 0.3070\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.7376 - val_loss: 0.2395\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6955 - val_loss: 0.2592\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.6627 - val_loss: 0.2221\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 6s 40us/step - loss: 0.6204 - val_loss: 0.2247\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 6s 40us/step - loss: 0.5890 - val_loss: 0.2092\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.5554 - val_loss: 0.2032\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.5278 - val_loss: 0.1798\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 6s 39us/step - loss: 0.5004 - val_loss: 0.1965\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.4699 - val_loss: 0.1904\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.4513 - val_loss: 0.1885\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4263 - val_loss: 0.1870\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 0.4026 - val_loss: 0.1751\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 6s 39us/step - loss: 0.3808 - val_loss: 0.1647\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.3611 - val_loss: 0.1774\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3409 - val_loss: 0.1613\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.3265 - val_loss: 0.1464\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 6s 41us/step - loss: 0.3096 - val_loss: 0.1614\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.2956 - val_loss: 0.1519\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2841 - val_loss: 0.1496\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2723 - val_loss: 0.1434\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2607 - val_loss: 0.1396\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2497 - val_loss: 0.1401\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.2430 - val_loss: 0.1469\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.2348 - val_loss: 0.1344\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2262 - val_loss: 0.1296\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.2212 - val_loss: 0.1308\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.2139 - val_loss: 0.1318\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.2075 - val_loss: 0.1385\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.2049 - val_loss: 0.1309\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1996 - val_loss: 0.1244\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1947 - val_loss: 0.1254\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1912 - val_loss: 0.1239\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1883 - val_loss: 0.1294\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.1856 - val_loss: 0.1198\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1842 - val_loss: 0.1182\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1834 - val_loss: 0.1132\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1793 - val_loss: 0.1194\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.1790 - val_loss: 0.1202\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1779 - val_loss: 0.1148\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1766 - val_loss: 0.1219\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1755 - val_loss: 0.1161\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1743 - val_loss: 0.1121\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1744 - val_loss: 0.1172\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1728 - val_loss: 0.1231\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1704 - val_loss: 0.1169\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1711 - val_loss: 0.1148\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1705 - val_loss: 0.1095\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1687 - val_loss: 0.1142\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1695 - val_loss: 0.1122\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1689 - val_loss: 0.1080\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1678 - val_loss: 0.1082\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1696 - val_loss: 0.1100\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1687 - val_loss: 0.1055\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1664 - val_loss: 0.1144\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1664 - val_loss: 0.1107\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1639 - val_loss: 0.1055\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1652 - val_loss: 0.1180\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1667 - val_loss: 0.1152\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1651 - val_loss: 0.1135\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1640 - val_loss: 0.1107\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1649 - val_loss: 0.1102\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1634 - val_loss: 0.1094\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.1619 - val_loss: 0.1146\n",
      "Building model 6 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 7s 50us/step - loss: 3.6219 - val_loss: 0.7676\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.8249 - val_loss: 0.8386\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.5763 - val_loss: 0.7691\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.3865 - val_loss: 0.7743\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.2605 - val_loss: 0.8039\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.1761 - val_loss: 0.7242\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 1.0980 - val_loss: 0.7234\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 1.0351 - val_loss: 0.7100\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.9721 - val_loss: 0.6204\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.9093 - val_loss: 0.5735\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.8585 - val_loss: 0.4814\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.8101 - val_loss: 0.4833\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.7427 - val_loss: 0.4286\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.7005 - val_loss: 0.3481\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.6696 - val_loss: 0.2938\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.6295 - val_loss: 0.2962\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.6021 - val_loss: 0.2836\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.5706 - val_loss: 0.2460\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.5411 - val_loss: 0.2451\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.5118 - val_loss: 0.2259\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.4869 - val_loss: 0.2067\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4628 - val_loss: 0.2165\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4428 - val_loss: 0.2097\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4198 - val_loss: 0.2081\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4002 - val_loss: 0.2046\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.3788 - val_loss: 0.1894\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.3625 - val_loss: 0.1887\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3485 - val_loss: 0.1782\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3298 - val_loss: 0.1727\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3164 - val_loss: 0.1716\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3001 - val_loss: 0.1640\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2899 - val_loss: 0.1640\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2765 - val_loss: 0.1549\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2698 - val_loss: 0.1632\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2569 - val_loss: 0.1529\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2510 - val_loss: 0.1585\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2420 - val_loss: 0.1418\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2360 - val_loss: 0.1482\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2303 - val_loss: 0.1427\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2251 - val_loss: 0.1393\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2184 - val_loss: 0.1401\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2136 - val_loss: 0.1433\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2104 - val_loss: 0.1364\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2073 - val_loss: 0.1327\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2026 - val_loss: 0.1505\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2010 - val_loss: 0.1434\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1987 - val_loss: 0.1282\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1961 - val_loss: 0.1372\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.1941 - val_loss: 0.1431\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1943 - val_loss: 0.1353\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1918 - val_loss: 0.1267\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.1894 - val_loss: 0.1363\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1899 - val_loss: 0.1362\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.1886 - val_loss: 0.1274\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1864 - val_loss: 0.1329\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1832 - val_loss: 0.1307\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1823 - val_loss: 0.1258\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1834 - val_loss: 0.1263\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.1816 - val_loss: 0.1265\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1825 - val_loss: 0.1284\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.1818 - val_loss: 0.1269\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1801 - val_loss: 0.1235\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1795 - val_loss: 0.1160\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 5s 36us/step - loss: 0.1795 - val_loss: 0.1180\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.1794 - val_loss: 0.1218\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1776 - val_loss: 0.1160\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1767 - val_loss: 0.1215\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1765 - val_loss: 0.1203\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1781 - val_loss: 0.1178\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1784 - val_loss: 0.1191\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1770 - val_loss: 0.1198\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1755 - val_loss: 0.1164\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1739 - val_loss: 0.1175\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1743 - val_loss: 0.1304\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.1766 - val_loss: 0.1223\n",
      "Building model 7 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 6s 45us/step - loss: 3.5808 - val_loss: 0.8408\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.6855 - val_loss: 0.6833\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.4355 - val_loss: 0.5736\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.2857 - val_loss: 0.4416\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 5s 38us/step - loss: 1.1549 - val_loss: 0.4331\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 1.0708 - val_loss: 0.4260\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.9772 - val_loss: 0.2945\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.8992 - val_loss: 0.2980\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.8547 - val_loss: 0.2973\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.7857 - val_loss: 0.2938\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.7444 - val_loss: 0.2602\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6935 - val_loss: 0.2310\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6508 - val_loss: 0.2308\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6170 - val_loss: 0.2307\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5884 - val_loss: 0.2134\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5598 - val_loss: 0.2167\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5320 - val_loss: 0.1987\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5059 - val_loss: 0.1952\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.4857 - val_loss: 0.1827\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4611 - val_loss: 0.1939\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.4445 - val_loss: 0.1824\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4224 - val_loss: 0.1697\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4044 - val_loss: 0.1703\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3872 - val_loss: 0.1665\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3704 - val_loss: 0.1601\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.3572 - val_loss: 0.1596\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3433 - val_loss: 0.1643\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3276 - val_loss: 0.1568\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3140 - val_loss: 0.1512\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3043 - val_loss: 0.1489\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2912 - val_loss: 0.1389\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2817 - val_loss: 0.1368\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2736 - val_loss: 0.1457\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2653 - val_loss: 0.1424\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2590 - val_loss: 0.1343\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.2493 - val_loss: 0.1363\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2417 - val_loss: 0.1353\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2355 - val_loss: 0.1244\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2328 - val_loss: 0.1324\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2272 - val_loss: 0.1271\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2222 - val_loss: 0.1308\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2180 - val_loss: 0.1339\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2145 - val_loss: 0.1209\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2142 - val_loss: 0.1300\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2074 - val_loss: 0.1280\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2038 - val_loss: 0.1222\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2059 - val_loss: 0.1221\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2018 - val_loss: 0.1228\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1971 - val_loss: 0.1277\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1967 - val_loss: 0.1153\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1946 - val_loss: 0.1172\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1917 - val_loss: 0.1249\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1949 - val_loss: 0.1152\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1917 - val_loss: 0.1201\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1907 - val_loss: 0.1177\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1894 - val_loss: 0.1225\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1889 - val_loss: 0.1178\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1870 - val_loss: 0.1176\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.1880 - val_loss: 0.1124\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1878 - val_loss: 0.1173\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1872 - val_loss: 0.1122\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1869 - val_loss: 0.1196\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1833 - val_loss: 0.1167\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1850 - val_loss: 0.1174\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1827 - val_loss: 0.1126\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.1833 - val_loss: 0.1182\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1812 - val_loss: 0.1152\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1814 - val_loss: 0.1154\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.1816 - val_loss: 0.1127\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 5s 37us/step - loss: 0.1821 - val_loss: 0.1114\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1817 - val_loss: 0.1085\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.1782 - val_loss: 0.1128\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1799 - val_loss: 0.1100\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.1784 - val_loss: 0.1098\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.1775 - val_loss: 0.1147\n",
      "Building model 8 of 8\n",
      "Train on 141859 samples, validate on 47287 samples\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141859/141859 [==============================] - 6s 44us/step - loss: 3.6126 - val_loss: 1.7920\n",
      "Epoch 2/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.8524 - val_loss: 1.3186\n",
      "Epoch 3/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 1.5096 - val_loss: 1.0780\n",
      "Epoch 4/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.2923 - val_loss: 0.8128\n",
      "Epoch 5/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 1.1580 - val_loss: 0.6719\n",
      "Epoch 6/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 1.0269 - val_loss: 0.6225\n",
      "Epoch 7/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.9255 - val_loss: 0.5364\n",
      "Epoch 8/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.8454 - val_loss: 0.4418\n",
      "Epoch 9/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.7776 - val_loss: 0.4037\n",
      "Epoch 10/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.7284 - val_loss: 0.3566\n",
      "Epoch 11/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6809 - val_loss: 0.3201\n",
      "Epoch 12/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.6390 - val_loss: 0.2997\n",
      "Epoch 13/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.6058 - val_loss: 0.2850\n",
      "Epoch 14/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5774 - val_loss: 0.2532\n",
      "Epoch 15/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5502 - val_loss: 0.2489\n",
      "Epoch 16/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5239 - val_loss: 0.2201\n",
      "Epoch 17/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.5016 - val_loss: 0.2321\n",
      "Epoch 18/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.4761 - val_loss: 0.2273\n",
      "Epoch 19/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4534 - val_loss: 0.2124\n",
      "Epoch 20/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4386 - val_loss: 0.1963\n",
      "Epoch 21/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4200 - val_loss: 0.1831\n",
      "Epoch 22/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.4031 - val_loss: 0.1857\n",
      "Epoch 23/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3894 - val_loss: 0.1833\n",
      "Epoch 24/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3750 - val_loss: 0.1831\n",
      "Epoch 25/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3598 - val_loss: 0.1640\n",
      "Epoch 26/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3502 - val_loss: 0.1669\n",
      "Epoch 27/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.3363 - val_loss: 0.1642\n",
      "Epoch 28/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3250 - val_loss: 0.1607\n",
      "Epoch 29/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3121 - val_loss: 0.1642\n",
      "Epoch 30/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.3042 - val_loss: 0.1563\n",
      "Epoch 31/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2950 - val_loss: 0.1518\n",
      "Epoch 32/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2862 - val_loss: 0.1536\n",
      "Epoch 33/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2791 - val_loss: 0.1446\n",
      "Epoch 34/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2692 - val_loss: 0.1447\n",
      "Epoch 35/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2664 - val_loss: 0.1502\n",
      "Epoch 36/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2607 - val_loss: 0.1531\n",
      "Epoch 37/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2530 - val_loss: 0.1464\n",
      "Epoch 38/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2467 - val_loss: 0.1387\n",
      "Epoch 39/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2465 - val_loss: 0.1408\n",
      "Epoch 40/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2400 - val_loss: 0.1418\n",
      "Epoch 41/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2358 - val_loss: 0.1420\n",
      "Epoch 42/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2345 - val_loss: 0.1459\n",
      "Epoch 43/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2307 - val_loss: 0.1430\n",
      "Epoch 44/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2313 - val_loss: 0.1354\n",
      "Epoch 45/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2287 - val_loss: 0.1449\n",
      "Epoch 46/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2261 - val_loss: 0.1499\n",
      "Epoch 47/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2237 - val_loss: 0.1372\n",
      "Epoch 48/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.2200 - val_loss: 0.1332\n",
      "Epoch 49/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2202 - val_loss: 0.1328\n",
      "Epoch 50/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2178 - val_loss: 0.1376\n",
      "Epoch 51/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2179 - val_loss: 0.1365\n",
      "Epoch 52/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2151 - val_loss: 0.1362\n",
      "Epoch 53/75\n",
      "141859/141859 [==============================] - 4s 29us/step - loss: 0.2150 - val_loss: 0.1317\n",
      "Epoch 54/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2147 - val_loss: 0.1372\n",
      "Epoch 55/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2123 - val_loss: 0.1320\n",
      "Epoch 56/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2128 - val_loss: 0.1317\n",
      "Epoch 57/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2142 - val_loss: 0.1310\n",
      "Epoch 58/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2121 - val_loss: 0.1318\n",
      "Epoch 59/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.2105 - val_loss: 0.1387\n",
      "Epoch 60/75\n",
      "141859/141859 [==============================] - 5s 34us/step - loss: 0.2108 - val_loss: 0.1281\n",
      "Epoch 61/75\n",
      "141859/141859 [==============================] - 5s 33us/step - loss: 0.2104 - val_loss: 0.1297\n",
      "Epoch 62/75\n",
      "141859/141859 [==============================] - 5s 35us/step - loss: 0.2100 - val_loss: 0.1219\n",
      "Epoch 63/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2087 - val_loss: 0.1311\n",
      "Epoch 64/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2088 - val_loss: 0.1287\n",
      "Epoch 65/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2058 - val_loss: 0.1300\n",
      "Epoch 66/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2063 - val_loss: 0.1240\n",
      "Epoch 67/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2068 - val_loss: 0.1299\n",
      "Epoch 68/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2050 - val_loss: 0.1280\n",
      "Epoch 69/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.2056 - val_loss: 0.1293\n",
      "Epoch 70/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2041 - val_loss: 0.1253\n",
      "Epoch 71/75\n",
      "141859/141859 [==============================] - 4s 30us/step - loss: 0.2054 - val_loss: 0.1280\n",
      "Epoch 72/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2049 - val_loss: 0.1229\n",
      "Epoch 73/75\n",
      "141859/141859 [==============================] - 4s 32us/step - loss: 0.2021 - val_loss: 0.1223\n",
      "Epoch 74/75\n",
      "141859/141859 [==============================] - 4s 31us/step - loss: 0.2036 - val_loss: 0.1184\n",
      "Epoch 75/75\n",
      "141859/141859 [==============================] - 5s 32us/step - loss: 0.2036 - val_loss: 0.1206\n"
     ]
    }
   ],
   "source": [
    "final_nn_model = nn_grid_search(X, y, grid_params=pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the grid search has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_model': <keras.engine.sequential.Sequential at 0x1a557d7d10>,\n",
       " 'best_score': 0.900713765015688,\n",
       " 'best_params': {'first_layer_nodes': 256,\n",
       "  'first_dropout_rate': 0.25,\n",
       "  'second_layer_nodes': 128,\n",
       "  'second_dropout_rate': 0.25,\n",
       "  'third_layer_nodes': 32,\n",
       "  'third_dropout_rate': 0.25,\n",
       "  'epochs': 75},\n",
       " 'best_history': <keras.callbacks.History at 0x1a57ce6550>,\n",
       " 'test_preds': array([[5.648533 ],\n",
       "        [4.497038 ],\n",
       "        [3.839902 ],\n",
       "        ...,\n",
       "        [5.1945095],\n",
       "        [4.4716697],\n",
       "        [3.4207933]], dtype=float32)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "nn_train_preds = final_nn_model[\"best_model\"].predict(X_train_sc)\n",
    "nn_test_preds = final_nn_model[\"test_preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.907672063389299"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2_score(y_train, nn_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8817637469051529"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2_score(y_test, nn_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help automate the eavluations process, we are going to compile the train and test predictions for each of our models, and from there we will be able to easily evaluate the desired metrics from one spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making train pred df\n",
    "train_pred_df = pd.DataFrame(columns=[\"base\",\n",
    "                                     \"linear\",\n",
    "                                     \"lasso\",\n",
    "                                     \"ridge\",\n",
    "                                     \"neural_net\"])\n",
    "\n",
    "# making test pred df\n",
    "test_pred_df = pd.DataFrame(columns=[\"base\",\n",
    "                                     \"linear\",\n",
    "                                     \"lasso\",\n",
    "                                     \"ridge\",\n",
    "                                     \"neural_net\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dict of our first 3 models\n",
    "models = {\"base\": base_mean,\n",
    "          \"linear\": linreg_model,\n",
    "          \"lasso\": lasso_model,\n",
    "          \"ridge\": ridge_model,}\n",
    "\n",
    "# looping through models to input preds into df\n",
    "for model in models.keys():\n",
    "    train_pred_df[model] = np.exp(linreg_model.predict(X_train))\n",
    "    test_pred_df[model] = np.exp(linreg_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>linear</th>\n",
       "      <th>lasso</th>\n",
       "      <th>ridge</th>\n",
       "      <th>neural_net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133.968872</td>\n",
       "      <td>133.968872</td>\n",
       "      <td>133.968872</td>\n",
       "      <td>133.968872</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93.642083</td>\n",
       "      <td>93.642083</td>\n",
       "      <td>93.642083</td>\n",
       "      <td>93.642083</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.437591</td>\n",
       "      <td>46.437591</td>\n",
       "      <td>46.437591</td>\n",
       "      <td>46.437591</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.529767</td>\n",
       "      <td>25.529767</td>\n",
       "      <td>25.529767</td>\n",
       "      <td>25.529767</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.680915</td>\n",
       "      <td>25.680915</td>\n",
       "      <td>25.680915</td>\n",
       "      <td>25.680915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89.005597</td>\n",
       "      <td>89.005597</td>\n",
       "      <td>89.005597</td>\n",
       "      <td>89.005597</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.064228</td>\n",
       "      <td>16.064228</td>\n",
       "      <td>16.064228</td>\n",
       "      <td>16.064228</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.888248</td>\n",
       "      <td>26.888248</td>\n",
       "      <td>26.888248</td>\n",
       "      <td>26.888248</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.113277</td>\n",
       "      <td>25.113277</td>\n",
       "      <td>25.113277</td>\n",
       "      <td>25.113277</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35.464983</td>\n",
       "      <td>35.464983</td>\n",
       "      <td>35.464983</td>\n",
       "      <td>35.464983</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44.514346</td>\n",
       "      <td>44.514346</td>\n",
       "      <td>44.514346</td>\n",
       "      <td>44.514346</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>93.793736</td>\n",
       "      <td>93.793736</td>\n",
       "      <td>93.793736</td>\n",
       "      <td>93.793736</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32.960335</td>\n",
       "      <td>32.960335</td>\n",
       "      <td>32.960335</td>\n",
       "      <td>32.960335</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31.884283</td>\n",
       "      <td>31.884283</td>\n",
       "      <td>31.884283</td>\n",
       "      <td>31.884283</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.836788</td>\n",
       "      <td>35.836788</td>\n",
       "      <td>35.836788</td>\n",
       "      <td>35.836788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40.871093</td>\n",
       "      <td>40.871093</td>\n",
       "      <td>40.871093</td>\n",
       "      <td>40.871093</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30.099228</td>\n",
       "      <td>30.099228</td>\n",
       "      <td>30.099228</td>\n",
       "      <td>30.099228</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>130.141728</td>\n",
       "      <td>130.141728</td>\n",
       "      <td>130.141728</td>\n",
       "      <td>130.141728</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27.977553</td>\n",
       "      <td>27.977553</td>\n",
       "      <td>27.977553</td>\n",
       "      <td>27.977553</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64.270864</td>\n",
       "      <td>64.270864</td>\n",
       "      <td>64.270864</td>\n",
       "      <td>64.270864</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>86.675270</td>\n",
       "      <td>86.675270</td>\n",
       "      <td>86.675270</td>\n",
       "      <td>86.675270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>119.545020</td>\n",
       "      <td>119.545020</td>\n",
       "      <td>119.545020</td>\n",
       "      <td>119.545020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>125.907886</td>\n",
       "      <td>125.907886</td>\n",
       "      <td>125.907886</td>\n",
       "      <td>125.907886</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17.237849</td>\n",
       "      <td>17.237849</td>\n",
       "      <td>17.237849</td>\n",
       "      <td>17.237849</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>163.279058</td>\n",
       "      <td>163.279058</td>\n",
       "      <td>163.279058</td>\n",
       "      <td>163.279058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>107.175603</td>\n",
       "      <td>107.175603</td>\n",
       "      <td>107.175603</td>\n",
       "      <td>107.175603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31.771757</td>\n",
       "      <td>31.771757</td>\n",
       "      <td>31.771757</td>\n",
       "      <td>31.771757</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>46.457250</td>\n",
       "      <td>46.457250</td>\n",
       "      <td>46.457250</td>\n",
       "      <td>46.457250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>68.414645</td>\n",
       "      <td>68.414645</td>\n",
       "      <td>68.414645</td>\n",
       "      <td>68.414645</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>27.368596</td>\n",
       "      <td>27.368596</td>\n",
       "      <td>27.368596</td>\n",
       "      <td>27.368596</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>27.501735</td>\n",
       "      <td>27.501735</td>\n",
       "      <td>27.501735</td>\n",
       "      <td>27.501735</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32.624827</td>\n",
       "      <td>32.624827</td>\n",
       "      <td>32.624827</td>\n",
       "      <td>32.624827</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>58.774944</td>\n",
       "      <td>58.774944</td>\n",
       "      <td>58.774944</td>\n",
       "      <td>58.774944</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>31.940140</td>\n",
       "      <td>31.940140</td>\n",
       "      <td>31.940140</td>\n",
       "      <td>31.940140</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16.441738</td>\n",
       "      <td>16.441738</td>\n",
       "      <td>16.441738</td>\n",
       "      <td>16.441738</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25.353795</td>\n",
       "      <td>25.353795</td>\n",
       "      <td>25.353795</td>\n",
       "      <td>25.353795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>29.249293</td>\n",
       "      <td>29.249293</td>\n",
       "      <td>29.249293</td>\n",
       "      <td>29.249293</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>136.586791</td>\n",
       "      <td>136.586791</td>\n",
       "      <td>136.586791</td>\n",
       "      <td>136.586791</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.532865</td>\n",
       "      <td>38.532865</td>\n",
       "      <td>38.532865</td>\n",
       "      <td>38.532865</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>26.478781</td>\n",
       "      <td>26.478781</td>\n",
       "      <td>26.478781</td>\n",
       "      <td>26.478781</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>27.501735</td>\n",
       "      <td>27.501735</td>\n",
       "      <td>27.501735</td>\n",
       "      <td>27.501735</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>24.720588</td>\n",
       "      <td>24.720588</td>\n",
       "      <td>24.720588</td>\n",
       "      <td>24.720588</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>35.399385</td>\n",
       "      <td>35.399385</td>\n",
       "      <td>35.399385</td>\n",
       "      <td>35.399385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>11.988822</td>\n",
       "      <td>11.988822</td>\n",
       "      <td>11.988822</td>\n",
       "      <td>11.988822</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32.955571</td>\n",
       "      <td>32.955571</td>\n",
       "      <td>32.955571</td>\n",
       "      <td>32.955571</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>224.347466</td>\n",
       "      <td>224.347466</td>\n",
       "      <td>224.347466</td>\n",
       "      <td>224.347466</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>77.647164</td>\n",
       "      <td>77.647164</td>\n",
       "      <td>77.647164</td>\n",
       "      <td>77.647164</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>90.474708</td>\n",
       "      <td>90.474708</td>\n",
       "      <td>90.474708</td>\n",
       "      <td>90.474708</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>45.141349</td>\n",
       "      <td>45.141349</td>\n",
       "      <td>45.141349</td>\n",
       "      <td>45.141349</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>124.956405</td>\n",
       "      <td>124.956405</td>\n",
       "      <td>124.956405</td>\n",
       "      <td>124.956405</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47237</th>\n",
       "      <td>22.201464</td>\n",
       "      <td>22.201464</td>\n",
       "      <td>22.201464</td>\n",
       "      <td>22.201464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47238</th>\n",
       "      <td>36.183208</td>\n",
       "      <td>36.183208</td>\n",
       "      <td>36.183208</td>\n",
       "      <td>36.183208</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47239</th>\n",
       "      <td>205.155993</td>\n",
       "      <td>205.155993</td>\n",
       "      <td>205.155993</td>\n",
       "      <td>205.155993</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47240</th>\n",
       "      <td>26.171111</td>\n",
       "      <td>26.171111</td>\n",
       "      <td>26.171111</td>\n",
       "      <td>26.171111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47241</th>\n",
       "      <td>90.983112</td>\n",
       "      <td>90.983112</td>\n",
       "      <td>90.983112</td>\n",
       "      <td>90.983112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47242</th>\n",
       "      <td>31.319874</td>\n",
       "      <td>31.319874</td>\n",
       "      <td>31.319874</td>\n",
       "      <td>31.319874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47243</th>\n",
       "      <td>31.348662</td>\n",
       "      <td>31.348662</td>\n",
       "      <td>31.348662</td>\n",
       "      <td>31.348662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47244</th>\n",
       "      <td>132.806265</td>\n",
       "      <td>132.806265</td>\n",
       "      <td>132.806265</td>\n",
       "      <td>132.806265</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47245</th>\n",
       "      <td>27.669675</td>\n",
       "      <td>27.669675</td>\n",
       "      <td>27.669675</td>\n",
       "      <td>27.669675</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47246</th>\n",
       "      <td>236.017902</td>\n",
       "      <td>236.017902</td>\n",
       "      <td>236.017902</td>\n",
       "      <td>236.017902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47247</th>\n",
       "      <td>37.030943</td>\n",
       "      <td>37.030943</td>\n",
       "      <td>37.030943</td>\n",
       "      <td>37.030943</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47248</th>\n",
       "      <td>46.847308</td>\n",
       "      <td>46.847308</td>\n",
       "      <td>46.847308</td>\n",
       "      <td>46.847308</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47249</th>\n",
       "      <td>28.008680</td>\n",
       "      <td>28.008680</td>\n",
       "      <td>28.008680</td>\n",
       "      <td>28.008680</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47250</th>\n",
       "      <td>27.180262</td>\n",
       "      <td>27.180262</td>\n",
       "      <td>27.180262</td>\n",
       "      <td>27.180262</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47251</th>\n",
       "      <td>31.305794</td>\n",
       "      <td>31.305794</td>\n",
       "      <td>31.305794</td>\n",
       "      <td>31.305794</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47252</th>\n",
       "      <td>68.589170</td>\n",
       "      <td>68.589170</td>\n",
       "      <td>68.589170</td>\n",
       "      <td>68.589170</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47253</th>\n",
       "      <td>50.149907</td>\n",
       "      <td>50.149907</td>\n",
       "      <td>50.149907</td>\n",
       "      <td>50.149907</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47254</th>\n",
       "      <td>24.931088</td>\n",
       "      <td>24.931088</td>\n",
       "      <td>24.931088</td>\n",
       "      <td>24.931088</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47255</th>\n",
       "      <td>36.451092</td>\n",
       "      <td>36.451092</td>\n",
       "      <td>36.451092</td>\n",
       "      <td>36.451092</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47256</th>\n",
       "      <td>59.973537</td>\n",
       "      <td>59.973537</td>\n",
       "      <td>59.973537</td>\n",
       "      <td>59.973537</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47257</th>\n",
       "      <td>32.547284</td>\n",
       "      <td>32.547284</td>\n",
       "      <td>32.547284</td>\n",
       "      <td>32.547284</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47258</th>\n",
       "      <td>54.826218</td>\n",
       "      <td>54.826218</td>\n",
       "      <td>54.826218</td>\n",
       "      <td>54.826218</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47259</th>\n",
       "      <td>48.428135</td>\n",
       "      <td>48.428135</td>\n",
       "      <td>48.428135</td>\n",
       "      <td>48.428135</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47260</th>\n",
       "      <td>123.460639</td>\n",
       "      <td>123.460639</td>\n",
       "      <td>123.460639</td>\n",
       "      <td>123.460639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47261</th>\n",
       "      <td>31.592417</td>\n",
       "      <td>31.592417</td>\n",
       "      <td>31.592417</td>\n",
       "      <td>31.592417</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47262</th>\n",
       "      <td>79.918029</td>\n",
       "      <td>79.918029</td>\n",
       "      <td>79.918029</td>\n",
       "      <td>79.918029</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47263</th>\n",
       "      <td>28.283319</td>\n",
       "      <td>28.283319</td>\n",
       "      <td>28.283319</td>\n",
       "      <td>28.283319</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47264</th>\n",
       "      <td>97.500641</td>\n",
       "      <td>97.500641</td>\n",
       "      <td>97.500641</td>\n",
       "      <td>97.500641</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47265</th>\n",
       "      <td>78.313950</td>\n",
       "      <td>78.313950</td>\n",
       "      <td>78.313950</td>\n",
       "      <td>78.313950</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47266</th>\n",
       "      <td>105.498314</td>\n",
       "      <td>105.498314</td>\n",
       "      <td>105.498314</td>\n",
       "      <td>105.498314</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47267</th>\n",
       "      <td>45.415032</td>\n",
       "      <td>45.415032</td>\n",
       "      <td>45.415032</td>\n",
       "      <td>45.415032</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47268</th>\n",
       "      <td>57.657429</td>\n",
       "      <td>57.657429</td>\n",
       "      <td>57.657429</td>\n",
       "      <td>57.657429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47269</th>\n",
       "      <td>90.139284</td>\n",
       "      <td>90.139284</td>\n",
       "      <td>90.139284</td>\n",
       "      <td>90.139284</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47270</th>\n",
       "      <td>93.364112</td>\n",
       "      <td>93.364112</td>\n",
       "      <td>93.364112</td>\n",
       "      <td>93.364112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47271</th>\n",
       "      <td>25.105437</td>\n",
       "      <td>25.105437</td>\n",
       "      <td>25.105437</td>\n",
       "      <td>25.105437</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47272</th>\n",
       "      <td>2.070079</td>\n",
       "      <td>2.070079</td>\n",
       "      <td>2.070079</td>\n",
       "      <td>2.070079</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47273</th>\n",
       "      <td>32.472592</td>\n",
       "      <td>32.472592</td>\n",
       "      <td>32.472592</td>\n",
       "      <td>32.472592</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47274</th>\n",
       "      <td>32.343564</td>\n",
       "      <td>32.343564</td>\n",
       "      <td>32.343564</td>\n",
       "      <td>32.343564</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47275</th>\n",
       "      <td>27.689764</td>\n",
       "      <td>27.689764</td>\n",
       "      <td>27.689764</td>\n",
       "      <td>27.689764</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47276</th>\n",
       "      <td>58.305273</td>\n",
       "      <td>58.305273</td>\n",
       "      <td>58.305273</td>\n",
       "      <td>58.305273</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47277</th>\n",
       "      <td>42.831539</td>\n",
       "      <td>42.831539</td>\n",
       "      <td>42.831539</td>\n",
       "      <td>42.831539</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47278</th>\n",
       "      <td>33.698009</td>\n",
       "      <td>33.698009</td>\n",
       "      <td>33.698009</td>\n",
       "      <td>33.698009</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47279</th>\n",
       "      <td>126.335271</td>\n",
       "      <td>126.335271</td>\n",
       "      <td>126.335271</td>\n",
       "      <td>126.335271</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47280</th>\n",
       "      <td>73.478376</td>\n",
       "      <td>73.478376</td>\n",
       "      <td>73.478376</td>\n",
       "      <td>73.478376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47281</th>\n",
       "      <td>20.458558</td>\n",
       "      <td>20.458558</td>\n",
       "      <td>20.458558</td>\n",
       "      <td>20.458558</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47282</th>\n",
       "      <td>76.887445</td>\n",
       "      <td>76.887445</td>\n",
       "      <td>76.887445</td>\n",
       "      <td>76.887445</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47283</th>\n",
       "      <td>75.361373</td>\n",
       "      <td>75.361373</td>\n",
       "      <td>75.361373</td>\n",
       "      <td>75.361373</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47284</th>\n",
       "      <td>147.844092</td>\n",
       "      <td>147.844092</td>\n",
       "      <td>147.844092</td>\n",
       "      <td>147.844092</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47285</th>\n",
       "      <td>105.339020</td>\n",
       "      <td>105.339020</td>\n",
       "      <td>105.339020</td>\n",
       "      <td>105.339020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47286</th>\n",
       "      <td>31.161360</td>\n",
       "      <td>31.161360</td>\n",
       "      <td>31.161360</td>\n",
       "      <td>31.161360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47287 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             base      linear       lasso       ridge neural_net\n",
       "0      133.968872  133.968872  133.968872  133.968872        NaN\n",
       "1       93.642083   93.642083   93.642083   93.642083        NaN\n",
       "2       46.437591   46.437591   46.437591   46.437591        NaN\n",
       "3       25.529767   25.529767   25.529767   25.529767        NaN\n",
       "4       25.680915   25.680915   25.680915   25.680915        NaN\n",
       "...           ...         ...         ...         ...        ...\n",
       "47282   76.887445   76.887445   76.887445   76.887445        NaN\n",
       "47283   75.361373   75.361373   75.361373   75.361373        NaN\n",
       "47284  147.844092  147.844092  147.844092  147.844092        NaN\n",
       "47285  105.339020  105.339020  105.339020  105.339020        NaN\n",
       "47286   31.161360   31.161360   31.161360   31.161360        NaN\n",
       "\n",
       "[47287 rows x 5 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso</th>\n",
       "      <td>-450216.910</td>\n",
       "      <td>-490520.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>-450693.411</td>\n",
       "      <td>-490641.546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train_r2     test_r2\n",
       "base         0.000      -0.000\n",
       "linreg       0.735       0.741\n",
       "lasso  -450216.910 -490520.151\n",
       "ridge  -450693.411 -490641.546"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a df to store everything\n",
    "model_df = pd.DataFrame(index=[model for model in models])\n",
    "\n",
    "# making a dict of our first 3 models\n",
    "models = {\"base\": base_mean,\n",
    "          \"linreg\": linreg_model,\n",
    "          \"lasso\": lasso_model,\n",
    "          \"ridge\": ridge_model}\n",
    "\n",
    "# making dict for preds\n",
    "pred_dict = {}\n",
    "\n",
    "for model in models.keys():\n",
    "    pred_dict[f\"{model}_train_pred\"] = np.exp(linreg_model.predict(X_train))\n",
    "    pred_dict[f\"{model}_test_pred\"] = np.exp(linreg_model.predict(X_test))\n",
    "\n",
    "# putting in the training scores\n",
    "model_df[\"train_r2\"] = [round(metrics.mean_squared_error(X_train, y_train), 3) for model in models.values()]\n",
    "\n",
    "# putting in the testing scores\n",
    "model_df[\"test_r2\"] = [round(model.score(X_test, y_test), 3) for model in models.values()]\n",
    "\n",
    "# getting the difference, or variance, between each score\n",
    "# model_df[\"var\"] = model_df[\"train_\"] - model_df[\"test_acc\"]\n",
    "\n",
    "# checking the final dataframe\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R<sup>2</sup> Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate both the normal R<sup>2</sup>, and adjusted R<sup>2</sup> scores for each of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in custom adjusted r2 function from 3.01 Linear Reg Lab\n",
    "def r2_adj(model, X, y):\n",
    "    # seting some base variables\n",
    "    n = len(y)\n",
    "    k = len(X.columns)\n",
    "    r_sq = model.score(X, y)\n",
    "    \n",
    "    # using the vars as inputs for the adjusted r2 equation\n",
    "    r_sq_adj = 1 - (((1 - r_sq) * (n - 1)) / (n - k - 1))\n",
    "    return r_sq_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 scores are above in the modeling phase, just need to compile them and compare against adjusted r2 down here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train MSE: 1006.5988194771304.\n",
      "OLS test MSE: 1066.4870318968215.\n"
     ]
    }
   ],
   "source": [
    "# finding mse for lasso model\n",
    "\n",
    "# making predictions\n",
    "linreg_train_preds = np.exp(linreg_model.predict(X_train))\n",
    "linreg_test_preds = np.exp(linreg_model.predict(X_test))\n",
    "\n",
    "# calculating mse\n",
    "linreg_train_mse = metrics.mean_squared_error(np.exp(y_train), linreg_train_preds)\n",
    "linreg_test_mse = metrics.mean_squared_error(np.exp(y_test), linreg_test_preds)\n",
    "\n",
    "print(f\"OLS train MSE: {linreg_train_mse}.\")\n",
    "print(f\"OLS test MSE: {linreg_test_mse}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO train MSE: 1006.7210399896342.\n",
      "LASSO test MSE: 1067.9519777748644.\n"
     ]
    }
   ],
   "source": [
    "# finding mse for lasso model\n",
    "\n",
    "# making predictions\n",
    "lasso_train_preds = np.exp(lasso_model.predict(X_train_sc))\n",
    "lasso_test_preds = np.exp(lasso_model.predict(X_test_sc))\n",
    "\n",
    "# calculating mse\n",
    "lasso_train_mse = metrics.mean_squared_error(np.exp(y_train), lasso_train_preds)\n",
    "lasso_test_mse = metrics.mean_squared_error(np.exp(y_test), lasso_test_preds)\n",
    "\n",
    "print(f\"LASSO train MSE: {lasso_train_mse}.\")\n",
    "print(f\"LASSO test MSE: {lasso_test_mse}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge train MSE: 1006.6401361031418.\n",
      "Ridge test MSE: 1066.5697180523443.\n"
     ]
    }
   ],
   "source": [
    "# finding mse for lasso model\n",
    "\n",
    "# making predictions\n",
    "ridge_train_preds = np.exp(ridge_model.predict(X_train_sc))\n",
    "ridge_test_preds = np.exp(ridge_model.predict(X_test_sc))\n",
    "\n",
    "# calculating mse\n",
    "ridge_train_mse = metrics.mean_squared_error(np.exp(y_train), ridge_train_preds)\n",
    "ridge_test_mse = metrics.mean_squared_error(np.exp(y_test), ridge_test_preds)\n",
    "\n",
    "print(f\"Ridge train MSE: {ridge_train_mse}.\")\n",
    "print(f\"Ridge test MSE: {ridge_test_mse}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network train MSE: 571.2166353561056.\n",
      "Neural network test MSE: 730.6331446433081.\n"
     ]
    }
   ],
   "source": [
    "# finding mse for nn model\n",
    "\n",
    "# making predictions\n",
    "nn_train_preds = np.exp(final_model[\"best_model\"].predict(X_train_sc))\n",
    "nn_test_preds = np.exp(final_model[\"test_preds\"])\n",
    "\n",
    "# calculating mse\n",
    "nn_train_mse = metrics.mean_squared_error(np.exp(y_train), nn_train_preds)\n",
    "nn_test_mse = metrics.mean_squared_error(np.exp(y_test), nn_test_preds)\n",
    "\n",
    "print(f\"Neural network train MSE: {nn_train_mse}.\")\n",
    "print(f\"Neural network test MSE: {nn_test_mse}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these numbers that the neural network had by far the lowest MSE scores, though it was somewhat more overfit than the others. However, due to how much better of a performance it was able to give us, we are going to select that as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert bar charts of mse and r2 here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = final_model[\"best_history\"].history[\"loss\"]\n",
    "test_loss = final_model[\"best_history\"].history[\"val_loss\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.title(\"Loss Curve of Best Neural Network\", size=20)\n",
    "plt.xlabel(\"Epoch\", size=15)\n",
    "plt.ylabel(\"Loss\", size=15)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line graph shows us how the loss of our final model changed as it ran through 75 epochs. There does not appear to be a lot of variation in the curves, and we have avoided divergence of the losses between the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a8292a610>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAHDCAYAAAB/Ho3HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3gU5drH8e+dBqEISBMpggoiIihEpIhUwd6O7djLsbejHvtBsXc99q7Yu752aQKCKAoiKogIioD0XkPa8/4xk82y7CYb2M1skt/nuvZKnpnZmXtn271PG3POISIiIiKpJS3oAERERERka0rSRERERFKQkjQRERGRFKQkTURERCQFKUkTERERSUFK0kRERERSkJI0kTiZ2VAzc2bWN+hYghbrXPjLxibxuMP8Y7RO1jEqgpntZGYvmdkCMyv0H1P9oONKtCr0fOm9L4FQkiYpzcz6lvXFb2at/W3mVlxkkgzV6MtwGHAaMA64HbgFyC3rTmZ2vJl9YWZLzSzfzFaY2Qwze9XMzkhuyFWHmdU0s/+Y2SQzW2NmeWa2yMymmNljZtYn6BhFADKCDkBEqpQ9gY1J3P/1wN3A30k8RlKZWRZwEDDKOXdKOe73DHAusAn4FPgTqA3sChwB9AVeSnS82ynlni8zq4OXHHcBFgPvAUuApkBb4Dygvr9NsceAN4F5FRqsVHtK0kQkYZxzM5O8/0XAomQeowLshNeKsTDeO5hZL7wEbQHQwzm3IGJ9Jl6SllJS9Pn6N16CNgI4wjmXF77SzBrg/dgIcc4tB5ZXWIQiPjV3SpUV3nRmZseZ2XdmttHMVprZm2bWPMb9uvpNSuvMbK2ZjTKzHmUcq73f/2a+mW02syVm9rqZ7RFl2+J+Orua2aVm9pOZbSpu0g1r4h1qZj3846/x4xluZjllPNaT/Wac9ZFNwGa2v5m9a2aL/Sae+Wb2tJntnIhzEatp2szSzewCM/vafyybzGy2mT1nZm39beYCN/t3GePvy5mZC9tPzD5OZnaCmX0Vtv+fzex6M6sRZdu5/q2Wmd1nZvP85222mV1rZhbrMcZ43G3N7GUz+9s/rwv9ctvI4wJ/+cUzwh7jsDIO0cv/+15kggbgnMt3zo2MEdtgM/vMzJb7j3GO/5i36gMXdl52MLMH/f/z/dfX036sR8Y4Tnd//Tthy7Z6vqyke8Iw//83/dhyzWyymR0eY//1zOx/5vXjyzWzmWZ2pf8+iuccFuvp/30yMkEDcM6tcs5NjDj2Vs3wZjY2/DUa5TY2Yh8ZZnaRmX3rv5c2mtlUM7vEzLb6LjazI81stHnNsJv919Q4M7sozscpVYBq0qQ6uAg4EvgIrwljf+BEoLOZ7eOc21y8oZn1BEYBWcD7wGxgH2As8GW0nZvZwf62mcDH/n1aAMcCh5lZP+fcD1Hu+jDQG6/p6jOgMGL9/njNRaOAx4Hd/X0eaGaDnHPjo+zzKrymtI+BMUC9sDjPAp4FNvvnYj5e886/gCPMrLtzbl7Y9uU+F9GY17z3KTDQP+brwFqgNXAMMAH4HfgfcDTQB6/Zbm45jnEn3rla7u9/PXAIcCcw2MwOcs7lR9wtE682ZWfgc6DAP/7dQE28fmLxHHs/vPNUF++8zgDaA6cAR5nZAOfcZH/z//mP+3JgGvB//vIfyzjMCv9vu3hiCovtJrzHsRL4BFgKdAL+AxxqZj2cc2sj7paF9/zuiHd+1uI1rQ7Hawo8w3+ckU73/8bb5LoL8B3wB/CKf7wTgQ/NbKBzbkzY46jpx9QFmAq8hvfavhHvPVQe23QuoxiG916I1BvoT1izv3k1nR8Dg4Hf8F6juUA/4FG89/ppYdufBzyN1xz7Md7rugnec3cW8MR2xi6VhXNON91S9obXhOOAsaVs09rfZm7E8qH+8rXA3hHrXvfXnRC2zICZ/vKjIra/3F/ugL5hyxsAq/A+RDtE3GcvvGThh4jlw/z9/A20KeUxO+CSiHVH+ct/B9KiPNYNwL5R9tkOyMNLtJpHrOuPlyB+sD3nwl+31XOFlyg5vC/2GhHragCNozyOvpGPIeLctQ5b1sNfNg/YKWx5Bt4XnANuiNjPXH/5Z0B22PImwGr/lhnH69OAX/19nRKx7kR/+cyI56r49TqsHO+D5n5MxefxZLwE20q5Tz9/+4lA/Yh1Z/rrHopxXkYBtaPs8ze8JL9hlOdxJV7frowynq/ix++AmyP2M7j4eYlYPsRf/kb4YwZaAsvKcz6Bw/3tN+MlO4cBzcq4T6mvy7DtOuF93iwDdo9y/0eB9LDl6cDzRLzPgCl+fE2iHKNRvK8b3Sr/LfAAdNOttBuJSdJuj3Kf4i+w+8OW9fKXjYuyfTpeghOZpBUnLBfHiO0hf32HsGXFX1yXl/GYt0jEwtaP9df3ifJYHyojjsNirP8Aryap7raeC3/dFs+Vv+1qvFqFneN4vkv9MiT6l/6z/rLzomzfDi8B/SNi+Vz/PrtHuc9L/rqOccRbfJ4mxlg/3l9/YJTX67Byvhf6hZ334tta4AvgVMK+/MOeUwfsFWN/U4GlMc5L5xj3uSHa6x04zl/+YBzPV/HjnxsZs7/+L2B5xLLZ/vPYOsr2N5b3fAKXUZL0Ft8W4dXQHRhl+1Jfl/42O+PVFG8CeoYtT8P7EbeIsAQ2bH19oAh4O2zZFLwfXA3K8xrRrerd1Nwp1cHkKMvm+38bhC3r4v8dF7EtzrlCM5sA7Baxqrh/VmczGxrlOMVNKnviNYOF+y5WwL7xzrmiKMvH4jUJ7hsl1lj7LI6zj988F6kJXkLVDu8LYlvORTTt8ZqlJjnn4u4oX07FsW7VBOucm2VmC4A2ZlbfObc6bPUa59zsKPuL9too97HDlh+A91x9Fcf+YnLOjTGzdniJYfHz3wuv9mkwXh+3w11J830PIB843syOj7LLLKCxmTV0zq0IW54L/BQjjJeB2/CaPB8PW36G/7c8o0t/dM5FNvGDd/5D/R7NbAe819p859zcKNtPKMcxAXDOPWJmz+F1DeiJdy574tVQnmxmtznnbop3f+aNGP0Er8bzn27LPm3tgIZ4P7r+G6O74ya2HKzwGvAAMN3M3sJ7H37tnFsWb0xSNShJk1RXnKSUNsileF20hAa8X8yRCvy/6WHLivtvLYmxn8VRljX0/54bMzpPnTj3F66sOOqVsi5ScZxXl3HM4ji35VxEU9w5PZlTMBTHGmsU4SKglb9d+Gsh2usCor82tufYUHIetouftI/3b/gDHA7CS44GAhfi9XsD7znPoGQwRix1KOmnBV7tmotx/AVmNho4yMz2dM79amZNgIPxkq5p5Xg4pZ3/8Pf7Dv7fWK/FWMtL5ZzbCHzo34r7Tp6L11d0iJl94JybWtZ+zCwdb3qOfYHrnXNvRWxS/N5rS+nPRegzwjn3oJktx+tPexneiFRnZuOAq11JH0ep4jS6U1LdGv9vw1K2aeT/jfWhX95jNY2xfqdS7tPZOWel3KLVMET9IgxTVhxroqyLtc/ibeuVEee4iO3Lcy6iKX5Ooo6kTZDiWGPF1Cxiu6pybJxnBPBff1H/iNhWlfF8m3Pur8jdlnHY4tdyce3ZKXjJYLLmaCse2BDrtRhrebk45/Kcc4/j9XsDr3k5Ho/g9Wt71jl3d5T1xc/9B2U8D20i4nnZOdcd77PvMLy+awcCw/3EWKoBJWmS6oo7Krczs1iJWnHTSHl+xUdTPAKzT+QK/9fyAVHu863/t7wjzOJxQLSh+ZTMh1Xmr/ww5Y1zW85FNDPxErVOFmOajwjFzV/x1GIVKz4PfSNXmNnueCNt/4xo6kyUmMeOWB5tdG8irfP/hrelfQs0MLO9Enys9/ESp1P91+cZeLVfryf4OAA4b/TpH0Dz8Kk8wsT7WoxXtHMZlZldhVfbNcL/G03xe6C7P8qzXJxzq51znznnzsXr47cjyfm8kRSkJE1SmnMuF68pIQO4zyI6dJhZC0qa8IZt5+Em4iWFB5rZURHrLiF6H6wX8T6AbzazbpErzSzNtv0SR22J+OD34+qD15E62hQcsTyG1z/pIb9fU2ScWWYW/sG/LediK36foyeAbOApi5izzD9u47BFxc1ureLZv+8F/+9/w/flJ5P3433OPV+O/ZXH13jn6QAzOy58hV8+EJjFNvSbitjXwWZ2bLQveb8/1L/9Yni/t4f8v89GS5DNrLaZdS9vLM65TcDbeLWjVwCd8UZjLi3vvsrhZbzn8a7wzwAza0nJY4+LefP1RX3cZtYeKO6/V+r7y8yOBe4FfgaOd84VRNvOX/4oXq3qI2aWHWVfzcysQ1j5YDOL1h2puAYtmVf1kBSiPmlSGVwF7Ic3P1APMxuJ90t+F7wpKeoC94Q11W0T55wzs3OAkcB7ZlY8N1hnvP4+X+D1vQm/zwr/y/gD4Fu/v850vP5xrfBq+RrizbtVXl8AD5jZIXi1hMXzpOUC58QYVBDrsc00s7PxEprpZvYFXvKQ6cfZG2/agPbbei5KcQvePFBHALPM7BO82oqWwCC8JHuYv+0YvHN3l5l1xJveBOfc7aU8tolmdi9wDfCLmb2LNzLuEKAjXoJ0X5yxlot/ns7AO09vmdmHeDUne+DNubYOOL08z1UM7fGSrlVmNh6vE3oBXi3hYXh93ibhJePFsY02s+uAu4DfzewzvPnO6uC9d/rgnZt4n8dwL+HNr3dXWDmZ7sU7nycBe5jZCLz+gCfgJaZHE7tPaqSDgSfNm1j4a7yBCjXwfhQNxntPPOKcK2tgz6t4ieP3wJVRBgTMdc4N8/+/De+9cwHenIRf4vXTbOIftxfeKNXiwUVvArn+AJ25eLV6vfE+B6fgTZEi1UFQw0p10608N7wvlhvwPhDX4tUKFU/0eGiM+wwlxrB5SpkGAeiKl4Ss82+j8JKtsvb3GN6XZ64f40y8STqPjth2GBHTEkSs7+uvH+ofd5S/v3V4zSr7leexRmy3t3/8v/CakVcCv+BNnNl/e88FMaZLwftBeAne6NP1eEnU78AzREyDgTedxI94I96c9zFV9rnD+wKf4MeZi5cs3wjUjLLtXCKmbCnvuYy4zx7+c73If20uwvsS36M8r71S9t8IOBuvv9QMvOQ1Hy+xHoNX45oV474H4NV8LcSbK2+Zf34fBHLiPS9R9vu7/zhWlHLsrZ6vsh4//hQzUZbXx+v/tdB/7c7E+wHXzd/f/+KMu51/v8/xfnhs8Pc3D68p94h4XhPFr81SbmMj9mF4E9aOxnvf5eElahPwPttahm17Ad4Pvz/was1W4jWtX4M/TY5u1eNm/gtCRFKE3zw6BrjFOTc02GhEUpuZnYuX7F/gnHs66HhEEkl90kREJOXF6FfXEu9qBAV485SJVCnqkyYiIpXBe/7AiSl4g3Va413iqRbe/GTJnItPJBBK0kREpDJ4Ba9P1z/wBg2sxx8s4Zx7P8jARJJFfdJEREREUlCVq0lr1KiRa926ddBhiIiIiJRpypQpy51zjaOtq3JJWuvWrZk8WZc1ExERkdRnZpGXZgvR6E4RERGRFKQkTURERCQFKUkTERERSUFK0kRERERSkJI0ERERkRSkJE1EREQkBSlJExEREUlBStJEREREUpCSNBEREZEUpCRNREREJAUpSRMRERFJQUrSRERERFKQkjQRERGRFKQkTURERCQFKUkTERERibAuN5+iIhdoDErSRERERHwb8wpofd2n7D10BKN+XRJoLErSRERERIAPf/ybDjcND5W779YwwGggI9Cji4iIiAQsr6CI/e4YxZpN+QCcmNOSe47rFHBUStJERESkGhvz21LOevH7UHnUlX3YvUmdACMqoSRNREREqp3CIsegh8YxZ9kGAAa0b8JzZ+RgZgFHVkJJmoiIiFQrk+eu5LinvgmVP7qkF51a1A8wouiUpImIiEi14Jzjn89+y7d/rASgc4t6fHBRL9LSUqf2LJySNBEREanyZixcy6GPjA+VX//X/vTcvVGAEZVNSZqIiIhUaZe+MZWPpy0EoHn9bMZd3ZeM9NSfhUxJmoiIiFRJf63YQJ/7xobKT53ahYM7NgsuoHJSkiYiIiJVztCPpjNs4lwAstLT+GnoIGpmpgcbVDkpSRMREZEqY+naXLrdOTpUvvcfnThhv5YBRrTtlKSJiIhIlfDI6N95cOSsUPnnoYOoWzMzwIi2j5I0ERERqdTWbMyn860jQuUbDm3PeQfuFmBEiaEkTURERCqtV76Zy5APp4fKk/87kEZ1agQXUAIpSRMREZFKZ2NeAR1uGh4qX9BnN647pH2AESWekjQRERGpVD6atpDL3pgaKk+4th8tGtQKMKLkUJImIiIilUJeQRH73zmKVRvzATi+awvuO75zwFElj5I0ERERSXljf1vKmS9+HyqPuvJAdm9SN8CIkk9JmoiIiKSswiLH4P99xeyl6wHo374Jz5+Rg1lqXhQ9kZSkiYiISEqa8tdK/vHkN6Hyhxf3onPL+gFGVLGUpImIiEhKcc5xynOTmDhnBQB7N6/Hhxf3Ii2t6teehVOSJiIiIinj10VrOeTh8aHyq+fszwFtGwUYUXCUpImIiEhKuOyNqXw0bSEAzerVZPw1/chITws4quAoSRMREZFAzVuxkQPvGxMqP3VqFw7u2CzAiFKDkjQREREJzNCPpjNs4lwAMtONn4cOpmZmerBBpQglaSIiIlLhlq7Npdudo0Ple/6xNyfu1yrAiFKPkjQRERGpUI+O/p0HRs4KlX8aOogdamYGGFFqUpImIiIiFWLNxnw63zoiVL7ukPZc0Ge3ACNKbUrSREREJOle+fYvhvzfL6Hy5P8OpFGdGgFGlPqUpImIiEjSbMwroMNNw0Pl8/vsyvWH7BlgRJWHkjQRERFJio+nLeTSN6aGyhOu7UeLBrUCjKhyUZImIiIiCZVXUET3u0azckMeAMd1bcH9x3cOOKrKR0maiIiIJMzY35Zy5ovfh8ojrziQtk3rBhhR5aUkTURERLZbUZHj4Ie/YtaS9QD03aMxL565H2bV66LoiaQkTURERLbLlL9W8o8nvwmV/+/iXuzTsn6AEVUNStJERERkmzjnOPX5SXw9ewUAHZvvwEcXH0BammrPEkFJmoiIiJTbzMVrOfh/40PlV8/ZnwPaNgowoqqnwpM0M7sC+BfggJ+Bs4BmwJvAjsAPwGnOuTwzqwG8DHQFVgAnOufmVnTMIiIiUuLyN6fy4Y8LAWi6Qw0mXNufzPS0gKOqeir0jJpZc+AyIMc51xFIB04C7gEecs61BVYB5/h3OQdY5ZzbHXjI305EREQCMG/FRlpf92koQXvylC5MumGgErQkCeKsZgDZZpYB1AIWAf2Bd/31LwFH+/8f5Zfx1w8wDRMRERGpcLd+PIMD7xsDQEaaMfO2gzlk72YBR1W1VWhzp3PubzO7H5gHbAJGAFOA1c65An+zBUBz///mwHz/vgVmtgZoCCwP36+ZnQecB9CqVatkPwwREZFqY+m6XLrdMTpUvuvYvflnN33XVoQKTdLMrAFe7VgbYDXwDnBIlE1d8V1KWVeywLlngGcAcnJytlovIiIi5ffYl79z/4hZofJPQwexQ83MACOqXip64MBA4E/n3DIAM3sf6AnUN7MMvzatBbDQ334B0BJY4DeP1gNWVnDMIiIi1cqaTfl0vmVEqHztwe25sO9uAUZUPVV0n7R5QHczq+X3LRsAzADGAMf525wBfOj//5Ffxl//pXNONWUiIiJJ8uq3f22RoH1/40AlaAGp6D5pk8zsXbxpNgqAqXjNlJ8Cb5rZ7f6y5/27PA+8Ymaz8WrQTqrIeEVERKqLTXmF7HnTF6HyeQfuyg2H7hlgRGJVrWIqJyfHTZ48OegwREREKo2Ppy3k0jemhsrjr+lHyx1rBRhR9WFmU5xzOdHW6YoDIiIi1VReQRE97hrNig15ABzbpTkPnrBPwFFJMSVpIiIi1dC4Wcs444XvQuURVxxIu6Z1A4xIIilJExERqUaKihyHPjKemYvXAdCnXWOGnbUfmis+9ShJExERqSam/LWKfzw5MVT+4KKe7NuqQYARSWmUpImIiFRxzjlOe/47Jsz2LtjTodkOfHLpAaSlqfYslSlJExERqcJ+W7yOwf/7KlR+5Zxu9G7bOMCIJF5K0kRERKqof785lf/70buIT9MdajDh2v5kplf0PPayrZSkiYiIVDHzV26k971jQuXHT+7CYZ2aBRiRbAslaSIiIlXIbZ/M4PkJfwJgBjNuOZjsrPSAo5JtoSRNRESkCli6Lpdud4wOle88Zm9O3r9VgBHJ9lKSJiIiUsk9PmY29w3/LVSedvMg6mVnBhiRJIKSNBERkUpqzaZ8Ot8yIlS+5uA9uKjv7gFGJImkJE1ERKQSem3SX9z4wS+h8vc3DqRx3RoBRiSJpiRNRESkEtmUV8ieN30RKp/buw03HtYhwIgkWZSkiYiIVBKf/rSIi1//IVQef00/Wu5YK8CIJJmUpImIiKS4/MIietz1JcvXbwbgmH2b89CJ+wQclSSbkjQREZEU9tWsZZz+wneh8ogrDqRd07oBRiQVRUmaiIhICioqchz6yHhmLl4HQO+2jXj57G6Y6aLo1YWSNBERkRTzw7xVHPvExFD5/Yt60qVVgwAjkiAoSRMREUkRzjlOf+E7xv++HIA9m+3Ap5ceQFqaas+qIyVpIiIiKeC3xesY/L+vQuWXz+7Gge0aBxiRBE1JmoiISMCufOtH3p/6NwCN69Zg4nX9yUxPCzgqCZqSNBERkYDMX7mR3veOCZUfO3lfDu+0c4ARSSpRkiYiIhKAOz6dwbPj/wyVf731YLKz0gOMSFKNkjQREZEKtGzdZva7Y1SofOcxe3Py/q0CjEhSlZI0ERGRCvLE2Nnc+8VvofK0mwdRLzszwIgklSlJExERSbI1m/LpfMuIUPnqwXtwcb/dA4xIKgMlaSIiIkn0+qR53PDBz6HydzcOoEndmgFGJJWFkjQREZEk2JRXyJ43fREqn3NAG4Yc3iHAiKSyUZImIiKSYJ/9vIiLXvshVB5/TT9a7lgrwIikMlKSJiIikiD5hUX0vPtLlq3bDMAx+zbnoRP3CTgqqayUpImIiCTA+N+Xcdrz34XKw/99IHvsVDfAiKSyiytJM7MOwACgG7ATUBNYCcwCJgAjnHObkhWkiIhIqioqchz26AR+XbQWgN5tG/Hy2d0w00XRZfvETNLMe3WdBlwKdAVWAT8By4HNQH3gEOAKYIOZvQXc5Zz7M/oeRUREqpap81ZxzBMTQ+X3LuxJ110aBBiRVCWl1aT96v99BTjNOTcz2kZmVgsYDBwP/GxmFzjnXk1smCIiIqnDOccZL37PV7OWAdB+p7p8dllv0tJUeyaJU1qS9l/gPeecK20HzrmNwAfAB2bWAmiRwPhERERSyqwl6xj00Feh8ktnd6NPu8YBRiRVVcwkzTn3bnl35pxbACzYrohERERS1JVv/8j7P/wNQKM6WXxz/QAy09MCjkqqKo3uFBERKcP8lRvpfe+YUPmxk/fl8E47BxiRVAfxju78E4jV7FkErAWmAY8556YkKDYREZHA3fnZrzzz1R+h8q+3Hkx2VnqAEUl1EW9N2nvACUA2MApYBjQGDgI2AJOB3sCpZna4c254EmIVERGpMMvWbWa/O0aFyrcf3ZFTu+8SYERS3cSbpC3FmxPtcOdcbvFCM8sGPgbmAR2Bj4BbACVpIiJSaT05dg73fFEyqcG0mwZRr1ZmgBFJdRRvknYZcF54ggbgnNtkZg8BzznnbjezZ4HXEh2kiIhIRVibm0+noSNC5f8Mascl/dsGGJFUZ/EmafWBpjHWNQXq+P+vAQq3NygREZGK9uZ387ju/Z9D5e9uHECTujUDjEiqu3iTtE+Ae81sDfCJcy7PzLKAI4F7/fUAewNzEh+miIhIcuTmF9Lhpi8o8ofHnd2rDTcd0SHYoESIP0m7AHgJeBdwZrYOqAsYXp+0C/3tFgI3JDpIERGRZPjs50Vc9NoPofJXV/ejVcNaAUYkUiKuJM05txo4ysz2AnLwLrK+GJjsnJsetl25J8AVERGpaPmFRfS6+0uWrtsMwFH77MzDJ+0bcFQiWyrXZLZ+Qja9zA1FRERS1ITfl3Pq85NC5S/+3Zv2O+0QYEQi0cVM0syst3NufHl2Zmb1gFbOuZ/L3FhERKQCFRU5jnhsAtMXrgXggN0b8co53TDTRdElNZVWk/a2mf0BPA984JxbFWtDM+sFnAScBlwDKEkTEZGU8eP81Rz9+Neh8nsX9qDrLjsGGJFI2UpL0nbFmx/tZuBpM5sF/AIsBzbjTcvRBtgX70oEnwEDnXOTkxqxiIhInJxznPni94ybtQyAPZrW5fPLe5OWptozSX0xkzTn3CbgHjO7FxgA9Ae6Au2BmsBK4DfgdeBD59zS5IcrIiISn9+XrOOgh74KlYedtR9992gSYEQi5VPmwAHnnMO7XueosrYVERFJBVe9PY33flgAQMPaWXxz/QCyMtICjkqkfMo1ulNERCSVLVi1kQPuGRMqP/rPfTmi884BRiSy7ZSkiYhIlXDXZ7/y9Fd/hMozbh1MrSx9zUnlpVeviIhUasvXbybn9pIeObcd3ZHTuu8SYEQiiaEkTUREKq2nxs3h7s9nhsrTbhpEvVqZAUYkkjhK0kREpNJZm5tPp6EjQuUrD2rHZQPaBhiRSOKVK0kzswZAR6Al8LlzbpWZ1QTynHNFyQhQREQk3Fvfz+Pa90rmTP/uhgE02aFmgBGJJEdcSZqZpQN3ARfjTVzrgP2AVcB7wGS8SW9FRESSIje/kL1uHk5hkQPgrF6tufmIvQKOSiR54p005k7gXOASvCsRhE/V/CFwRILjEhERCfn850W0H/JFKEH76up+StCkyou3ufN04Drn3It+rVq4OXiJm4iISEIVFBZxwD1jWLw2F4AjOu/Mo//cN+CoRCpGvElafbxkLJosIDJxExER2S5fz17OKc9NCpU/v7w3ezbbIcCIRCpWvEnaL8BRRL801CHADwmLSEREqrWiIseRj0/gl7/XAtBzt4a89q/9MdNF0aV6iTdJux14z8yygXfwBg7sY2bHAOcDR4XRkjQAACAASURBVCYpPhERqUamzV/NUY9/HSq/d2EPuu6yY4ARiQQnriTNOfehmZ0M3Auc7S9+DvgbOM05NzxJ8YmISDXgnOPsYd8z5rdlALRrWofPLz+Q9DTVnkn1Ffc8ac65t4G3zawd0AhYCfzmnHPJCk5ERKq+2UvXMfDBr0LlF8/aj357NAkwIpHUEO88aQcC851zfzrnZgGzwtbVBfZ1zn0VcwciIiJRXP3ONN6ZsgCABrUymXTDQLIy4p0dSqRqi7cmbSywyczOd869GrGuAzAGjfAUEZE4LVi1kQPuGRMqP3zSPhy1T/MAIxJJPeX5ufIpMMzMHokyV1rczKy+mb1rZjPN7Fcz62FmO5rZSDP73f/bwN/W/OPNNrOfzKzLth5XRERSw92fz9wiQZtx62AlaCJRlCdJux84GjgNGGNm29ph4GHgC+dce6Az8CtwHTDaOdcWGO2XwZveo61/Ow94chuPKSIiAVu+fjOtr/uUp8Z5027ednRH5t59GLWyynUZaZFqo1wN/865T4D98QYOTDWzHuW5v5ntABwIPO/vL885txpvDraX/M1ewksG8Ze/7DzfAvXNrFl5jikiIsF7etwccm4vmWpz2k2DOK37LgFGJJL6yv3zxTk3y8y6AS/j9UV7pRx33xVYBrxoZp2BKcDlQFPn3CJ//4vCaumaA/PD7r/AX7YofKdmdh5eTRutWrUq70MSEZEkWZubT6ehI0LlKw9qx2UD2gYYkUjlsU1DaJxz651zxwJ3UDJvWjwygC7Ak865fYENlDRtRhNtgpytpvxwzj3jnMtxzuU0bty4HOGIiEiyvP39/C0StEk3DFCCJlIO8daktSGi9grAOXebmY0BdotzPwuABc654ouxvYuXpC0xs2Z+LVozYGnY9i3D7t8CWBjnsUREJAC5+YXsPXQ4+YXeb+oze7Zm6JF7BRyVSOUT7xUH/ipl3QRgQpz7WWxm881sD+fcb8AAYIZ/OwO42//7oX+Xj4BLzOxNvL5wa4qbRUVEJPV88ctiLnh1Sqg87uq+7NKwdoARiVReMZM0M7sXeMQ5t8D/vzTOOXdtnMe8FHjNzLKAP4Cz8Jpd3zazc4B5wPH+tp8BhwKzgY3+tiIikmIKCovofe8YFq3JBeDwTs147GTNmiSyPSzWVZ3M7E/gaOfcNP//0jjn3K4Jj24b5OTkuMmTJwcdhohItTFx9nJOfm5SqPz55b3Zs9kOAUYkUnmY2RTnXE60dTFr0pxzbaL9LyIiAlBU5Dj6ia/5acEaAHrs2pDXz90fM10UXSQRNIOgiIiU27T5qznq8a9D5Xcv6EFO6x0DjEik6imtT9rOwC7OuW8ilu8DDAHaA0uAR51zHyQ1ShERSQnOOc55aTJfzvQG4e/epA7D/30g6WmqPRNJtNJq0u4A9gB6Fi8ws7bAeKAIGIk39ca7ZjbIOTc6mYGKiEiwZi9dx8AHvwqVXzxzP/q139YrBIpIWUpL0noBj0QsuxKoAeQ4534CMLP/w7/2ZlIiFBGRwF3z7jTenrwAgHrZmXx/40CyMrZpPnQRiVNpSdrOeBc/D3ck8E1xguZ7AXgq0YGJiEjw/l69iV53fxkqP3zSPhy1T/MAIxKpPkpL0jYC2cUFM2sDNMNLysKtAuonPjQREQnSPV/M5Mmxc0LlGbcOplaWxpuJVJTS3m0/AqcBn/jlU/Cum/lJxHa7EeWSUSIiUjmtWL+ZrrePCpVvPWovTu/ROriARKqp0pK0W4ExZjYN71qa/YExYdfdLPYPIHKZiIhUQs98NYc7P5sZKv9400HUr5UVYEQi1Vdpk9lOMLN+wAV4zZl3APeFb2NmjfFGeg5LYowiIpJk63Lz2XvoiFD53wPb8u+B7QKMSERK7VxQ1sXTnXPLgKMSHZSIiFSctyfP55p3S8aDTbphAE13qBlgRCICuuKAiEi1lZtfSKehI8grLALgjB67cMtRHQOOSkSKKUkTEamGvvhlMRe8OiVUHvufvrRuVDvAiEQkkpI0EZFqpKCwiAPvHcPCNbkAHLZ3Mx4/pUvAUYlINErSRESqiYmzl3PycyWD8T+7rDcddt4hwIhEpDRK0kREqjjnHEc/MZFp81cD0H3XHXnj3O6Y6aLoIqksriTNzFqVsroIWOucW5uYkEREJFF+WrCaIx/7OlR+54Ie7Nd6xwAjEpF4xVuTNhfvagMxmdk84BHn3EPbG5SIiGwf5xz/emkyo2cuBWDXxrUZeUUf0tNUeyZSWcSbpJ0M3AP8AnwELAMa482R1hG4E8gB7jUzlKiJiARn9tL1DHxwXKj8wpk59G/fNMCIRGRbxJukDQQ+cs5dGrH8aTN7FOjpnDvdzNbjXaFASZqISACuffcn3po8H4B62Zl8f+NAsjLSAo5KRLZFvEna8XjX6IzmI+Bd///P8ZI0ERGpQAtXb6Ln3V+Gyg+ftA9H7dM8wIhEZHvFm6TlAr2AUVHW9fLXAxiwIQFxiYhInO79YiZPjJ0TKk+/ZTC1a2jwvkhlF++7+BlgiJk1BD5myz5pF+BdfB2gJzAt0UGKiMjWVqzfTNfbS347Dz2iA2f2ahNgRCKSSHElac65IWa2ErgauARvpKcBi4GrwwYKvAW8kIxARUSkxHPj/+D2T38NlX+86SDq18oKMCIRSbS468Odcw+Z2cNAS2AnvARtvnOuKGyb6YkPUUREiq3LzWfvoSNC5csHtOWKg9oFGJGIJEu5Oi34Cdlf/k1ERCrQO5Pnc/W7P4XKk24YQNMdagYYkYgkU9xJmpntDBwOtAAiPxWcc+7aRAYmIiKe3PxCOt0ygrwCr+HitO67cNvRHQOOSkSSLd7LQh0DvAGkA0uBvIhNHKAkTUQkwYZPX8z5r0wJlcf8py9tGtUOMCIRqSjx1qTdCYwAznTOrUxiPCIiAhQUFtHnvrH8vXoTAIft3YzHT+kScFQiUpHiTdJaApcqQRMRSb6Jc5Zz8rOTQuVPLzuAvXauF2BEIhKEeJO0icAeRJ/MVkREEsA5x9FPTGTa/NUAdGu9I2+d3x0zXRRdpDqKN0m7EnjNvzbnSGB15AbOuY2JDExEpDr5ecEajnhsQqj89vk96NZmxwAjEpGgxZukFY/5fhFvkEA06dsfjohI9eKc49yXpzDq1yUA7NqoNiOv7EN6mmrPRKq7eJO0s4mdnImIyDaYs2w9Ax4YFyq/cGYO/ds3DTAiEUkl8V4WaliS4xARqVauf/8n3vhuPgB1a2QwechAamSoQUJESpTrigMiIrJ9Fq7eRM+7vwyVHzqxM8fs2yLAiEQkVcVM0szsO7x50WaY2feU0dzpnOuW6OBERKqS+4bP5PExc0Ll6bcMpnYN/VYWkehK+3SYDmwK+1990kREtsHKDXl0uW1kqHzzER04q1ebACMSkcogZpLmnDsr7P8zKyQaEZEq5rnxf3D7p7+GylOHHESD2lkBRiQilUW81+48C3jfObcmyfGIiFQJ6zcX0PHm4aHyZQPacuVB7QKMSEQqm3g7QzwFPGlmI4A3gQ+dcxuSF5aISOX17pQF/OedaaHyt9cPYKd6NQOMSEQqo3iTtKbAscAJwDAg38w+B94APnXO5SYnPBGRyiM3v5B9bh1Bbn4RAKd2b8XtR+8dcFQiUlnFO0/aauAF4AUzawgch5ewvQVsNLOPnHOnJi9MEZHUNmL6Ys57ZUqoPOY/fWnTqHaAEYlIZVfusd/OuRXA08DTZnYY8AzwT0BJmohUOwWFRfS9fywLVnmD4Q/puBNPnto14KhEpCood5JmZnsDJ/q3XYE5wJ0JjktEJOV9M2cF/3z221D5k0sPoGPzegFGJCJVSbyjO/fEa948EdgDmA+8DbzpnPsheeGJiKQe5xzHPjmRqfNWA7Bf6wa8fX4PzHRRdBFJnHhr0qYDi4B3gHOcc98kLyQRkdT1y99rOPzRCaHyW+d1Z/9dGwYYkYhUVWUmaWaWBpyON+3GuuSHJCKSms57eTIjZiwBoE2j2oy6sg/paao9E5HkiKcmLQ14ETgcGF7GtiIiVc6cZesZ8MC4UPn5M3IYsGfTACMSkeqgzCTNOVdgZn8BGksuItXO9e//zBvfzQOgTo0MpgwZSI2M9ICjEpHqIN4+afcAN5rZeOfcsmQGJCKSChat2USPu74MlR88oTPHdmkRYEQiUt3Em6QNApoBc81sCrAEcGHrnXPuxEQHJyIShPuH/8ZjY2aHytNvGUztGuWesUhEZLvE+6nTCPgtoiwiUqWs3JBHl9tGhso3H9GBs3q1CTAiEanO4r0sVL9kByIiEqTnJ/zJbZ/MCJWnDjmIBrWzAoxIRKq7bbnigOE1fS51zhUkPiQRkYqzfnMBHW8uGbh+Wf/duXLQHgFGJCLiSYt3QzM71MwmAbl4Vxzo5C9/1sx03U4RqXTem7JgiwTtm+v7K0ETkZQRV5JmZqcDHwEzgfOA8NkbZwHnJD40EZHk2FxQyJ5DvuCqd6YBcMr+rZh792E0q5cdcGQiIiXibe68EbjPOXe9maXjTW5bbDrwn4RHJiKSBCNnLOHclyeHyl9e1YddG9cJMCIRkejiTdJ2AUbGWJcL7JCYcEREkqOgsIh+D4xl/spNAAzeqylPn5YTcFQiIrHFm6TNB/YFvoyyLgeYHWW5iEhK+PaPFZz0zLeh8ieXHkDH5vUCjEhEpGzxJmnPAzeb2RLg//xlZmYDgGuAW5MRnIjI9nDOcdxT3zDlr1UA5OzSgHcu6IE3SF1EJLWV57JQLYGXgEJ/2UQgHXjaOfdIEmITEdlmv/y9hsMfnRAqv3led7rv2jDAiEREyifeyWwdcLGZPQgMwLviwErgS+fcrCTGJyJSbue/Mpnh05cAsEvDWoy+sg8Z6XHPOCQikhLKNZmtc24OMCdJsYiIbJc/lq2n/wPjQuVnT8/hoA5NA4xIRGTbxUzSzKwWUN85tzBi+c7AVUB7vAutP+2cm5TUKEVEynDjBz/z2qR5ANTOSueHmw6iRkZ6wFGJiGy70mrSHgAOAPYuXmBmTYGpwI7ANKArcLKZ9XLOTUlmoCIi0Sxas4ked5UMPH/g+M78o2uLACMSEUmM0pK03sCwiGXX4PVHO9g5N9LMagLDgSHA0UmJUEQkhgdG/MajX5bMAPTLLYOpU6PclyQWEUlJpX2atQR+ilh2NDDFOTcSwDmXa2aPAg+W56D+VQsmA3875w43szbAm3g1dD8Apznn8sysBvAyXo3dCuBE59zc8hxLRKqelRvy6HJbyfzaQw7vwDkHtAkwIhGRxCttuFMRYdfoNLNmQBtgbMR2i4HG5Tzu5cCvYeV7gIecc22BVZRcC/QcYJVzbnfgIX87EanGXpjw5xYJ2g9DDlKCJiJVUmlJ2nTgyLDysYADPo/YriWwNN4DmlkL4DDgOb9sQH/gXX+TlyhpOj3KL+OvH2CahVKkWtqwuYDW133KrZ/MAODS/rsz9+7D2LF2VsCRiYgkR2nNnfcAH5pZK7zaspPxBguMjdjuCLwmynj9D69vW12/3BBY7Zwr8MsLgOb+/83xLkmFc67AzNb42y8P36GZnQecB9CqVatyhCIilcH7PyzgyrenhcoTr+vPzvWzA4xIRCT5YtakOec+Bk4BGgA9gPeAI/2JbQEws8Z4U3G8Gc/BzOxwYGnESNBoNWMujnXhsT7jnMtxzuU0blzellcRSVWbCwrpcNMXoQTtn91aMffuw5SgiUi1UOowKOfcG8AbpaxfBnQpx/F6AUea2aFATWAHvJq1+maW4demtQCK52ZbgNecusDMMoB6eFc6EJEqbtSMJfzr5cmh8pdX9WHXxnUCjEhEpGJV6HVSnHPXO+daOOdaAyfhXVbqFGAMcJy/2RnAh/7/H/ll/PVfhtfkiUjVU1jk6HPfmFCCNnivpsy9+zAlaCJS7aTKhELXAm+a2e14k+U+7y9/HnjFzGbj1aCdFFB8IlIBJv2xghOf+TZU/uTSA+jYvF6AEYmIBCewJM05NxZ/EIJz7g+gW5RtcoHjKzQwEalwzjmOe+obpvy1CoAurerz3oU90WBuEanOUqUmTUSqqV/+XsPhj04Ild84tzs9dmsYYEQiIqlBSZqIBObCV6fw+S+LAWi1Yy2+vKoPGekV2lVWRCRlKUkTkQr35/IN9Lt/bKj8zGldGbTXTsEFJCKSgmImaWZWRJQ5yWJxzqUnJCIRqdL++38/8+q38wDIzkxn6k0HUTNTHx8iIpFKq0m7jJIkLRO4CliPNz3GUqAp3mWbagMPJDFGEakCFq/Jpftdo0Pl+4/vzHFdWwQYkYhIaouZpDnnHiv+38weBCYBx0dcceA64B28C6+LiET14MhZPDL691D5l1sGU6eGeluIiJQm3k/J04FTIieSdc45M3sWeB24PNHBiUjltmpDHvveNjJU/u9he/Kv3rsGGJGISOURb5KWDuwJDI+ybi8q+MoFIpL6Xvz6T275eEao/MOQg9ixdlaAEYmIVC7xJmmvAXf618/8CK9PWhO8Pmm3UnKFABGp5jZsLmCvm0t+z13cbzeuHtw+wIhERCqneJO0K4F8vITsnrDlm4GngWsSHJeIVEIfTF3AFW9NC5UnXtefnetnBxiRiEjlFVeS5pzLA64ws9uATngjOxcDPzvnViYxPhGpBDYXFNL1tlGs31wAwD+7teKuY/cOOCoRkcqtXMOr/IRsbHJCEZHKaPSvSzjnpckl5av6sFvjOgFGJCJSNcSdpJlZJ+BGIAdoAfRwzv1gZncAE5xznycpRhFJQYVFjoEPjuPP5RsAOKhDU545rasuii4ikiBxjco0s0OAKcBOwMt4k9sW2wxcmvjQRCRVfffnSna74bNQgvbxJQfw7Ok5StBERBIo3pq0u4Bhzrlz/RGeN4et+xG4IOGRiUjKcc5xwtPf8P3cVQDs26o+71/YU8mZiEgSxJuktQf+4/8feT3PtcCOCYtIRFLS9IVrOOyRCaHyG+d2p8duDQOMSESkaos3SVsKxJomfC9gXmLCEZFUdNFrU/js58UAtGiQzdj/9CUjXXNYi4gkU7xJ2pvArWY2A/jGX+bMrB1wLZrMVqRK+nP5BvrdPzZUfua0rgzaa6fgAhIRqUbiTdKGAB2AcXjzowF8iDeQYARwZ+JDE5EgDfm/X3jl278AqJmZxo83DaJmZnrAUYmIVB/xTma7GTjczAYAA4BGwEpgtHNuZKl3FpFKZfGaXLrfNTpUvu+4Thyf0zLAiEREqqe4kjQzawUscs6NBkZHrMsAdnbOqV+aSCX30MhZPDz691D5l1sGU6dGuea8FhGRBIn30/dPoAfwXZR1nf3lagcRqaRWb8xjn1tLKsX/e9ie/Kt3rLFCIiJSEeJN0kqbBKkm3oS2IlIJvTRxLjd/ND1UnvLfgTSsUyPAiEREBEpJ0vzLQO0TtuhQM2sfsVlN4ARgVhJiE5Ek2rC5gL1uHh4qX9xvN64eHPkWFxGRoJRWk3YMJVcWcMBNMbb7Ezg/kUGJSHL939S/+fdbP4bKX1/Xn+b1swOMSEREIpWWpN0J3I/X1LkW6A98H7FNnnMuP0mxiUiCbS4oJOf2UazLLQDgpP1acvc/OgUclYiIRBMzSfOTr+IETFOLi1RyX85cwtnDJofKo67sw+5N6gQYkYiIlCbeKTguw5tm47oo6+4C/nbOPZbo4ERk+xUWOQ56cBx/LN8AwMA9m/Ds6Tm6KLqISIqLd3TnRXhNn9HMAq4GlKSJpJjv567k+Ke+CZU/uqQXnVrUDzAiERGJV7xJ2i7A7Bjr/gRaJyQaEUkI5xwnPvMt3/25EoDOLevzwYU9SUtT7ZmISGURb5K2CtgDGBtl3R54AwtEJAVMX7iGwx6ZECq/fu7+9NytUYARiYjItog3SfsYGGpmE51zPxcvNLOOeNN0fJiM4ESkfC5+7Qc+/XkRAM3rZzPu6r5kpGvcj4hIZRRvknY90BOYamZTgUVAM2Bf4BdgqwEFIolUVORYsSGPvIJCsjLSaVg7S013YeYu30Df+8eGyk+d2pWDO+4UXEAiIrLd4krSnHMrzWw/4AygH9AQmAM8A7zsnNNloSqByproFBU5fluyjnNfnsyCVZto0SCbZ0/PYY+mdVMm/iDP7c0f/sJL3/wFQI2MNKbdPIiambqUrohIZWfOuaBjSKicnBw3efLksjesZipDohPLsnWbOeaJr1mwalNoWYsG2XxwUS8a1w3+GpNBndsla3PZ/87RofK9x3XihJyWSTuepI7K+oNLRLZmZlOccznR1qmzSjWxYkNeKIkAWLBqE+e+PJkVG/ICjqxseQWFWyRo4MWfV1BYYTEUFTmWrdvM36s2smzdZoqKSn7cxDq3i9fmbrFdIv1v1KwtErSfhw5KaoJW2uOXilX8o+CYJ76m1z1jOOaJr/ltyTo9JyJVUGkXWF8KDHbOTTWzZXjX74zJOdck0cFJ4qRCorOtsjLSadEge6uatKyMimnSK6umLNa5Xbh6E2s25Se0Rm31xjz2uXVkqHzjoXty7oG7JmTfsVTmWtiqKNaPglSpWRaRxCmtT9rjwJKw//UzrRILOtHZHg1rZ/Hs6TlbJQkNa2dVyPFXbMjjoZG/MeTwDtTPzqTIOdZsymfB6o1kZ2aQnRX93K7YkMe/3/oxYV+eL02cy80fTQ+Vp/x3IA3rJP9LWUlBaqnMP7hEpHxKu3bnLWH/D62QaCRpgk50tkdamrFH07p8cFGvQPrgFBUVcfXgPTBLIz0N0sy449MZjJixNHQe3zh3f2Yv3UCjOllkZ2WQkWZkpBsndm2x3V+eG/MK6HDT8FD5wr67ce3B7bf3YcVNSUFqqcw/uESkfOKdgkMquaATne2VlmaB1dpkpBsOWLByI7Wy0tmYV8hF/XZn2bo8ps5fzbkvT+ad87tTp0YGZsaZL34XSoSfPq0rdWpu+5fnhz/+zeVv/hgqf31df5rXz07Ao4qfkoLUUpl/cIlI+cQc3WlmX5ZnR865/gmJaDtpdKdsq1gj5pas2cQfyzdw9bs/hb4U7zuuEwD/fHYS+7asz73Hd8IoSdCKtWiQzbsX9GCneuVLrPIKisi5fSRrcwsAOCGnBfce1zlxD7Yc1Cct9Wh0p0jVUdroztJq0lZElHsATYEpwFKgCdAFr9/aN4hUYqUlIvlFLpSggdfUd/W7P/HK2d0AuOWovVi9IQ8zi9EsWFSuWMbMXMpZw74PlUdd2Yfdm9TZ7se3rV/qqV4Lm+yEJRUToiBrlkWk4pTWJ+344v/N7By8a3T2dM7NC1veCvgEGLn1HkQqj9I6xxcWuajJV6FztGiQTcPaWZz4zLfcfezeDOrQhH90bUn97ExWb8rnvSnzSY/zC72wyDHooXHMWbYBgAHtm/DcGTmYGQUFRSxdv5n8wiIy09NoUqcGGRnxzaCTiJqwVE0Kkl3Lp1pEEQlSvH3SbgSuDE/QAJxz88zsZuBB4NlEByeJUVwTUFRURKED51zK1AikitI6x2emW9TkKzsznbfO606Bn8Q1qpPFJf3bctFrP4S+0J84pQs140imvp+7kuOfKqmQ/uiSXnRqUR+AgoIiZi5ZxwWvTgnt96lTu9K+ad24ErWqPDoz2Y+tKp87EUl98U5muxMQ6xOpBl7Tp6Sg4pqAGz/4idnLNnDC099oAswoMjPSaNFgy35jLRpkk5mRRnZmGpcOaMdtn8zgxGe+5bZPZnDpgHZkZ6Vx4jPfkp5mtGiQTe0amaEEDbwv9Ite+4FNBUUxJ4B1znHi09+EErROLerxx52HhhI0gKXrN4cStOL9XvDqFJauj+9qbFVpdGbkpLrJfmxV6dyJSOUTb03aWOAeM5vjnAv1yvev53kPMC4JsUkCFNcE3H3s3lz73k+qEYghI82477hOWw0OyEgzNuUXcmFEknThq1N4+7zuLFi1iZoZabx8djeKXPRm0aIixzFPfL1VM9mMhWs59JHxoW1f/9f+9Ny90VaxFRQW0bhOjdA8bas35fPU2DkUFMbX1y0rIz1qTWBlG50Zrenx9X/tX+bI0/z8Qpau30xBkSMjzWhSpwaZcV7btDh5j9x/ZpxNzSIi2yPeJO084CNgkpktoWTgQFPgJ3+9pKC8gkIa16lBs/rZgdYIVGTn62051qa8Qu79wpuwdtdGtaiRmUFhYRGb8gpj9kkrKHLs27I+i9d6NV2vnhM9YchIT9sqKb7k9R/45KdFADSvn824q/uSkR79i79GRhpDj+zAyg35AGSle+UacSYKDbIzuWxAu62aSxtkZ8Z1/1QRrenx9k9n8PRpXTn/lZLHFj4dRX5+ITOXrg8l2S0aZPPkqV1p36ROXIlaacm7iEiyxZWkOecWAF3M7FBgP7zmz8XA9865z5IYn2ynrIx0LhvQlnkrNgY211VFdr7e1mNlZaSzbP1mRs9YQrMeu3DWsG9D938tRm1NWppx1aCS5Ccjjahf6MWHXbBqE78vWcd+d4wK7eepU7twcMdmZT6ujXmFDPnwl62mAInHqk35UZtLw2tRy0ps40l8k5WIF+93Y17BVsnyiBlLue2ojjFHni5dvzlqLehb53WneYNaZR47PHkvroW894vfeOzkfaH2dj80EZFSlWsyWz8hU1JWiTSsnUWbRrW54q0fuecfnUJNni0aZPP0qV0rZALMiux8va3HKp4gNDM9bYu5zhas2sQdn87ghTNz+HtVbmgy2x1rZ5KeZlvUUOYWFEX9Qn/4pH0AqFMjnZOfmwR4E+T+fNMgsmuU/RaMNQXIm+d1j+uclNWvqqzENp7EN1mJePh+hxzeIUaynBbzuS0opRY0HsXJ+/mvTNnimJWtqVhEKqe4kzQzqwGcDeQALYBLnHO/m9mJwE/OuV+TFKNsh7Q0o1YN74vm/uElCcTGvEKa1a9ZIfNJVWTn6209VvFcYPNXbdzq/svW5ZFf4LaoyXrq1K5s2FzA8vV5e/1+bwAAIABJREFUocRh8ZrcqF/oqzZ6zZTrN5fEsNMONVmfVxhXkharuTXeQR9lXTGgrMQ2nsQ31jbvX9QTw7a5di38uqlN6tbg5bO7cffnv25xSa7SfmhkpEUfmRtvc2WD7EyeOrVrpW8qFpHKKa4kzcza4c2FVg9vMtu+QF1/dW/gMOD0JMQnCdCodo3QZWTOf2VK6HJFO9RI3BdNaTUpFXlZoe09Vkaa8e4FPVixIY+nxs5h6vzVXDagLQ+PnrVFDdkjo2dxw6EdcM7xxClduOi1H3hgxCweOqEzV7w9LXQOeu7WkKMe/3qr43hJVhErN2z2+r05R2ZaGmkGaWlpWyQzGf7o0cjHFO/8a2VdRqisxHZzHDVxm/K3bopcsGoTGzcXcurzk7a5dq2oqIgzerbZogb48ZP/n70vD4+iTLc/X1X1lu6QPWyJgshiwISkJSToOCgOIyPK1bAoCZiAhMVtHAW84zAyF31+LDq4sY8T9iWAc3XgqiiuAyIYtpEoIhBNBEkIadLprbqr6vdHdVW6uqqSYNgc6zyPj5Duqv6q0vR3+n3Pe04OZt/TF4IA2Mwt/15T7GZMv7MPas6JazPTFKbf2QcpbawgN4R/19G/++fvzTQGbgwYMHDJ0dZK2isAvgdwN4AmAGzEYx9DnPA0cJWCogh6pjiw/qGBqHUHUO9h8fL73+CJ3/S+aLqwlqotFyNrsK2aKJoClhU5MXmtUkieYDPJlg1mhkaCzYQGXxBsiIPNTEOAgB9dAcVx8woysWr3SfTq6FARhXkFmTAzBJ3irNh28BRWT8gFTRGccvkw974bEQjxmLjqC5R/UQMA6JXqwKLCHJzzsHD5gthfVQ93IIS6swGFhu3FUVl4/V8nFL8bC0NhSZFTJX5v6+BAa4kBrRFbEv579OPSPT96xo0fz/s1n3PyrKddbe4QL6imkh9evx8rS3Jxx18/bpX4NQU5nHUHVHq+JLsZiebWP/7YEIcdlbXYUVmr+PmzdxsWHAYMGLj0aCtJ+xWAUYIguAgh0V9dzwBoXfls4IqiwRfE2L99rthEK0+7L5ourKVqTHtjhS5UE5XisGDh6P7oHGcFDwFWhsb3DV6M/3tz8PnSIide2fkN6twsZtzZG/4gL2/k0tpnbj2MTWHdVzRRmLn1MMqKB6Bk5T4sGJmJ7YdO4T5nGj76+gxO1vuwo/KMvP5tj9yMUNieI8ZMw0xTGDXgGpxtYlVasyc3H8Ks4RlY+N5RzL6nHwRBAE0RbDtYg7LiAaApAo4XsOWL71Fyy3Vtvn/n/QGwIV7UYoU4cBwPihL/KbdGok00kauF0uOLC3NgoolMzlMcFpXmcUlhDv785hHVe8IX5MDzQpt+/3qtXrc/KP+5JeLnYzlNPd+m0rw2Cf+NcHkDBgxcSbSVpPkB6CVEdwXgujjLMXCpcKl1Ya1tZu2JFbpQTVSKwwKW43H/ij2K6kmKw4KaBp883ThreAYAYPqWw3hxVJbm/QH0iUJTQGzxle06iUdu7wl/KIRln1bJz7EyFDZPHYTOcRbV8Z5ACJ3jrJrnvT7VgWm3XY/Ryz5DTYMPu5++DcMyu8gkz8tyGJbZBYLQrEnTqzTyvIAfXF40eIMKkrWkyIlrEq1o8nMwMRQcFhpzRvSTzx9ZpQtyArYf+kFFEscP6g6Ak++ppHnskWJH9Tnx/tRFGe6mJdhwvLYJnkBIVf3SugaG1vYpq3U3n7el9zGn413HCW3T812MKrABAwYM/FS01ZHxPQB/JITERfxMCA8TPApj4vOqh0SiInGhFYFot/dI4bq0mUmvcTE3s7YQzMjnTBncQ1X5mr7lMKYM7qE4vkeKHT1THZg1PANBjte8PwDkRIHoxySiUOBMR+HfPseQFz9VPMcf4jF1bQVCnABCCGa9+SXGLN+DWW9+CUIITDRBWfEAZKfHK84rCGKLMcUhElCaEAgClMeDgCIEPzR4Uev2o6reg3sX71KlSdR7WARCgioJYeraCjT6ONw870Pct3g3ahp8eGXnMYxZvgclK/dh/N/3ot4jqhooCri1d0eUrNyH21/8GCUr9+HW3h1BUcr31YFqFyavqYDLG0TJyn2Y/85RzCvIRFqCDdnp8SgrHoBVE3JhogkWvndUPj/QXAmNvoYUuxlLi5yK99WCkZlY+tFxxT3Tex+bKJ0kCerCW8W7Zt6Gf0y72cjtNGDAwGVDWytp0wHsAvAtRMImAPgzgL4AzADuuySrM3DR0N6KQFtajhaG0q3GtAdtaTlFuur3THVokrr4iIm8tAQbqs/5ULJyH9ISbHhtbDZeHJWFJzc3i/4XjMzEI+sPYHFRtkoTtrTICUKArK5ximnOaNQ0+BDk1SRp2rr9mDU8A3O2VWLByEzMf+co6poCeHFUFho8LJIcZrw4OgtPlh8CLwCPbjig0mXNGdFPXv/C0VlYNDYbHawmcIKAJn8IZ5sC8AU50ASa94MXBCwb58TSj45j+pbDmDU8Q74W5WCAdrt3U2keBErA1in5YDkBIU7Mhu1gFX9fB6pdeOHdo9g4aSAavEFMjajkzSvIBM83Jya4fCx+PO/Hi6Oy5EQFqVrap2MsyifnIxQOl/cFOblCl5Zgw+oJuRAg4IcGr6qVThHgpTH98ftNB+XXfmlMf1wIx7paw+UNGDDwn4+2mtlWE0KyAPwBwBAAxyHq0DYD+KsgCPWXbokGLgbaqwtri02DpPmSkJZguyiat7YQzHgrg2fuykCdOwCOFzRJnZfl5D8vGpsNtz+ETaV5cPmCWPzht3j4tuuxskSs9Jyo82D+O0dxoNqFIAe8uvMbzL3vRnSOt+H7ei9m/e+XOHnWA5cvKL/GKw9kY/47X6tel9dpl6bGWuQq35qJufihwQeLicIj6w8oyAzL8ZrHx4QnG2safHii/BBWT8jFuAjd3cLRWQjxAtISYmQCmxprgcPCgOV40BTB1opqPPXb3njh3aMqEmsz06hzi3FKWrFUHC+g/rwfFCEqi4oNkwbigRWf40C1C/6QIBM0ab0ztx5G+eR8AOIXgNMuv0LcP68gEy+8exRsiAPDWNAlvrkaxvOC/D62mWmcaQxgfLgaGP3lwcRQiLXSWFmSC4oAvABwPGfEOhkwYOBngVZJGiHEBCAXwElBEGYBmHXJV2XgkqA9FYGWWo48L4ANcYoqyIFql8qmQU8z1drUJkURXJ9sx6bSPEX+YuTQwDd1TXI00NCMVJXQ/cVRWeiWFINdM28DQxFU1Xvx9Bv/VjxutzAoLtuLF0dloWTlPvn1OZ7HjspaFDjT8WAUEZXw1sODEGNhNAX2lI6FhiPskVbT4ENtYwAelpPXJP185lbRtFbr+EiCWNPgwzkPqzj2ifJDmHvfjbCYCB4d0ktRCZxXkImX3/8GDw7qjlW7T+KxIT0VJHb1hFycaQxg0uovsKl0IGbc2VuVpGBlKNQ3saqBiylrK1A+OV8mUnqZppKmrt7DylO1kdc9Z0Q/mBla8z0ivY/r3AHZR00ikAvfOypbZAgQ4A+KVieRRsQC2qZJuxpwOSPVDBgwcHWhLZU0DsAHAH4H4NSlXY6BqxV6LUermVK1QaUqSF1TQN5kq+o9+K7eK2+U1ybF4JqEGByra2rVpT4U4nG0tklVrenTMRYMQ4mb/JrmTV6yS5CE7sdqm/DVKRfSEmyy07zU1gSapyrnjOiHFIcFiXazwiuNCeua4m0mTYL2wZO/hs1MIcQBbIjH+kl5IGFysOWL71GU3001+TivIBNcuN0nVfm0zl/T4INJJz9y/jtHFb+LSI2XfCxNwRNQB8TP3Cq2N6X/d0+2o4ONwa6Zt8HMiJYkUnUqyGknHqx7aCBizLTmmkMcL1e/at3a9hxSNUvvC0D3ZDsSbKZW0hDUPmqRrdRQSNCM1AqFfh4k7XJGqhkwYODqQ6skTRAEnhByDGKYuoFfKLRajgtGZsIb4FVtUKkK0inOiiS7GS4fizONftVG6bAwbYpwqm0KaGZPlk/OR5d4m+Ymv6OyFqW39kC9h8X+qnoM75+GMcvFac8tU/K1NWsxJjz1294oWblPseHbTBTWPpSLwQs+Vt2XtAQbzjT6AUBBoiSPtUdu74kQL2DV7pOKas+q3Scx/bd95Gqb1UThhwZtMsNQBJ3jrXLLDgBomih0WZKlSPTaXL4gUmMtmu1KiRQm2c2wmChFVFIw1NxiFaCvaXP5gtqO/hFh8WaaYPWEXJzzsKj3sNhaUY2Sm7vLrv96XwBiLDQafMEW3yOcoK2Xk1qp7Y3UAq5sJetyRqpdaRgVQwMG1Gjr4MAzAOYRQv4tCMK/L+WCDFydoCiCznEWmShwvIAVn5zA6AHpmht4jxQ7usSJIeR6XlUbS/N0W6iRCOloskKcWC3R2+RTYi3oEm9Fvy4dZIIGiBuf1vNjrSaVQH/m1sN4+s7eeGTDQfm58TEmuLxB2f0+PsaEwggPOum4VRNyca6JRcdYGo/c3lPVBo2zMVg/aSBe2/ktXD4WfxqegSWFOQqB/dIiJygKaPJzqkri1in5CHA8aEKwevdJPDioOypPuxXPWb27CllpcZrtSl4QtXvJDgtqznkVSQnrI0LlaYrSJGImisLOyjOqa1tS5JQd/XlewA8NfoVJ8KKxOVi35zs8fkdPwK6vOUy2W3D6vE+TYErvEaGVVqpeq5VvowXHla5kXc5ItSuJK32fDRi4WtFW9eyfACQBOEgI+Z4Qso8Qsjfyv0u4RgOXCS1ZbEibbXHZXtmGYUR2VwDNVhUS0hJsOHXej6O1boRCvO5GKQn8o4+NFnXrWWBIsUha9h9Li5xYv6cKg+Z+iNPn/YrXX/rRcdkaQnr+snFOWE1Ec52RBC0rLQ7zCjKxZUo+NpXmYdGHx/Bj1Pml4xiK4P3K02j0h/DaB8cwa3gGNpXmYdbwDLz2wTGwnIDaxgDKK2qwo7IWbEgknWXFA/CPaYOwZmIuZv3vl/CyvGYl8WS9F4EgDzbEY9mnVbJPmfQaSXYTHr9DrORpkWRCCBaNzUGDNyATNOnx57ZXYlnY+oLjODz1294wh6tjZpoS/84QDLuxs6a9R0PYbFZLb/bw+v0YdmNnhYdezxQHyifn45Ppg1E+OR89UxygKAKrmcKMO3tjzrZKjFm+B3O2VWLGnb1hNYfX0oq1jNWk/bjV1DwZ3NL7Xq+SFd1avlS4GNY5Pwdc6ftswMDVirZW0o4A+PJSLsTAlYXWN9ll45zonSrqvlw+FrXugGI4YObWw1g/aaCmXkoQBExeU4H1Dw2USVZ05QoAFo3NwcPrlRWm6PBrC0NpCvItDCW3SBJjTCifnA9BED3JZr/1paxNi66cHah2YdXuk7JmTdywBZz3hVTrjMahmvOyTcXH0wejwJmOTnFWzes7fd6Pu7K6QhAEzWihP/4uA4l2M5aNc2JrRTWO13nkc6cl2DD3vhsxZXAPhHSmQwlEAiuEny/5lEnHb56cD0DQNePtEmfF4xsP4ulhfTTbxY/f0QvrHxoIC0PhxFmPql0dZzWhR6pd89zeAAfeLuhWgq5LscvTuTwv6GoT2aA2wdwcbme2FoAemVsbXaWTXrulCs6VrmT9Usx0r/R9NmDgakVbLTiKL/E6DFxhaH2TlUhWlzibrkWCIAD/2K90o1/xyQk8MPAapDgs4AQBDCEq4fyCkZn4/caDSIk1Y/WEXJz3BVHrDiAhxiRXNiRtiiBArkRJLa/XPjiGv9zTT3OD7WBlFIRIqpxFvv6Dg7pjwbtfo8CZjv5pcSAgiLXS2DIlH6OWfobqiA1jx+9/hQmrvlCRMMnCouTm7rpEddq6/dhYmqej2yI4csqNOdsqsaTIiW0Ha+RzLynMgYkheHpVBTbpTHcmhSdcKQKUFd+EkpXN92FJOLZJAAWEOM3jaYrgQLULXlb78ZoGH+Zsq8TG0jzddrXNxGgee/KsB3YLo9uKjqThLemu9OxHguFW9zkfqxmA/ty9NyI11tqq9Uxrmq8rHQvVXuucnwuu9H02YOBqRYskjRBigzjV2Q3AaQA7BUE409IxBn6e0PsmW+sOgKEpXYsEm4nGvTldFWJ7MYLJjNn3ZGDc63tFp/mwcF6KDJI8yAAxQ1Qydl09IRchPoSqs168svMY6poCWFbkRJ2bVZnG/uku9dDCpNVfoHxyvmblbFNpHgIhXo41evyOXnj5/W+QnR4PhgaO1HhQUtZsvWG30CifnI94uwllxQNQ09Acy9Q1wYo1u0+iwJkOihDMfftrBVGY/85RuUJlpommJs1MEyz96DhqGnzYdrAG4wZ1xwN53UARgg8qT+Pa5FjUNPhgYSisLBmA6nPNr5+WYMX8d77Cjspa2RNtwchMUITA5Qvi1Q+OYfY9/ZDqMOOcl8ffi2/CDw1+xfpNNMGumbfBZqaxYtxNmLRGPaErtaX12tU0BVUlSzr2tbHZ6BwnVmSl6Vvp8ee2V8o2GS1VUUw6sVDSYII/qB2A/qfhzUa5LVnPtFbBuRoqWb8EM92r4T4bMHA1QpekEUKuA/A+RIImoZEQMloQhB2XemEGLi/0vsnWe1ikxKqzJ6WWFaejd3phVBZ8rLgBrvjkhExSoj3IpGOS7GYsLszB3LebiYe02U9eWyG760euzcxQmHvfjegUZwVNCH5s9GP+O0dBIKiIwSO398Rf/nlEPvfiwhykxlrw9LAbUOcOYPZbR7DvuwbFujwBDpPXVGDDpIEIhHhFJfGlMf2xt8qF22/oBJcviLqmgIJESpOVaQk2BDntxIFNpXk4UO3CaGca7srqivuX71GQOJtJJCKEQBEAL5GyeJvYKo23mdAU4GA1URizfI+8hmfvFtDgC0IQgPPeoOr4OKsJXRNiwPMCXN4gVpbkwuUVJzBfeFck0WkJNph02tUmiuCe13Zh0dhs1UCJZL9CUQTJdrOCwErnfvZursX3npmhEW9lVGkPkYMJNNFeGx1RaGpparAtmbPtrWQZU4ut45dSMTRg4ELRUiVtPgAewK8AVADoDmAxgGXhPxv4D0JSWBsVXfFYtfskZt/TT1dT9oPLp6uXkhzxyyvENl5Z8QDYzNqbYmoHK57bdkSuiEjVOimqqFtyDMqKB8iVoC7xYspBpCHtgpGZmH1PBqrqvSjbdRIbS/MQ5ETz2zlR5562bj+2TMlHUyCEe17bpXtfxIoRVML93286iDkj+sn6PKmdmuKw4LEhPdEtOQYAsPah3BYHJwBg0q3XyZXIyPVtmCTaRPiD6sEBKWFgfETCwJLCHGSnx8vkyszQYEMcBEA1GPBE+SHZhqLew2Lu219hfH43dI63we0Pyb+XFeNvgpmhVFOnYjuWQorDAn+Qx8Pr9yp+D1MG95CrIBRFYc62Sl0i1FIVpd7DYtvBGlW4e6dbr0eKiYbNTGP5eCfOnG82q+0YZ4Et/N5rTXPWlgpOeypZeh6B3ZLsBgGJwi+hYmjAwIWiJZKWD+BJQRCkHewrQsjk8P87C4Jw+kJfjBCSDmA1gE4QCeByQRBeJoQkAtgEsWpXBWC0IAgNhBAC4GWILVcvgGJBEPZf6OsaaB0URdA7NRbrHxqIWncA9R4Wnxw9gz/dlQGaAMuKnCobhee3V6LAma4gXdnp8XhsSE907GCFAEEmDcdqm+Tq27qHBuL57ZVyVWvByEyc97KqllVNg5i3mZZgAxUOKJdef91DA1W2F9O3iC1YE02wo7IWM+68AcVle7GyZIDq3CkOC5596wjePdLcvZ9fkIlXPjim1k/pZF92S7bj//1fpdxOfWNKPmqbWEXrb0lhDjrGWrR1WUTcpGlKe6pUssjQi2WKThiYGpEHGklyvGxI+/xhkqhlCLskXGlMibXC5QugY5wFG0vzwi1OAjNDIAgCpgzuoTIGnr7lMN6YNkgmIQk2k6pdm55ok8X9LVVR2BCHZZ9WYdmnVYr1jx8kfk+0MzR4Hor3xtIiJ+xhAtia5uxSV3BcPhZuf1DxM7c/CJePRaLdICQGDBhoGS2RtM4ATkT97DgAApFkXTBJAxCCSPz2E0JiAVQQQt4DUAxR7zaXEPI0gKcBzAQwDEDP8H8DASwJ/9/AJQDDUEhLiIHNzCA9wYa0BBueCxOxTh2s2DApDyGOQ4gHGJqgwJmOnZVn5GDyFIdF5ce1cHQWNn9Rg3tzuip+vrgwB3++uy/OusUQcIdVW4DuZTksG+fE89srFRttnTugSTyk6h0ghmvXNPjw43m1SaykhwOAbkkxSIgxo1cnh2oAYNk4Jxiddp83EML03/bBM3dlgCIEQV5QVbymrtuP8tI8lW5rSZETFhOFLVPyYWa0dVe8IGDW8AxYGUrT50wSz0de/w2dYrF+0kDZYiLJbkaQ43UHB35o8AIAVu0+qV735HyZrJw5H1BV0hwWWs4fjV5HMNS8tsZAEA0eVtVuTbSbkciIREWvitJaO7Ley2obHZfmoYuFadPU4KWs4ARDvGbiQeT9MWDAgAE9tOaTdlGzUwRBOC1VwgRBcAP4CkBXACMArAo/bRWA/wr/eQSA1YKIPQDiCSGdL+aafkloyQ9KgrRhURQlZzvO2VaJEYt24YEVe9AU4LHg3a9x2wsfY862ShTmXYu9J+qxakIu/jomS6VPe6L8EB6/oyfKdp1U+YSddQfwX4t344EVn+MPmw5hwcgo77IiJ7LS45AQY8KOylpkp8dj7cRcvP+HW9GxgxVDM1IVa5dIncsXRHZ6PGxmGmXFA2C3MFgzIVf1fAn//bsbUNcUwF/eqgQArJmQi51/+DXKS/OQGGMCLwh4cVSWYm0vjekPi4lCycp9qD7nxZxtR3StMoK8AEEQsHpCLj548tcoKx6ANbur0OQPYeTSz+APclgS9iSTzr+kyAk2JGrieAG6PmfR13+8zoNb53+E+xbvxtEzbvF3SqC6t4sLc/CXfx7BzfM+xJjle/DgoO7ITo9XrFsyhPWxvCogfeq6/TAzNGKtJvm8keuIXJuP5TTbrT62dXsFLQ+8yHZksIV7Dlwcn7G2/LvRg17iQfACzmHAgIFfLlqz4HiXEBLS+PnO6J8LgqC9A+qAENINQDaAzwF0lNqngiCcJoRI5+oKoDrisJrwzxRVPEJIKYBSALjmmmsuZBlXBJdKSNzSeS/U0ZsNcShwpqsid6auq8Cs4RnYUVmLmgbRmHT1hFy4/SH4g9pVC44XNPMVkxxmuR16oNqFf+z/QQ5RN9EUkmNMOOsNIsgJ2Dw5H4QAv990UFGNA6AYBrAwBK9/WoXZ92SgwcuGr4XHN2eaFC1Ph4VBU0B8C0dqyh5Y8bl8LoYxgYHoQ2Y1UZgzop/crjPRBPE2EzaV5oEiwKO399StuNEUkVuRkT5mk269ThTg0xT+9U2tSnd1f+614cEDbRuKlIg2qlShkfI8pbaedD/nv9McQp5oN2PBu1/r6v+k9UmmwnrkM8QLsFsouZIqrePFUVkK4T6np8lrA09prR2p58EXbXT8U6cG2+uE397EAwMGDPyy0RJJ+8ulelFCiAPAVgC/FwShMboiEPlUjZ+pPt0EQVgOYDkA3HTTTVf1p1/0h/7QjFRR90WRdhG21jaTtmQARpI8QkRRtdYGEx/WEkl/P+dhkZ5ggzug78ella+4siQXUwb3wOQ1FchOj8e9OV3l+CapmvTqzm+wo1IkMFLLSDqHJK6ffU9fnKjzyN5pj9/RE6dcfnjZoOIYCS/f3x8DuiVi9LLPUNPgw4FqF1549yjmjOiH9ETRjZ4iQJM/BLONgcATPLz+gOq6NkwaiAdWfI4lRU78+c0jWDHeqWm6a6ZFzZlEDKSfb/nie3mKc/a2rwF8rVhnYV43vPXIzfDp+Jg1eNhwsoAZHTtY8diGA4o2bk2DONTRNcGmmD7dVJqnqf+LXN+CkZmyqbAe+WQoAgtDqwis1UQpUiMk1//o462mtgWetNSOtJkoTY86aTK2vZqz9mZn6l+74f9lwICB1qFL0gRBuCQkjRBigkjQ1gmC8Eb4x2ekYYRwO1PaQWoApEccngbg1KVY1+VC5Id+dno8HhzUHWPDAvgL/Zaud15AvZm0ps3RIo/P3JWhucG4fKIQWhoSSO1gwdkmFi/v/EZlGrtwdBYoSlt47/YHZWLw2JCeqrbQ1LXNVbsYM615jjONflgYCjQlDgs8e7cAThCQ7DCj6PXP8YPLrzjmo6d+jSc2HcL2w6cUOrG6pgAS7CacbWKR7LDgx/M+WBgaJoYCTbSF/WcaRV3cKZd4PMcLsDBEtqPgBYDjOfiCoiasc5wVH08fHK6W8Rg3qDtOu/z4/pxPeyNnKJw+78fL76vvq6RJm7NNjG+qafDKgeuR5whyPARBwMLRWXLLUc+8Ni5cGZR83l4bmw3YARNNNMmniSYI8YImgX1j2iD578l2i8qHbcW4Ztf/9iAhxoKOsUEFSewYa0FCzMXRmLXXCb+1xAMDBgwYaAltjYW6KAhPa74O4CtBEP4a8dBbAB4EMDf8/zcjfv4IIWQjxIGB8z9lqvRqQuSH/pTBPVQVpgv5lq53XgmRm0mkADs7PV62SCCEyBW0SJJX52bR5A+ppjoXF+bgtQ+OITs9XhazSxOFNQ0+1LmbqzuJdrM42SZoV2Jc3iD6dIrFJzNu0w3Klqp2euL3eg+LOdsqsWFSHsqKB8BEE/j8PHZ9W6cgaIOuS8KDN3cDLwAvjM7CU+WH8M+DNVg7cSDONgUQ5HgEQzyeimjbLRiZiT9sOoSZw/povrY/KN7bpR8dx4ujsuAP8Xj90ypMuvU6gIjTj69/WoXCvGtlDVik+Wx6YgySHCJJjZ5+THaY4QvxsiVK5H2Ns5kwY8thvHx/f2wqzYOJprDv5FmsnpCLcx7R52xrRTUm3nIdLCbR/mLabdfLRIYiROU9tmBkJmZsOSxX4iRdGc8LCHIC9lfVY/2kPDl264PK0/hN384AdDRhUcJ4h5VWkFcLcxHp8rmTAAAgAElEQVTtJ6Ir8RF/b2+7sr1O+Ib/lwEDBtqDy0rSANwMYByAfxNCpNTqP0IkZ+WEkIkAvgcwKvzY/0G03/gWogVHyeVd7sVH5Id+vM3Urm/peueVoOVFtfC9oyp92IrxNyExpnkt2enxeOq3vTF5bQVSHBbMGdEP16XY8fWPbqz97DsUONPRK9WBcWGPrsjriMyP/ODJX2P2W5VYXJitICEUEdtXAkRBPUMBvB6RCw8BOKyMohok2YBQBJg1PAMurzg9uHhsNu5ZtFtxb35zQwoeu6O3yhB1ze4qeNgQ6j0sMjp3wAMr9qgE3rOGZ0AQBM2WGkNR8jVvrajBzDt7Y8rgHgqiVHJzd3QKVxrr3Kx87hWfnsCMO29AfVMAFCGwmWnFBODL9/cHHVGBjLyvm0rzUNcUAMsJuGPBRxiakYpHh/RSeKYtLsyBIAhY9OG3KHCmy6avXpZDjJnGli++R1nxAJzzsOgUZ0WDl5UrcZJmkCZihfZ4bSOc3ZMxdoWyFZ1kM8Md1K7KRZIYl4+V72fk/bNbmDbZULSktaz3sPJ1R76+9EWnve3K1rJB24Lodm107Fk0aTPMbw0YMCDhspI0QRD+BW2dGQAM0Xi+AODhS7qoywieFyBAwNqJA3HyrEf2wdLb4C7kw7o1gbT0jX72Pf1kLRagHaUUWeGrafChZOU+lBUPkKtl5RU12FSaJ59DctaPvg4ASIk1o6reC6tJJCGSTUfR680t3nkFmfjk6Bm1TUVhDl794BimDO6BR9YfQIrDglUTctHkDyHWyijSCRaNzUGPZLuCoKUn2FDd4MPIm66RCZp0zVPXVmDNxFzQlDgAoCfwlip5WrFPL4wWJz5THBZM/FU3/NgYUHnJMTSBL8iDoYGFY/qjwcvCEwjBYWVQXCaSi42leSobicc3HtTN7PSyHOYVZMLHioMP4/O7qa5v2rr9WPfQQM2BDYYmuCurK1Z8cgLlFTVYNs6J/VX1qsGFXh2vBxviEG+3at6/LVPykRpr1Xzf0RTwQ4NXNtTVmnDcVJoH2JX/PqLf7wDaFYAeaGe7sjEQBMfzinYqx/NoDARl+5ALQWuVvfZW/n4JMEisgV8SLncl7RcLrQ/fZeOcWFkyAMVl+xQfyEl2c5s+rKM/rHqmOFpsq1AU0W0r8oKAspIBqDnnw7VJMZg1PANLPzout79e2XkMZcU3oSac/5jkaJ4slKYjV4WzLJPsZiTZzdi49zs8PewGnPOweHSDqFuaNTxDtWHP3HoYqybkwsIQbJiUB04QQBMCQgSMz++GtMQYpDgsmDK4B7yBEOJsJpnkSee4d/EuRLoadI23Ym44VkqvYlnfxCJBihfSEcdL+jut2Kc6dwCzhmcgMy0OPpbDxFV7FWt6eL040bm1ohqP3t4TE1c1E9OFo7OQ4hA9xpId2gMagRCv0qItKcxBUyCEsl3ivc5Oj0fneJvm8QBgYURhPReemPWyHBLtZjz75pcocKajvKIGpxs8GN4/TZG/uqTIiXgrA5cfiLeZNM102RCvaueZGApNfjHFQTrX+ocGaq6Pi/iF6b3fkxzmFithRCcWShpGIoDuF4i2wMdympq7aILZVrRW2Wtv5e8/HQaJNfBLg0HSLhO0Pnwnr6nAG9MGaRKrOnegxQ/rn/phpdcWpQiBK8pwVMrOPFDtQkqsGYGQID8+NCNV1jUBQKyVwWNDeikqYS+OyoKJFls9KQ5L2KsrBitLBuDH8368uOMbHKh2oaZBDCF3eUOK4xcX5iDWyoAiUJi5vvnwzapNP9p26geXX7aV0Kv01XvE9uOY5XsU1xPZkpv/zlGkxJplAisHnCfaYGEo+FgOHC/A7dd29Y+3mVDgTFf5jD1Rfghz77sRRa/v1c2fJABeePcoXhrTH0kOC6rOevDnN4+grimAJYU5+OjrWswfmQlBECO3Xtl5TKEpO1HnwSs7j2HGnb0V8VlLCnNQ52blNIc7+naWc0Ol9U1dW4Hyyfno1MGKOp7XNNO1MM0TlBKBqHMHFO3HFIcFArSJEhPxPj3r0X6/r5+kTfCkShhNoElkaSJu6ETj8XkFmWjrfq5rH8ILcqXwQio5rVX+2juo8J8Og8Qa+KWhbTPwBtoNvQ/fYIhHSqwFXRNi5Jialp4vfVjrfVhJxEMPeuagFFHnO87cehhTBvdAWoINTw+7QdGS21FZi1d3foPNU/Ix57/6ob5J7fz+5OZDCIRE9jT7ngwAwAMr9uCOv36Cp9/4N54e1gfZ6fFIS7AhEFK79U9btx+JdgsoQmSCkJ0ejxiz2qBUCxJJ2lpRjaVRZrHzCjKxtaJavl/S9ayaIJrlbizNQ/dkO166vz/+NDwDbl8Is978EmOW78GsN7+EP8iDDYeuU0QkqdFrkipxepW8TnFWAMCPjX6V2ezC0Vmwmmn8dXQWOsdZIaaoAU8P64M5I/rBzBAM7pOKkpX7cMdfP8asN7/EjDt7y/dzSWEOXtkptopVU7Pr9uOxIT1l7Ran54PGiZWykI4hayisrQqFeNnslQ1xSHE0b5ZTBvfA89srVWbAS4qcsEakQ+h57FFhAht9X5sD0Cms2n0Sc++7ETuf/DXmjOiHP795BAVLP8PRM24QEKzarTRSXrX7JARd1YUSkoVG9Osfr/Pg5nkf4t7Fu3D0jLvNBretmeteDPPd/2QYJNbALw0GSbtMuNAP39ae/1M/rCLbU7tm3oZ/TLsZvTvGgg1pG6b2THVg1vAMnPcFVY/vqKwFF45C0rPIkHILGYpCSqwVqyfk4r0nbsWg65Lw5OZDeGxIT7HyoWPTIQgC+DCJkAYaZr+l9D6zm2l8PH2w5v1K7WBBgTMdX5w8i3UPDcSWKfnyRj3xluuw9KPjius56w5g+ubDOHKqEd/Ve0FTBAQEj208oCQ6aytQfc4XXiMw9+2vMK8gU0UEl350XK7kRa9NMlwt23USyQ4z5ozoh02leXhhVBbi7WaMWvoZbnvxY4xZvgduP4e3/30aY5bvQcnKfTjlCqiqc9O3HMZfR2dhzoh+aAqEcKDapUsQe6TYYWIosCFO9kGLXh9Dix8PeiSO5Xg884/D+PqMG/cu3iWnF0hkERBbpXVuVpwyDV/fnBH9YDNR6GBpFt/TOmTMRJEWEweS7GY88ZveCHICHvz7XpSs3CdXZyet/gJmE8FjQ3phzrZKjFm+B3O2VeKxIb2Q6mhb1UWy0Ih8/QUjM/HKzmPyfWjLlyMJrSUotPb4Lx0GiTXwS4PR7rxMuFDnc73nS4JsPS1OW4YOtKbNpOOjz3estgmT11Tg7cd/pfl4iBM3cL2WYq07gM5xFjQCslBeamUCQPcUO57YeBALwpWW6ONpioAQ8c9TBvfAhJX7EIiwd/jbeCeuTbJj0QffanqJ/WHTIbkFePq8H+MGdUdyrAXP3t0Xq3efVJi/piWIvmJP/ba34jxrJuZqkhQpJzTEC9hRWSvbZKTGWhBnM2Hu21+FJz+rVT5jC0ZmIsZE45MZg0FAEOJ59OroAEUBPC+uNVIXOG2dmOxQXlEDAIiP0dHZeVikxFrwz4Pi8/R+LwKA1z85jmWfVmm2epcWOWUiE2nIGmnfwgtAyc3dVRXQ6VsO44VRWbh/+R54WQ6PDemJRzR0XZEtKpuZ1pygNTFUixYW0pcOu0X7S4Kf5dGnYyzKJ+cjxPFgaAqpDgsYpu1GupGvDwCPrFebBre1ktOaJYdh2dEy2psgYcDAzw0GSbtMuNAP39YE2bOH91F5mF3I0EEk6j0sntteidfGZqPBE5R1V+mJNsx/R3TBT3aYVURjZckACBAnVCOjlaTHF43NQaLdBI6H5vThypJcCLyAA9Uu+NiQ6vUT7CYEQjzsZhr/795+GPf3faq1P7S6AptK83CstgkUAdZOHCjbZP1+40F5M81Oj8etvTvK2iup5Xay3itPiC4pcsLPcnii/KBirVVnvbpTlgDkKd1Im4yhGamYNbwvJt5yHQQAcTZGMSHYwcqApgiq63zyPRuakYpHb++pCDKP1AUSArlCFRfWk0WvKc5mwoJ3v8Yjt/fEyXovln50XEV+5hVk4vntlZg1vC9uv6ETXL4gth0UJ3Y5XlARmcQYM1aWDMDZJpEAfl/vxfPbv5K1cdIAhISaBh86drDi8/++HSaGQqNPW68XSWzibWJqgsKUtoMV8TZzqwHoFEVgMzG6X1oYhkKX+LYPC2idP1Jzp2UafCGVnLZcj6Gv0oZBYg380mCQtMuIC/3w1RNkj3amIadbEl7e+Y1scJoSa0GXDtY2DR1Egw1xqHOzcFgYOCwm0ARgaAomGpg1vC8AIBDisf3QD7JNA00RNPpDeDUiZUCKVromKQZuXxAxFhoPrPgcqydoV6JMNMEpl/jzpkAIDEUUgwsLR2ehvonFS3uq8NYhbQ/jtAQbeEHAU79VCtsXjc1BSmzzt2st4+CpayuwakIuJt5yHVy+IDrHWeAJNLeRpapRWqJNbQ9S5ESyQyRKTf6QiqA+OKg7XF4WY5bvASCSNskXDQDMDIVAiFesSWvAQBp+mLOtEmcaA5gyuAfMNCW3V6NJsWRIW3najZUluTAzFMw0wdz7boSJpuDyBWXS98xdGRizfI9M3BiKoGtCjOoeu1kW/qDS6Fcij1PX7cecEf1QsrKZQKcl2HCm0Y8eKQ4k2i0I8W2zmrEwFHp3igVNRK3ZhWy+l6vCYlRyrjwMEmvglwSDpP1MEKlBm3TrdbJdgpTBmJZgk6fxfEH9yoWWiaaZoTFv5I3wspwq+mf7oR/w2JBeMNMEt/buKL9uWfEAbNgrGtvGmGmUFQ8Ay/Ew0xSeKj+EKYN7YE7YdoPT2aQZisiB4JyGOP3xTQdxKiI14PoUO14vHgCGIqg+58FTW/6NJYU5cFgZjHtdbX+xekIuKk+7UdPg080hFQRBJipzRvQDG042SHFYFG3Pyb/qhg2T8hDieITCPmLjB3XH6gm5MNEUXv3gmMKiQrIjkVDnZuXQcRNN4GU5nI+qMOnpx5LsZiwYmQlBENCnUyy4qPaq9JoUgVw5rGnwibYi8TacPu+Tpzsj7z8VLjlKZLB8cr7me88X4FUtzchA9m7Jdvn3OzQjFU8PuwGEAMEQB54XdKOR9Kq+y8Y50Tv1wiwVLleFxajkGDBg4HLCIGk/E5gYSt4IaUo7S5JA3PB+PO/XJEUcL+DexbsUG2XvjrFIspvBhjhMWKm0YZi2bj/KigegZOU+bCrNU1R9kh1mTaPUDlZGJVhf8ckJzexHq4nCY0N6yr5r0dcUSdD+9qAT3ZIc4MIxRZ0TYvB/jw/Cd2f9qG9iNe/HeV9QEUCuqXkLa/uWFjkx63+/BAAsGpuDcx5Wvrbs9HgMy+yCb2ub5FbcsMwuclt1zrYjmHjLdXgyKlJKIqBShNa4iEQAybssck16+jEpBuqZu24AAPm4yPZqWoINs4ZnKI6TLDL0sjfPeZrbdikOCwRB21YiqDM4IFl4WGiCOSP6Idkh6tQikw+k91jPFIdKF6ZX9Z28pgLrHxqItISYCyZql6PCYlRyDBgwcLlgTHf+TMBQRLZpkDbpSIhtP2DS6i/wys5jqknDZeOceG57paoFetYTkG0WtDZiiRCGeAEpDguWjXNiU2ke4mLMqvbhzK2HYTWJLazIicbyihqs/ew7rCzJxUfTB6OseADWfvYd2BCP61Md6BhnhdVEYWhGquq6rSYKXzwzBKmxVhSX7cXtL36M4rK9OO8Ngg0CU9ftR72HVd2PoRmpcFgYuRX82bd1WDQ2R2kDUeiEzUxj7n03ItbKoK4pgAPVLlAESE9sJksz7uwNH8spLDh8LAeaEAgQq2SJdhNWluTiwyd/jQ2T8tA1wSZrl7TC46dvOQyaIorfkzRgED0hOmPLYdQ1BZASa8Hz2ysxY8th1e93SZETWyuqFX9PsZtR72Hx+qcnEB9jwtqJA/HpjNuwftJAWBiC2W9VAmgmkWOW79G0lZCMfqPfb16Ww7JxTrgDIXHi9LwfD6/fr/ke+77Bi6M/unH6vB9Hf3Tj+wYveF7QnVJu8AbbPDF5qSHFOP3Q4EWdO9Bmuw0DBgwYaC+MStrPBD6Ww/x3joarJQKWFDoxdZ1SIyVA3OBqGnx44d2jcissLcEGioLcGpVQ0+CDPyhOSko2DFrVt7QEG6wMhT/+ro/spbZlSr7m5iqJ6bdWVCsmBnefqMeI7K6yHmpoRirOeYNygHhagg3DMzsDaF7jM7+7Af/379MIhHi5CiS9zrR1+7GpNA9rJuQCBFj30EA8v70SOypr5SxLqTU7NCMVj9zeE6+FW5JS+LvDQsNiovD0G2LbdElhDs42idq8qvrmYYFOHaxyFUx6/elbDmNjaR78LIe/jOiLs02sQhO3cHQWNkwaiFMuPzrFWTXvFUUI5r/ztbym+BgzzAzBG1MHwctyOHnWgxfePYq6pgCWFjkR5Hh5sEDS/3VLtsNCE9itFJ69uy+euSsDDEWQ6rDAZKLBe1nc2rsjxq5oTjtYXJiDOBvTIomM1DCaw18QIq9vSWEOUsPVpIM155GWYJOrp5KWT2rDAsCZRr9Cb7hgZCbiY0y65sqxVgY8rwxpvxIwHO4NGDBwJWGQtJ8JzAytiCYa7UzDypJcMDRBiBOQ7DAhyDXbaEitMMnqgA1xGJqRigJnurx5bq2oBkXEoQSriVLZMCwpcmLXsVosGJkJXlCa3UrVq+jNNcZMY1NpnjwxuGGSODFICPD89kocqHYhLcGGZ+/uizFRLvdLPz4hn2vH73+FF3aIhE6q4kXHEoV4QdFCXDQ2B4/e3hNWEy0TNEAU5EskL1LD98KoLFyTGIMV429Ckz+E+Bjxn0NTIISuCVaZmPCCtocbzwvgBQGJdrOKRD5RfggbJuXhyc2HUD5ZP4PzQLULc7ZVYmmREx2sDBJjzGAYCjwvwG5h8NrYbADAX/55BDsqa5GdHi+Tus5xVnSOs8lkIU5jgJEToKp4Tlu3H+WT88VpTgEAdHzQwtOXJhNBl3grVpbkgiJiuoOJBswmghBHsLWiGvMKMuFlxfdYdBt83UMDdbM7O8fZVFPK8woyMfftrzD7nn6t/ru41DAc7g0YMHAlYZC0qwh63mbRweyv7DwmV6Y+OXoGhXnd4AlwsJlplUB7WZETNAU4LBQeHdJLRcJsZgqHq8+jd6dYvBqeFpWI0Ks7v8Gzd/fFGxU16BKVD6llufHiqCx5ulDCqAHXwGaicbYpgAdyr8XEW64TQ6o14nYA4H9G9MUt1ycDAB69vScqT7thZSjNWCIzQxSbp5SVGS3A18uetDAUQhyP3h1jcbYpgON1TYrYq1nDM7CxNA80IZj8q24YedM1igByE01hajjIXJPECQLmFWTibBOLhaOzZJKblmDDy/f3R7LDEg4pt6BLnE3h3RWpe/qu3iOTy0gd2ifTB7dazdHLahUEQZ7krHMHWpy+jDWbcapBGR6/rMiJrnGiZuyJ3/TGwveOouTm7njmrgwU/k2Zq1rnDmjef04QrzPZYVY8JlVbn737yrcVDYd7AwYMXEkYJO0qQSjE42itW9H+W12SC5uZhj/EoeqsFzYTQa+ODrx0f38wFEG8jUK8zYSxf2tuZa2ekKtol/3pf79EXVMAG0vzVF5lU9dWYM3EXMx680tsmDQQdW6lBqjOzYIXBPy6T6oczyMdf6DahVW7T2JjaR5+CLc5rSYKPVMdcqtLgBjwLQCqkOp/zRisuge/uSEFt/VOxa/mfyiL+RcX5oAToFuJiURNg+ikHz04wQuCJslLdpjlIO4gxyteY0dlLSpPu7HuoYHoEENh5IBrFNmdIwdcA7tFJIkmnVYxTRG88O5RPD2sD+a+/bWCiDy37Ss8PayPPFn6xtRBCPECghwPU5RPmV4rmm5Du02vnRjp69WarUSDLygTNOk+T15bIVeTeneMxfP3ZsLHhjSHDIKcdvan3SJlf1KYs62yxTVeKbTl/hkwYMDApYJB0q4C8LyAU+d9MkEDxGm7M26/vLG9en8mkmM7yC1CqRJWVdeo2DzH/30vyifno+j1zxUbS5DTjn2iiEg0zLROtYqmMG3dfmyekq/SJZXc3B00RRReYNEh6wtGZqJr2NJCIimfnajHLfM/UqwlLcGGx4b0goVptoWYslY0qg3prJ2LEnCnJdhQfc6HRLsJSwpzZM8xQojs8SUdK5G8OncAgiCAEKJpyhpjpuFjBZx1B1Saqg4WBmXFAxASBIUmTtJsvfK+GHge5HhFq1paa5Dj5dfxsBzGvd5MtpcWOdGnYywYhoKJprBmYi44HnK7kaYAE9363E9bfL1as5VoqZoUaeliNdM4/kOjitTo3f83pg1q8xqvFK7mtRkwYOA/HwZJuwpQ72FR6w4oNrboYOzsa5Nkt3yguRK2UaOapEXIaIrSrAgw4Y1YL0R7U2mefE6bmVY4wtvMtEw0AFH7pRURtGHSQMXQgRZqGsTg7/KI66lpEKdKW6okST+PNFitawpgcWEOyooH4JyH1RXuh3gBj4S93IZmpOKvY7LAC+LI84+NfpTtOgk2xEOAdiVvY2megrgtK3Ji9j19wQvA+0dOyxFONEWwaGw2zkWkKSTaTfKQhUjYOKyflAdeEEARgg8qTyPRbkaXeBvsZoJaN6cgv2Jsk0lxTXrt8rb4erVkK6FXTYq2dFk9IRfXJsWoyHxKrNpepabBh2A43utq9h67mtdmwICB/3wYJO0qABviVEL8aB2VXsi1VjUpyCn9t7LT42GiiaZrvtVEYdk4p64Fh+QW/329F6s/q0LJzd2RGmsBJwg428TKDvoAdA1jAYIXdxxVPPZg/rVY9dl3mq8XeS1Sdme0/m1eQSYIATZMysMpl0+hZQIAh4XBk+VibqcUvh5NMk6f96OmQZxGfHBQd9kQV5rOfOq3veX1aGrOIu6Z1AKU0gGWFDkxNCMVOypr0cFmUtk2UITAYRGjjNZPGojzvpDsUyf9bqSq4nkfpyK/U9ZWoLw0D3areL7WphDbI3LXqiYtG+fEuj1Vihbu3Le/wtwCcWpTGkqwhpMVWmsZXs3eY1fz2gwYMPCfDYOkXQUwM7Q8IScREYoQRfvx/T/8WrcSJk1tSp5gbx34AfMKMrFq90mMz++GzvE2fH3ajS9rXPJEKMcLKN/7HcYP6g4zLbbUtM5voiksK3Li5Z3fYMadfeBlOcVE5cLRWRjtTMOwGzsjJdaCsuIBeGXnMZkspSXYUPT65/iu3iuft0u8FQ/96jrs/LpW9XpShUJqKZ73BRFvM2HV7pMqR/9n7+6LytONmnomihDZYoIQqAjqsiIn/hQ2r9WKjHqi/BBeGJWF9ARGpceLXquEmgafPLQwNdyqLb21BxLtZlSd9ajapd2T7SifnA9BEDT1gpLmTs9MNhhB/C71FKKFoRRVVAtDYVhmFzk4XSLOwRCPjnE2wN58bCjEY9k4p0JvabQMDRgwYKB1GCTtKkCS3YzHh/RSZHF2jrMqLCqWf3xc5Rq/pMgJM0PwyO09FT9fODoLe47X4+HbesrmopJXWHHZXsWmSggw680vsXnKQE0Ljoqqs+iWHIuHb7seJprCtHX7FERgxacn8PiQXorJP8lt//R5n2Z785TLjx/P+1VtsQUjM2GmCDaV5kEAkJZgRZ2bhYlWX+PiwhyY6Oa0AEWFsDAHDd4AFozMBEUIaELwz4M1cu4oxwvwBTmZxOnFMaXGWkAIQNNEZROxbJwTa3afVByTlmCTfcGkquDIpZ/ho6cGa7ZL1z00EEwLRsJSFY9uw+BAa1OIeq3QtqDew8opApGvP2dEP8U16UVLMQyFGzp1MFqGBgwYMHCBMEjaZYbeZtk53ooHcq9FjJlGvYdVRT9J+qZNpXkI8QJoisBuoXDw+0a5QgM0V4FWluTKhAxQeoVJz5u59TA2TBI1Z56AgG1RRGbLF99j5E3X4GxTANckxuCsRvxSgTNdNfk3fcthdI234cfGgOK5kSangRCHa5JiFNWZlFgLLCaCa5NiUOsO4H/+WYkCZzpiraL2avWEXACQ1zZ+UHfwvAAPy2HzlDz4gwKqznrw5zePICXWjD/+LgMCRKIzLLOL7J2WlmDDa2OzZeKlF8dEUwSCAPA8j3g7gw0RmjGaEnBP/zRs//KMShMnHS9Nfeq1qgHAZqbhY7kW9YJmmtImtBGDA0Sn2kcI0ZwcvhBDVj0CGGOmVT8TBG3bjPa0DNtDMC83fk5rNWDAwNUPg6RdRrSkG+pgMSEl1iJXhMqKB6g23d0n6jGJFX3G4mNM+L7ejxgzrbmBmmglydOrFgU5Htnp8bAwygB1iXQ4rDRYzgQBAmKtjGpNejo06Wd51yXihVFZ+J9/HlGZnC4rcqJP51j4WA4cL+Bf39RicJ9OIARi2Ptt1yvaaVKFrq4pgHkFmfhL+Jyrdp+UEwUkw9cHB3WXJ1yl6qJUWXP5glj84bf4y4h+KCseAKuJUlXj5hVk4vntlfjv390AtyeEQJBT+JwtHJ2FGDODWcMz0DPVAQCY+/ZXslnvgpGZ8AY5rJ04EAytTaB4QcCZxgB6JMVoVjGTY8yocwcQ4nkkOcwKQtvByoCPIES0jm6PJlBNDl9oK9TEUJpGyNLgQ+Q1XWxriqvB8T8U4lHbFNC0R7na1vqfBoP0GvilwyBpPxE/5cOjJd0QALwSYSbLCwJevr8/Ht94UP7Al8xiZ9zZG+c8LNITbbBb1MQpLcEmC/6ln7t8Qc2N9rt6L2bf0xcUIZq6rz/f3Re1jQFYGBrz3/lKRQSSHRZxgs9hgd3C4F/fnpXXsak0D40+Fv4gh1nD++KBFcrp1MlrK7B24kCcafTDaqLg7JakIFYLRmbKthgpDgv8QR4v3d8fbIjHik9OyF5ms4ZnYNo60ch2R2WtrsZszoh+KFm5T76XPC9gxpbDAIBXx/bXNFR95pCXa/IAACAASURBVK4MJNrN+J9/HlE8vuLTExif3w2T14jasblvf40pg3tg4i3XweULYv47R7FwTH8Uvf45UhwWlZnti6Oy0OQPYeq6/fjHtJvRK9muqJKeqG3EMUAmjkMzUvH0sBvAhjgk2s1oCoQQ4gV5ICEQ4nV1e9GTw9I9aashq5kmchxVJImMszW/9y6VzuxKO/4HgxyO1japJmsle5Sraa3/aTBIrwEDBkn7SfipHx6t6YaizWTNdJRYO2wW67AwmL6lWWsWrVV7aUx/BDlO0SLbX1Wv8jBbUuTEmt1V2H2iHuWT8/D4Hb1QG25RmmkKj9/RCxQBYsw0QhyHqYN7oFOcDRtL88CHyUSdO4CVxTfhjoWfyut2WGg0BTg8ufmQaBURa0K9J6R57Q1e8Zpd3pCqbTt9y2HMGp6BpR8dx1O/7a2qEh2rbcKBapdcJZQIgl7VMD3RJkdWvf6vE/jj726QK19BTtAdQKAIVFXAeeEpRkAkwFo+aBQFmTQRQrB+0kDwAhAM8djyxffI6ZYkas84Dt/W+5VGxhNyFTowKXFA1BnuU7zvOnaw4Lntlao1LhqbA5cvqBvhpVX10qoaeQMcXg3nnkamUcy+p99F05npfem5ko7/PC/gVKNfe7J2cj66xCtzuAJGOsFFhUF6DRgwSNpPwk/98LCZaZQVD0BM2F+MpghMNAVCRH3Zi6OzcM7Dot7DIs5mkisXkpaLgODxO3riL/88otq8N0zKgyAICPICLDTB/SvECs6CkZno1MEKi4nC6GVqn7WXxvRHeUUNGELgDYQUE4gv398fNDGjc7wVbIhHDAE8AQ4UEScmIQAb91Vj475q+RpTYy1YOKa/XImSDGmrzno1iUKczQSXN4hrk2I0N7h4m0mzMjZz62HZ7kLSlCXazbJ4P/q1hmakAmjWeD16e0/Zf23h6CywIU7dcizMAUMDPK/Ov5y5VfRJS0uwYelHx1U+aJ3jLWj0hWTiF92uXVrkBCHAlin5CPHAy+9/ozj/OQ+r+N3H20xItJsVmaTS+25TaR52VNaizs0qiBRFxOdETw5L1aAEm9JnLRTi8fUZt6pqlGQ3YeIt1+HJzcpKIIFwUTbLlr70XEnH/3oPizqdKmSIU4e/E0BzrT9nXMl2oxHJZcCAQdJ+En7KhwfPi/qjaBuG57d/hZRYsypXc9WEXHmT1qoixdvMuDenKzp1sIITBFAE4AB8X+/FtUkxmDU8A9+cboTdwmDc3/dirU6+ZGqsBRtL8xDkBYXZbE2DD49vPIgXRmXhvC+ItHgbTAxB9TkvYsw0znuDKF1bobrOWndAJlBSZSjEC3j736exekKuTEK3VlTjsSG9wPE8Zr35JWYNz9Dc4Lwsp5u9mWQ3Y0lhDvxBHmXFN2HpR8cxa3gGusRZsWhsTouTrQtGZqJjrAVrJw7EE5sOYsrgHthaUa2sFn1wDH++u2+L+ZdlxQNgM1Fo8AaxYe93shWKzcRgfUWVZmVw8poKTFlboWi/zivIRJ2bla1L6j2sKqx8y5R8zXVwgnivInM90xJschVy9j19sejDY/LkcKLdjE17v0PHW69XkKzapoCuH9uTUYkBT24+pDAebg9a+tJzJR3/tfwLgfBQh0bag56f38+1M3el241GJJcBAwZJu2DwvAAuSu8FtP7hUe9h8b/7q1XTk1MG9wAAlU/W9/Vi5enJob1UVZxVu0/iyaGiLi3Ss2xJYQ427P0OOyprMTQjVRF2zbTg9fXU5kO6IeHJDjMsDIVEh9LrqyVIFTDpNWwMhXtzusrtO8lCo2MHC+5dvBs1DT7dwHaKEKQn2jQjqzp2sOCxDQdR1xTAsiInAMgkZWhGKlaW5KK+SQwPHxOV1iAlBgQ5Xm6ZamWXci0kHlCEoGTlPqyfNBCvfnBMs924t8qFA9UuuSLWM9WBZeOc2Fl5Bt3DOjRJPzZlcA95/VsrqlVh5bqEgUDlQyZNmh6odoGhiaxFrPeweH67OOAwflB3xfXqxW/p+bRFGg//VPC8AF9QuxUufenR8mi70Nf4KdUgLf9CqbqY6tCqIGrrOmff0++C1nu14Eq3G41ILgMGDJJ2waj3sHhue6Xqg3vZOGeLHx4EAkbelI6ahoiQ7pvSAQD+IK+qEr2y8xjKim8CQ1OqDazAmY5TLr9KwzU1Qjxf4ExXtGpommjaOEhWH7peXGFnfI4X8NTmQ/jB5Ves5ZPpg+WA98jjpJbjgpGZmpFT09btlyOnADGw/YV3j2LW8Az06RSL6nNevP6vE5h4y3XgdQLWV0/IlStPosdcXxQ40+TYpembDwEAXrq/v3YFihfAhttWeiHsFoYCLwi6iQc1DT7UNgZQ4ExXkemH1++Xq1lPD+sjV6OGZqTi0dt7KoYkIjVuaQk2PHJ7T3gDyoqtFpFdMDITD68/gJRYM9Y/NFD+AvDc9kpZbxdrYTT1dqYostOSH5sewW8PpErNj+f9ul969Dza2koU2lMNSrKb8cRvemPhe0flKmRKrAVdOlg1pztTHRaV7lOf0F39uNLtRiOSy4ABg6RdMNgQp6n/SW7lw4MXoLIs8LIcusTbcMrlV2iXpLQAmqLA8WqdS5LdrPJRA5QVLKlqIh3b6GNxbVJMOKhbgJflkBBjhj/IYdk4J/xBTpPE/djoR0KMCR98dVZF0ACAotQtnkVjc0ARUTQ//52jLZKkyGs7UO3CnG2V2FSaBxNNYXx+N1hNFE651Ka4kX+XLDek6VFpeAIQ0wT0SIaJIrCZxJYKxwt4+o1/q4igZDirNzn5/h9uBSFEtSbp70l2M54c2kvRLixwpst6Q+l5M7eKOamfzLgNp1w+8IKAHxv9qvuzavdJOZO0a4JNDnEHgMrTbrlF+Py9mXj2bnFjM9FQTZcuHJ0l+7BJYCiiSUbNlDbBt5kvrKIVDalSk+KwqF5XqphoGSJfCFFoTzVIIgnP35vZJpLAMBT6dIxF+eR8hDgeTAt2HRJ4XoDLx4o2NIIAq4lGst1yVRCRq6HdaERyGfilwyBpFwjpgyta/yPZaOiBFwSwIV6hSVs4WrSBkLRTQPOGve6hgfAEQuB4Aa+NzUaDJ4hkhxk2MwO7hYY/qG2AKjneu3xB7K+qx6KxOVj04TH4gwImrtojV3Jm3NkH39Y2IcZMy6ao3ZLV5rLz3v4K731Vp3lNaQk28EIzgemRYkf1OR9mv3VEEQul1y5kaEqTPJxy+TFm+R6UFQ/Aw+sP6OrVpNxSrcGC3286iLn33QgTTcHHhrCkMEdpIVGYA4YmWPr+ccwZ0Q/pidqDCzwvwGqhVYkHS4qcWL37JJZ9WiW3lrXW2DnOqkoU0Js+5cITs/cv34Ps9HjMuLO3apjhwUHdMWPLYdQ1BbCxNA9DMjrKRscSeYne2M6c98EU1TI0MZQccC6BoihNMjq3IBMdO1gVx3fsYEWCrX2bp1SpqWnwyVXUeJspfN/ESl17iUJ7q0EXShIYhlJNfeqB5wVU1XtwptGvIMBXi82E0W40YODKwyBpF4iWPrj0tC+Sji1amP9E+SFsjGj5Sahp8EEQgA42E067/AAEbNj7HR4c1B1T1+3HS2P6Y/kn6tbXsiIn9p0Ufcr2V9VjeP80vLrzGzw97AZFy2jq4B6ocyuHGBYX5qCDzQSHpdkkdf93DQqCVjKoG977Sumw7w0EZS1WisOCGXf2luOWJDJkoglefSAbj25oNqZdWuQEL/Aq8pBgN8PMUPh0xm0ABF292pIiJ7Z88b38O9G6h53jbPi2rglelsOe43UqPeDo3Gux+0Q9RmR31a22MTSFRn8Iaz/7Tj6eoSms+0wkaIA4Yds9KUYVHbWkyAlCBNW59RIOmIgMVQlWhoQzRG0QIKYtvPJANkI8B0EQ5MqpdA4t8sIJkE2BI58bHeEktfei39vxNjMcJhoWhkIorNFLuQhtp0gCJn3pkb7wSOduL1G4GqpBeqj3sPiu3quSLWhV+q7UlGV79YAGDBhoHwySdoHQ00kA0NS+9Exx4Fhdk24yAK8zhEAIQFP4/+2deXhU5dn/P8+ZNSsJkLCmLIpARBAiGNBXUaxLxfKzbCqgoAJqra11fWvpxttWxa2uoFVwQYSCvlbr1qJoX8UNqRuKCKgEkQRIIMtk1uf3x1mYNSRsCXh/rmuuZM4855znnDMz5557+d50aufjgofeYdboUsdIKcj2pA25FuZ4OOOYrvxX30743AbnWcnyN5zVn4rq3XIORbl+zksSlr1i4QcsvPR4IrEY5z/0Tspxv3bNSfz5xc9TvCznD+vB3cvXMXvMAH7QIZv6YIQnpx9PNKaJxiDHZ3Dby19wQfkPEr7sFRqt4f7XvmRsWQntPV46t/Pzx3+s4ZU1lY7heHppMa+sqXQ8LR1yzHGrv97OhSN6cUF5z4yeOq/boH+XPEKRGH5PYjeFuZPL8LgUs0aX8uhbG/n9mAEpIb0HJg3B5zGIhbSpJRfXmss20Gzm/fsrpozoyVMzyonGzNZRCs2Im1fwzBXDE7a9bNWmFG07O3fJMBSPXTyMrbsaaQzHmLbgfSaWdSenf6eUjgSd891OCL0p46Wp6tTmvLej0Rhrq+pT9t+vOBePZ++NneYYYPual3QgvEH7y2AKRaIZvxfiPX2tVWW5r/mAgiDsOypTr71DleOOO06///77B32/VbVBzr3/zbTeignzVrLw0uMTKvXs1xfPKOebHQ0pxkF9KErXAj8Giqq6IAVZHn655ENWb6rhtWtOdqo647e1aHo5hoItViL2l5V1jmfmsbc2clLfTtyw7CNuHz+IiQ++nXIMSy8bzs5AmEse3X3+po7owWUnH0FdMEJVbTBhnndNPJbuhVkEIzEMpagPhtmyM0hhjoff/91MXH/mihFU1gbTJq7fe/5gcnxudtSH6JDrS+g1ao+JP2/x+XrT/6s3oBg/byV/mzmcSCyWkjPVs2MOm6sDFOX52BkIk+dzE9Wa73Y28tjKr7jxrP7sqA/RPsdLIBzFbYDH5cJlKMJRzYOvr+esY7o4XkzbSJ4/dWhKlWv3wqwUSY1eHbOZMO9tRvTuwPSTelFR3egYqb2Lcsy+oFrjdRsU53jZGYwSikRRSjFh3krnOr1x/SlcEGdUx793/F6DxlCsSWMh03uzuTfbzdUN/P65T1O6Vfz2nKPpVpgN7L3hcjA8RPtzHy01mJrad1VtkE8270z7Xoq/Nvt6/faWzdUNnHDLaynL37zhFOe6C4Kw7yilVmmtj0v3mnjSWkhLldHDlqxBTUMoxXty/6Qh1DSYLYRsD5WhFBp45P82pEg62GKouxrDabfldSnWbKmlY66XytqgkwifbOxkCrdd9dRqvo0rDuha4GfBW19z8lHFzHr2kwStsoZQFK3N8Fs649MWmq2sDVKc50urc1aQ7eXqxaaEhq0Ll3z+bMHXaEwTsTxUvx5ditaa+15dD8CfXviM3/24NMFT1zHXx2+f/cTxyt0ydiBPrPyaUaWdKMjyMLasBLdLMW7uSroXZrFg2lA8LoOq2qCj43bRiF5ke10pXsuY1qmhzUlDiGmdIKnxm9FHM2t0Kcd0y+d3fzeNnGxchKIx/viPNcw46QjGzV3J6aXF/Py0oxwJDVsPzb5OsQyeMLMtlNrjDXNfvUkqQ8cFq15inzw9ByMxfF/3Ef+ZV0px5z/XNqsQYU/npUOOlx4dslM8uMnXprWqLNtyqFgQvi+IkdYC9koZ3WU2p87ze7j5xc8SDJV7X13HhcN7JhQhzJtSxuzn1zhhuPjx89/cyFWj+uB2GdQ2RlgwbSgupfhuVyP3vrqOX48uZdazn3DzT45JqVSMl+NIzvHqnO9LmPeJR3bkq+31zjI7JFNRHUhoffTatSfjdadKhFRUm1WNcyeXcffyL7jp7NK08hZZHoMbz+pHTSDMlppAxvNXVRtMSPqfM24g2V4XPz31SKc11O/+voY7JgxiV2OEboVZ/OG5T51uDBXVAd5Yu5ULR/RMkEeYN7mM5648gbpghGAkxtT57yUYXY3hGO2sRPbkQpGnLx/OwkuPd4y6e15dx7QTeiU0gHe54Nju7QhGYryyptKZj81NZ5cCZrVnfAN0uyrXvk6uDBp3Sqlm3aj3NWSodfqOC4stMdvW1tM6kKT7zCcLD2cymPZ0XgxD0bNDDgXZHhbPKCeqwe8xUqo7W8tYksIBQWh9xEhrAc1RRr/zn2sd1flO+X5QmitP7cOO+lDaG/WMk45IeG4nwXdt50/rvehTnENlbYhr41r02DeNcERTlOuja0FWiuEUXw26elMNz67ezIJpw/j1Mx/z9sYdzrgBXfP52alHJoRDbY9OUa7PaVHUEIri9xgZc+o65fupbghx3Rn9cBkqrc7Z4jgl+8ElBWk1wKIxnSJXcd3Sj5g9ZgDtsrxce0ZfbnvZNIy+2t6A32NgNKiE8zy4pICJw3ok5NdUVJsN3meNLsXrMrhuaaKY8OULP2D2mAEUZLuZN7mMvyz/wrmu7XO8VNWGHE+ajd3sfebjqxwjxu02cBnpz5E7Ljk+/rV4I/q2l9cyZ/zA1JZVk8t4dc0Wzh7Uvek3rcWevElNheU06eVF7ESJ5nh6WrO90L6Q7jMf31EDMhtMzTkvhqFon+ODnMxzaC1jSXTKBKH1ESOtBTT1pWsYij5FuQlhKzsMee+r6xhbVpL2Rm33m7THF+X56F6Yhd/jSqul9dSM8pTWPTcsM40Wn1tx67iBfL09tU+mz+1ywipFuT5O6VfEaXe87rxe1qOQIzrmsGRVRUo41O5N2RCKJnjD5k4uo1uhL62GFmh+++yn3HhWPzoavrTnLRrT3D5+ENf8zcy1e2PtVhZNLycSjRHVoHWMaIZQX7bXvCnax94x10dhtpufLTJbPMXP/7KRRzi9MJO3Y1dHZtrHtAXv8/Tlw63G5okSHkW5iceVvL1oTFshsvTtgpQy3wPF1jW3t2XroS2ZORytNUopvq3e6RQlKKV4dc0WhvbuuF9u1HsKy/kyeHJ8lmGyJ09Pa7cX2hcyfebt896UwbS/PGCtaSyJTpkgtC5ST90C7C/deOK/dHcEQglhq4pqs2pybFmJ4x2x17dv1HNXrOfmnxzDa9eezOwxA7jv1S+ZM24gDaH0N4dkzS17+RFFOdQEIuwMhLl7+bqEfZ1eWkxxvs/JfetWmMXlC1cnbGPrrkbOOqYLgFN9aK9fVRekfY4vxRt22ROrqA/GKMrzMXvMABbPKGf2mAG0z/Fyz/IvqaoLOrpt6c6bnWO28NLj+ff1Ixk9qBvnP/Q2p9z+OlPnv8uuxgh+t8H8qUPNisopZQwuKaB7odnT01b879cll4JsD40RzVWj+rB8zdaE4++Q43VCiMlzaJ/jJRyNpX2tJhCmojpAOJqqZXf5wg+4alSftOvY/9s6X+GodkLXi2eUO6FsUDxzxQl0bWfe6OPfG1f/sC+d8/10K8ymc76fovxs/vDcp3z+XS2Vuxo5tX9n+hbvNnJiMU1VbZDN1Q1U1QaJtaBlUyYP8fb6kHP+kucXb5js6fU9bb8tk+kz37UgizdvOIVnrjiBvp3yAFLO/57OS0uwjaVuhdlOqFQQhMMf8aS1gD2FHRrDmX91222PFl56PDFt6qZ9t7ORdZV1hKOaKQ+/64QTc31uCrK9aX+FZ5KbiMQ0l1nhu6q6oCNZ0bWdHw1sqKpna21jQk5Z8jx7F+3uJfn655UJDdHrQ+n7K8a05sHXN3DWMV0ozvexdVeQG5d9TFVd0PEiXndG34SG57aBWh+MUJzv44//WMPYspKECtCK6gDzXl/Plaf2SWlKn+114TYU39aYwrw76sMJ3ss54wbyzAebmT1mAL065uB1qbS6cvdPGsKclz/nmtNTRWPt3pfdC03R3HTH3rNjToIX1C7ssP/3WMnhgXAkbbugbK9BQbbppWjKU7In5ft99VTtKSy3J0/Onl5v7fZC+0Kmz3znfH+zzr+ECwXh0KStpGiIBEcLaerCbakJMH7eyhQD6vFLhjHl4XcZ0bsDU4b3cMKYp5cW898/6o+hFJt2NJDrdzuio6eXFifc2G1V+0g0RmM4llBZeP+kIXTI9fLJ5l0sX7OVMYO7pchFRKJmmyGb4jwflbXBhHk+NaOczdVmVWG+353gOWtKBuKr7Q1OXpi9jXA0RlGel12BKFleF3cn5XQtfvdryo8oolfHHNZuraVv5zxGzlkB4Oi5HVWcm1ZqZPGMcupDEZa9v4lJ5T3T9g59akY5XreZhF1Z28jGbfXMf3NjwhxqG8OMue8tx6hqDMfo2TGHrbsaueXFz6mqC3LfBYMpzPFywUOp+3js4mGsq6zjqE55KExPSkxbLbdyPHTJ99OpXRbbahsJx2JEohDVmlhME4pGKczy0qld89Xpm5JyOPf+N1NyBgeVtDPznfbAgZZ4aC0Jif3Fnr6sD/XjEwQhkYOdoiESHPuRpnI0DEXa/Kxsj4slM8sJR3fLVdj9Jqc8/K4z9s4Jg5gzbiCGUrgMRVGeh0XTy3EZUF0fdjxOA7rmW0ntHtplmVWj8VITz67e7Ai/5vsTWxD53AavXnsy2+tCCRIet48fxM5AmIkPvu1UPi6ZWc5d/1zHklUVKDI3GY9PpA5HtbONhy86ji4FfkdU107mtw2c2sYw1Q1Bp1+nXZxw7Rl9HT23dB6YcFRz28tr+e05R2fMWftuZyM5Pjcdc3yEojFufWmtY8Bsrw/xx398xo1n9XPGG0o5GmdLZg7nL+cPZutOU1D2f55fw10Tj+UXi//jHPs95w9m6y7TM/nEJcN4bOVXjo5YKBrj/te+ZNbooy2DTDNhXqqBm6z4n4k9fWGEItGE82aPmTe5jIKsPf/6O9CJ6Yd6leCe8rIOZU+hIAiptKWKdTHS9iONkViC5llNwNRAu+ns/nRp50+QwUjXb/LqJR8mCKI+MLmMeywP1LJVm7jilCOprg8TjMTwewzaZXnYUR9ibFkJVbVmfk8oEmPGyb3ZtCPAvz7bytzXNzjz+9WP+vHYyq+5cuHqFF0xv8dgl5VPVVFtVj4+dvEwpp/Um+kn9UKTucm4nTDfvTCLHI/B/11/ClGtqa4PZcytA1j49jdcfsoRPHrxMFyGYt6UMip3BZ3zkknPLaY1VbUhIrHUlkv2mO31IX6x+D88c8UJ+D0uquqCCaHe5Pyxmrhjj8Q0PpdiR0OI2c+voSjXh9ulEjsmKCjM9jBvShldC7K48az+CcbynHED+W5nIz6P0WzF/0zs6QvD63Zx1ag+Ke+nmU+salEj8QMVljvcqwRFT0wQDi/a0g8vMdL2knQhELeh0hoDHXK8RGKmsrz9ZZ6pybZdtVhRHeCe5V9w3Rn9cBuKaSf0IhCKOqKy15/Zl5tf/IyxZSV0zvdz/+QhbK8LJVR+xvPIRcfxm79/SkV1gFmjS/lpml6OCy89nnlTypi7Yj2rN9Wwoz5EbWPEfG3lRn426qiEvK37Jw0hYEl7NISi3HfBYLYmaZo9fsmwtDewb2sCnDukW4InccG0ofQuynHGpu3ZOWkIi9/9muvP7Mu3NQHmv7kxJZ/svguG8DvrWEORKMW5vpQx8fljdv6ZPbf1lXUU5/vonO93zle63peLppcz+/nE/f7s1D58u7ORW19ay41n9aMhGCXbd2CbhHfI8dKrY84+fakc6Cq+w7lK8FD3FAqCkEhb+uElRtpekCn81LXAmyBwumzVJqad0ItfLvmQh6eWEYrGnHBoJi9RcZ6PRdOPx+8xcBsG0xa8x6zRpfTtlMfkh99xjIb5b25MaGw+Z/wgGsNRfnRMFx58Y7f37IqRR3DG0Z0oyN6txZXJQPxuZyOzn1/jtF7aXh+iIMtjermsXpVP2hIZMc1Db2xgwtASHpg0hLpghNrGSIqI7p9f+CylO8J9FwzBbagEnbGK6gBT57/HoxfvNursYovZYwZQ0j6L9VX13GPJmVy39CPumnisE0J9cno5W2pM75uhTBkL+0O1rSHEPcu/cLyA4WgMt2Fwx8RjMRT88R9rnPG2wWbn13UvzMp4vuJlPSqqA/z0yQ+csK9tuG7cVs+gknb7dBP3xBn38e8Vj9Xs2jDUPhuCwt5zuHsKBeH7Rlv64SVG2l6QLvx05z/X8vNRR6Uk9D+x8mtWb6ohEIoxbf57Tnukru38KcbLA5PK2NUYxm0oahoiTlXj3BXrueu8YxOMrLFlJY6Bdu0ZfbnokXfYHNfSCUy1+/OGlQAkqNZnMhBtyYkbln3EYxcP4+YXP+P8YT2IxjSDSwoY0rMDkWiMdZV1zF2x3vQajjyCa62eonY7o3heWVPJrNGlCZWi9722juvP7MfNPzkGj8tw2kSt3lRDJBpztNMqqgNU1QXxug2u+9tHjsL7JSf2Nr2R2R5nHzee1d/JhZs1ujThQ7WpuiGtkPCKa0fidxv89pyjueTE3tQEwtz28lpnP564EGy68xVfiGG/D+yw75xxA8nyuvj939dw7wWD9+km7jZU2lxHd9z6HXN8beZL5fvI4ewpFITvG23ph5cYaS3ADnE2hCLMGl3qGBaDSwq47ox+TFtgthWaUNad6Sf1xmUorjqtDxef2NPRN6uoNlsrDS4p4Poz+zoCpVt2NrLi8638vyHdCUdjdG5nMKJ3B5asqmD1ppoE6Y2aQNhRqZ81upSfP7WabXW7NacKsjzcOfFY/B4Dj8sw2xvlGE7Ib+6K9Sk3/fiQX0V1gNrGCNNO6EWHXC8vfbwlJSndlsLYURdyvFDxwrw2pkaU4sJHdldHDi4poCqpt6jtvfN7XLz62SZmjS7lyKJcvtnRkGA42cZk98IsXFbzSFsmo3thFvOmlNExx+t0gTAMlTFvze1SRLXpqbKNwvjXDcOgf+d8urQLpfTqtFtexWPrZy2aXo5Cs6k6QFGed5+9WYFQNG2u470XDHaU6tvSl4ogCMKhTlv5K+IiiwAAIABJREFU4SUSHM0kUw+/Z1dvZszgbvjcBuPmrmRCWXcmD++R6CGbXEZxrpexc1c6lZ3JRs9dE48lP8vNxQveT/HELVlVwcr/PoWqWrMisyjXx+0TBnHhI++yvS5EIJyad2S3XFo8w6wq9bgU2V6DhlCMSEzjdxuEoqZe28Zt9dy9fF2CIfTUjHLchmL+/21gSM8OCRpm9pibf3IMP2ifTWVtkMZwlI55XqrrwwnG350TBtGtMIsRN7/mrGv3J03enu29u/Gs/igFtY0RDKUS9MVsY276f/UmEtNct/QjHphcRpd8HxqV1jCp3NXI+qq6FE9U14IsJv31HR67eBjBSKzJcuvkJtuPvrmBk/p2SqmmbJftZt1W83xW1QWZO7mMvsW5fLmtPu32gT1q8YjEgyAIwuGLSHDsBzL18Js/daiTN9a9MIvpJ/V2PGr2uMufWMXCS49n/rShVOwI0LNDdoL+V0V1gF8s/g+zxwxIWHbFwg9YNN1sYr2tNsQ/PtzM/KlDcVlCrukKBCBJLT+mGXnbCsf70yHXCzFNfSjK3BXrWVdZx7Vn9KWqLuis+8CkIYBme12Y84b1QCnlGJe2jEVNIExhjheXoQhHY9z49McU5fr41Y/6s2h6OTFtege9bgM0CZ6sTDleOwNhXllTya/OLmVLTYDzH3qH00uL+dvM4YSjMZRSaDQXn9ibwhwvOV4XS2YOpzjXh9uduXlG+2wvu6yuCHZ1ZodcL/e9+iUV1QEufORd/n7lCU16oeJ/VW2ubmDev7/i3a9qmDW6lOI8X1oplNteXstlT6xiyczhaaszn75iBNvrQnvU4knXF7Y4z0eh1YJKEARBODyRtlDNJFOFnddtOHljt4wdiMtQGeQWIBiOsejdrwG4ffwgp82RPcau7LQZ0bsDbpfimtOPoj4UZeqJvfB7XFz66PtMfvhdZ9xT08utsGJiu6nuhVlsswRrK6rNNk6fb6nl5DkruPCRd5k8vAd9inOd7gRvXDeSJTPKKcrzEY1Bu2wPf37xM9ZureX00mKuPaMvs59fw8QH32b282vQGrK8hhOK+8OYoynI9rCtLojLUHQr8PO7v68BBXMnlzlzbAhFnf9tuhdmUVkbdMKYt75khl5fWVNJJKZZ+PZXNIajRKJmu52/vfcNGkXXgqwmDbRYTFMdCJPrc3NUp1w6t/NzRHEuL3/8HUtWVQBQlOsjEIoSi5mGYCgSZXt9KGNrJa/bxemlxY7BmutzOwaafa5vWPYRl408wjSUo7G074nGcKxZ7ZLi+8LOfn4N4+au5IK/vsO6qroWtX+KPyd720JKEARBOHiIJ62ZZCrJDUfNXKjVm2p4dvVmfn5an4z5T4FQlEtO7M2fLemMDjlebp8wiLkr1lMTCFGc7+NfvzwJl1KEYzH8HheVuxoxlOK1z75j1NGdmDD3HWe75wzqwk0/6kddMMpt4wdRnOfj6zj1/7mTy6htDDO4pIDVm2pSJD6uWPgB86cO5Yd3vsGyVZu4/sx+bK4JpjRR97kVN51d6gjx2utf9sQqFs8o56az+9MYjhLV2vEi2uuOGdQZrSE/y83NPzmGzu38+N1GStGEHca8c8IgXAYJoVeXASf17ZSw7VvGDkTRtHFhh6jv/OdapxI2fn3bQL7+zL78/rlPU8ZkUpguzPI43SCKcn2mRtlZ/blweE9uf+UL51zbRQQeV/rqTJdK39g9nWxGdSCc0hd2+mPvs2Tm8IQWRXviUG52fijRVlrKCIJwaCNGWjNJLsk9vbSYX/2olHAs5vSovKD8B2yrC6atxPvZk6u5alQfFr37dYox8MjU4whFdIJm2JxxA+mQ6yUU0Vy9ZDU9O2Q7MhgAd04YxIDu7fC6DYJ1YYrzfPg9Br2LcrjzvGPZWFXPrP/9hKq6YIKshC3aCuaN3k6qv/Gs/ny9vcGpKLVfv+yJVcyfOtR5Hk9FdYBgJMa4uSudORfl+pwCicueWGWFazUel4HLUEyd/55z/h67eBhulyIU0QRCEc4f1oN22R6q63eLzM4ZNxCtySika5PupmiHqGeNLk0RerW7JHhdBtct/SjtmEwK09WBsGOg3XhWP6fooHuh2bnhZqulVEMoykMXHkdxbvrKyyxv82UzMnlyv60xw8TNNbLakpL24YoYwoIg7C/ESGsmdshp4aXHUxeM4DYUG7fVk+11oYHfnnM0X2ytc8RmZ40u5YiiHDbtCHDrS2Z1YrbXxbQTetEYNmUmbOmJzdWNKcbRdUs/4rbxg8jxuhyjB8y2TsFIjNv/+QVzxg0k3+emIRQlpjWhiCLX73HaMNncsOwjZo8ZgN9jOGFEwPHyzBpdSmM4Sra1r3jsXLHt9aG0BkXUCpXZc7Z1wuxl4WiMrbsa6VqQldAL9JU1lazZUsviGeWcdseKhG0umVHuNHq/9aW13DZ+YFpPmGFFOTPdFPP9bsejle64+nXOQymaHGN7teKNQDBDpL85pz81DZGEa3nN3z7k/klDaJflwVDg87gyVl4CzZLNiMW0U72afP7jOys0x8hqS0rahytiCAuCsL8QI60FVAfCTPrrO8wZNxDAMazsysTsOINq5uNmKHDagvcYXFLAvClldCvwUxOIcN3SxGrFTMbRwne+5rkPtzjLTjyyPVOG93K8SfPf3MhvzjmaLK+L5/5TwcRhPQhF0uc/9S7KobohlFggMLmMe5av460N23lqejlbM+iBba8PpVf/n1zGSx9vSdhPQVwyu51n1jnfnyD8Gj8+GImlLIsk5UjFNGk9YYtnmEUVmW6KS2YOT5DsSD4uj8twQpGZxnjdrrRG4H0XDMZlGMx69j8J1/LZ1ZsxlHJCw/FelHQ36ObIZmyvD/E//1iTtnfqbS+vbZGR1ZaUtA9XxBAWBGF/IYUDLcD+8u2c70/wClVUB6isDaYkxNcEwgkJ9zsawgltm2xjo2OuLyWRHkgw0N668RQuH9knIXH/ohG9cCm47IlVTBzWA7/HcDwu8Zh5XYocn5snp5fz2rUnM3/qUDrmevjpqUey8NLjCUaiFOZ4mDNuYEIRwgOTd7eJsgsMXr3mZGaNLuWe5V9wVJf8hP00hKLO/2aoUhPV2ikKSJ5XpVXYEL/s252NzjFee0bfjIn3UUs+JtNN0aVMT9WyVZu4ZWzicc2dXEZxrs8JY6cbY3u10hmBO+rTX8sZJx+RsjxdMYCNXTXarTCbojxf2nBYKBLllTWV3PbyWuZPHcrSy4Yza3Spox/XEiPLPt50xynsH2xDOB4xhAVB2BvESGsB9pdvNKlh9uCSAnp3zKakvaltZn9BL1u1iZvOLuXRtzZy18RjaZchpLarMcxdE49Na6jZ6AzepJje3aIoGoOl73+TMAfb0PrXp1v44R1vcNKtr3HKba8zbcF7fFSxC4XZFgmluP+1LwF4/OJh/OuXJ3Hb+EE0hqJOEv/qTTXMfn4N6yrrmPn4Kl5ZU+nc3O399OuSxzNXjGDW6FJufWktHpfBdzsbHQHd+HnNm1JGx1xvwrL7Jw0hx+ti3pQyinJ93LDsI/KtBPx47CrQqtogUa2ZP3WoUwhgv24YBn075fHHcwdyZFEOi2eUmxWsM4fTr1MebrfhhCLtMUtmDufNG07hmStOcHKI0hmBmbyfHlf66t598aLY77vVm2q4fulHBCMxZj+/u5VVS4ys+NBr8nEK+wcxhAVB2F9IuLMFdMjxMm9KGdvqQpxeWszYshK6FmTRPttDIBIjFI1RkO1h/tShhKIxvC4Dv1tx41n9LSMqfV5RRXWAbu18jhcKwOtSFFsNvgGiMZ3Rm2SHJDvm+ji1f+cEPTWvyyDLZ6B6dXT2FV9N+dtzjuaVNZVU1Ya49oy+CQUP86aU4YvrGxkfYrPn3i7L4+SP/eZ/P+HGs/ox8cG3ndc75Hr55eIPqaoLUpTn48lLjycc1fg9Bo+9tZHLTzmSJTOHE4nG0JgGo601dvv4QcS0xlDwwKQhCY3b/3LesQTCUSZa+Xe25+7Wl8wCCfum2BzV6D2NSRcitL2mKRWbGbob7IsXJb5oZfWmGh59ayNPXnq8eX33onKwrShpH65I9wdBEPYX0nGghWzdGaA2GEEDFTsCTuFA747ZeFyKhlCMqNYYSuF1K6p2hZhpVQL+7selNISiCYbQXROPZdE7X7Ns9bfOPjrl+5g3pYxgOOZUDv7vFSO4ctHqlJv/kpnlhCKanYEwOT439cEw3+0KUpDloTjPxy+XfMhlI4/gg6+2M3FYD6d/5rJVm7hq1FEUJXVCuGzkEXTI8dIp38+OejNE+dmWWnJ8bgqyTN0024i6c8IgIjHzWGsCYZat2sT5w3owbcF7jldMa01MkyL2eueEQRTmeOndMRfDUBlV9WePGcDdy9fxux+XsqM+7IjRlrTP4taXPk/ox9m9MIvFM8r3+00xXU5aui4F908awuff7uS4Xh0SzvPVP+y7z94qkXQQBEE4PGmq44AYaS3EFP+MsWFbPfPf3MjYshJKCrPomOOlsi6U0MJo3uQy2ud6WPpeBWce0wWPS1nJ6soUt43GuHjB+3xZWQfAgG759O+Ux2lHd6ZDjpf2OV627mrE4zIoKcyiKmn7d583GJcBP31y9e5k/klD+M2zn1JVF2T+1KFcv/QjivK8XDXqKO5e/oWjz1aU5yPLaxCJxtheF+Hyhbu3+8DkMmKxGPe99iW/OedovtvZ6LR92lITpCDbQ4dcL8GI2TQ+fr2OuR4CoRhet8Fb66oY3KM9XrfBhqrU1lN/mzmcLgVmSGhzdQMn3PJayvm2vXTp2kjFV5LavHH9KbgU+92QCYejVNYFicQ0bkNRnOvD5TLYXm+25VpfWceLH29hzOBuia2ippTRtzivScFdQRAE4fuLtIXaj7Tzuaisj7Li861ceWofR5B1/tShKTIaM59YxaLpx3NK/0Qh1vnThvLehu386n8/dbZ7y9hjOOmojmyvCycYYnPGDcTtUtz+yhdc/F89WTS9nK27GtleHyIYiaYUMFy+8ANmjxlAnt/NnJc/5/oz+5LtdVGQ5eams0upqg2yvT7Eg2+s52en9uHxlV9TmO12Gr2Ho5oHX1/PWxu2m4KxigQdNNvQSu6/WVFttr+aPWYA0xa8x+mlxfxs1FEpArR2sntFdYBY3A+ETFWHNYFwRnmM5Byf7oVZfFsT4LwH3+b00mJ+fXbpXocE44lEYqytrEu4LnMnl9HPqtjcXN3AtAXvMW9KWUre4MzHV4n0giAIgrBXyM/7FhCLaXYEwhgKpp7Yi+11Iae9U0F2ekMiEtMp1X5XP/WfBAMN4J5Xv6SuMZoy9rqlH1HXGGHJqgrueOULYlozbu5KZj6+CkOlT1Lv0SGbP/7DDC1et/QjahsjaExZCHvdV9ZUcvnCD7jqtD4M6dmBcDTGpL++w2l3vM6SVRVOYYJtR9lzuXWcqdSfyXCyOxqMLSvh8jTVj5eNPAJIzdNKl2w9Z9xAqxtDOG3hQPscb8p4rTWDSwq4aEQvLvjrO5xwy2uce/+brN1au9ftjyrrginX5bInVlFpyZnYBuaetNYEQRAEoSWIkdYCtteH0IDfY7C9LsysZz9xpCLaZahAjMVSlfo/+XZXyrYrqgMZ+37aWl7TTujlbBfIaLysq6xzwooV1QH8HlfmwoOYZvbza1BkqkqMJTw3lOK2CYPomJcqG2J7viBzE3W7VdJDUxKr3ZKrDp++YgSd8v1me6t0laGWNMis0aUsnlHuVJMaSnHZyCPSdg/IJIOxJzJJgESi5rmxDcxMPUlFekEQBEHYGyTc2QJCkSguQxEIxbh7+RcJbYoWv/s18yaX8Ze4vK/2OV687tRqvzy/m3y/m801jc6y7oVZGas/i/N8zB4zwKyOfPsrR9TUNl7iCxHunDCIP73wecL6RXk+3BmqDt2GYvaYAWyrSy9k+21N4vNvdjQ44cwHJpc53rL46kogozisfSwF2alvveSqw4452qmQy/K6ePqKEYQjMbxuFy4D3tqw3WmSbm+/qfBoKBKlqjbY4uT7TL033S7DmXffTnlmwcfkMmbGnRORXhAEQRD2FjHSWoCpPh9DKVLaFN17wWCK871O4217+R9+fHTCzb19joenZpSzsyHM1Ut293y8ZexAHnpjQ4qq/LzJZfi9pjHw4OsbGFXaiXy/mwXThuF1mS3G7z1/MIU5Xqpqg3hcKrGrwKQhKAW5fiPFqHpgchketyLb6+LBN9Zz+/hBCX0oH5g0hHteXZewrd88a4Zp7arK+VOHssPyMHbM9Tr7XrZqU8r+bhk7kF8u+ZDVm2pYetlw3C5Xk7laTUlFxGI6paXS3MllTnFEOqMqGtNOBaltQDWn6jLbm9oQ/v5JQ8j27nZEG4aifY6PgiyvSC8IgiAI+wWp7mwBkUiMrbWm92tiUn/M+VOHUtI+m6nz303x4tj846oTqdwV5MWPtzD+uBI6t/OjtU7QBzu9tJibzi5FYXqj7n11HT8/7Si8biOhkvL+SUN4YuXXjifpmStG8LNFq3n4ouPwe8zwmttQuAyFYUAwrPG6IBLDqVD0uBXrK80q1d+eczQajdYKrTXrq+p58eMtjCrtREGWh4ZQFL/H4PyH3kk4psUzyrnmbx8yb3IZXQp8BMOamNZ43S7a+VxU1YfYsrPRaS1lC7DOGl3KgK75dCvM3uvrkSxLUZjloToQJhaLsa0+xMzH4yptp5Txl399kSLZ0Zyk/s3VDfzlX+uYflJvXIYiGtM89MYGfn5an32avyAIgiBIded+YmcwRCSmUaTmmWV7XdQ2htMaaL84rQ9LV1Xw62c+4apRfbhs5BF8WxPgqkWrWb2phtNLi7nxrP5ccmJvagJhfvHUf6iqCzJrdKnTiHzhpcezYNow3C5FxKrAtA00O2l91uhSblz2MVV1QRbPKCcS03yzowFDqQQPmS1ke9GIXjz61kZ+ftpRADz+1leMO+4H+D0GRXk+J5xoG4X3Wl41m+6FWY6hk8lj1MXtMiU0Fq9J2f+QHwzcp+uRztNmPy/K8yd4tGKxWIKBBs1P6ve6XWlDq9e5+4l+mSAIgnDAECOtBTSGzFCnoVLzuxpCURpCUfKz3OwKRJzlXdv5GV/WnQnHdScUMb2WOwMhbnz6Y8douWrUUVxjhQHjsZuV2wn+p93xOoNLCrj2jL68tWE7sLuVUrzI7ANWiLQxFKMxHEuRBrGbk7sNxR/PHUiHHC87G4OMPa7EqdA0lOLJ6eVU14eobQzjUoqrRh3Fmi21Cd6pLu2ymjRKDEPRtziPJy89nkpL/uPRtzZy9Q/7HtBcrWQDrqo2fc5dc5L64xX/40OlhVmeFJHb5oZQBUEQBGFPiJHWAgwFeX4X4SjMnzbU6TjQEIpSlO9l9N1vJoy3E/ndhiIQjvGnF9Yw7YRelBRmsWh6OeFoDL/bYHNNo5PLFb+uXSkZX1QQ3+jc7gwQ1TFuOruUm84udYRWPR4XsSxT9yxdEj1Ap3a7KxFjMdheF3IMOrsQ4IiiXAqzPXz2XS3L12xNKJbo2Eyvkdtt0L0wmyyvmy7t/Az5wcCD7nHKZGg1x1DM1OYnXeP16Y+9L7pogiAIwn5BctJawM6GRnY1RvG5XQQjMYKRGN/tDPCnFz5jzZZaZ9zSy8opyPayrS5Eh1wvPrdBKGJ6wpJ547qRhKIxqmqDaas0q+qC3DJ2IM+u3sy5Q7oljInvDJCp9VBlbSM/uf+tFA9SsiFRUd3AeUl5dt0Ls3hqRjk+tytty6ZDzRjZ36HJTF0S3rzhFMlVE4QkJDVAENJzSOekKaXOBP4CuIC/aq1vbq25RDVooC4YYYeVEP+zRaud18eXdad9tpt2WV4MpeiQ42Xpe99w4YheZHnSS2AA3PrS5/xhzABH9d/jMnAbijsmHstX2+q57WWzafiMk3uxZEY5YSvxP6o1sDtkme4Lr2OOr1kepKZ01PbFC9WW2NvG4pluLpm6JIgumiAkkq7/raQGCMKeadOeNKWUC/gC+CFQAbwHnK+1XpNpnQPlSQuHo9Q0htmys9GRYojnf/7fAE7s05GRc1akrPvG9SPxuhTbklo+zZ1cxmNvfcWSVRWcXlqc0GbK9qaVtM+mLhihrjFCrt/N1LgKz+Z+yTXnF+y31Q1MSONJWzKjnK6F2d/bX8FN3VwAufEIQjOoqg0eFt54QTgQHMqetGHAl1rrDQBKqaeAMUBGI+1AUVkXRAOXP7EqQYQ2y+PirvOOpUOON6MYbSii+WxLLR98tZ35U4fidRuEIjEeemODUzE4tqwkwfirqA5w9ZIPWXjp8Uyd/x4PXXgcPdvn7JUGV3M8SNk+Fw9MGsLlcUbiA5OGkO1zNXsbhyN7yjtLl6smBpogJBKKRKVlmiDsBW3dSOsGbIp7XgEc3xoTiVh9H+MNNIBAOEqHHG9KN4D4vLEHX1/Puso6rj2jL9MWvMes0aUJzcnBTGxP9yWmIEHi4kAZSvl+LwXZERZMG4ahIKbB51bk+w+tkOb+Zk83l++r8SoILUFSAwRh72jrvTvTuSRS4rNKqRlKqfeVUu9XVVUdkIm4DYXHUHQr8Ccst9suZXld/PjY7vTumM3iGeW8cd1IFs8op0u+qTcWX5Vphzrje1EWZeiF6feaqvwH2jtjGIpuBdm0y/Lgcxu0y/LQrSD7e+8Vsm8u8cjNRRBahp3XGv+ddyjmtQrCwaat56QNB36ntT7Dev7fAFrrP2da50DmpIWiYTZuD6bklRXne3n4jQ2MPrY7HXM9LPi/jcz791d0L8ziwQvLqA1EEsRk77tgCDk+F36PC601LsOgKMfLl9vqJb+pjSEJz4Kwf/i+5rUKwp5oKietrRtpbszCgVHAZszCgQu01p9mWudASnDYhlpNIOa0VsrxGQTCMWIxyPEZRGNYzzV+jwu3SxGOxIhqiGmzW4GhQGPqmbndu52Z8iXWNpHrIgiCIBwoDtnCAa11RCl1JfAypgTHI00ZaAcaj8eFx+MiJzHiScF+2r7kN7VN5LoIgiAIrUGbNtIAtNYvAC+09jwEQRAEQRAOJm29cEAQBEEQBOF7iRhpgiAIgiAIbRAx0gRBEARBENogYqQJgiAIgiC0QcRIEwRBEARBaIOIkSYIgiAIgtAGESNNEARBEAShDSJGmiAIgiAIQhtEjDRBEARBEIQ2iBhpgiAIgiAIbRAx0gRBEARBENogYqQJgiAIgiC0QZTWurXnsF9RSlUBXx/g3XQEth3gfQgHDrl+hzZy/Q5t5Pod2sj12//00FoXpXvhsDPSDgZKqfe11se19jyEvUOu36GNXL9DG7l+hzZy/Q4uEu4UBEEQBEFog4iRJgiCIAiC0AYRI23veLC1JyDsE3L9Dm3k+h3ayPU7tJHrdxCRnDRBEARBEIQ2iHjSBEEQBEEQ2iBipAmCIAiCILRBxEhrIUqpM5VSa5VSXyqlbmzt+QipKKVKlFKvKaU+U0p9qpT6ubW8vVLqn0qpddbfQmu5UkrdbV3Tj5RSQ1r3CASllEsptVop9bz1vJdS6h3r2i1WSnmt5T7r+ZfW6z1bc94CKKUKlFJLlVKfW5/B4fLZO3RQSl1tfW9+opRapJTyy+ev9RAjrQUopVzAfcBZQClwvlKqtHVnJaQhAlyjte4PlAM/ta7TjcByrXUfYLn1HMzr2cd6zAAeOPhTFpL4OfBZ3PNbgDuta1cNXGItvwSo1lofCdxpjRNal78AL2mt+wGDMK+jfPYOAZRS3YCrgOO01gMAF3Ae8vlrNcRIaxnDgC+11hu01iHgKWBMK89JSEJrvUVr/YH1fy3mTaIb5rV61Br2KPD/rP/HAI9pk7eBAqVUl4M8bcFCKdUdOBv4q/VcAacCS60hydfOvqZLgVHWeKEVUErlAycBDwNorUNa6xrks3co4QaylFJuIBvYgnz+Wg0x0lpGN2BT3PMKa5nQRrHc74OBd4BOWustYBpyQLE1TK5r2+Iu4HogZj3vANRorSPW8/jr41w76/Wd1nihdegNVAHzrXD1X5VSOchn75BAa70ZuA34BtM42wmsQj5/rYYYaS0j3S8E0TBpoyilcoFlwC+01ruaGppmmVzXVkApNRqo1Fqvil+cZqhuxmvCwccNDAEe0FoPBurZHdpMh1y/NoSVKzgG6AV0BXIwQ9LJyOfvICFGWsuoAErinncHvm2luQhNoJTyYBpoC7XWT1uLt9qhFOtvpbVcrmvb4QTgx0qprzDTCU7F9KwVWOEXSLw+zrWzXm8H7DiYExYSqAAqtNbvWM+XYhpt8tk7NDgN2Ki1rtJah4GngRHI56/VECOtZbwH9LEqXbyYCZV/b+U5CUlYOREPA59pre+Ie+nvwEXW/xcBz8Ytv9CqNCsHdtqhGeHgorX+b611d611T8zP16ta60nAa8A4a1jytbOv6ThrvPySbyW01t8Bm5RSfa1Fo4A1yGfvUOEboFwplW19j9rXTz5/rYR0HGghSqkfYf6ydwGPaK3/2MpTEpJQSp0I/Bv4mN15Tb/CzEtbAvwA88tovNZ6h/VldC9wJtAATNNav3/QJy4koJQaCVyrtR6tlOqN6VlrD6wGJmutg0opP/A4Zt7hDuA8rfWG1pqzAEqpYzGLPrzABmAapkNAPnuHAEqp3wMTMavkVwOXYuaeyeevFRAjTRAEQRAEoQ0i4U5BEARBEIQ2iBhpgiAIgiAIbRAx0gRBEARBENogYqQJgiAIgiC0QcRIEwRBEARBaIOIkSYIhzmWBtVGpZRWSh25F+sXK6V+Z7XYOiBY29/WjHHnKKXeVErVKKV2KaU+VUrNtbpLHBSUUguUUgdVJkIpdaJS6p9KqSqlVL1Sap01j+5xY75SSt12kOaTrZT6Til1cgvXG6+UWquUch2ouQnC4YQYaYJw+DMc6Gn9f95erF8M/DZuG62CUup8TPHMj4HzgQmYzZ3/Cyg4iFOnMfcPAAAIw0lEQVSZDUw9WDuzdP9WYPZFvASzufU9QD+gR9zQc4G7D9K0foapTP96C9dbhtlKaMr+n5IgHH6ITpogHOYope7BFBT9BMjTWh/dwvUHYBpGp2itV+z/GZqeNOBKrXXHJsa8idno+ew0r6l9UTq32ojFtNbRvd3GgUIptRAYCAxMPsZ9Pe69nI8BfAXM1lo/tBfr/xo4V2tdtr/nJgiHG+JJE4TDGCusNB7TA/UIUKqUGphmXA+l1CKl1DalVINS6iOl1AVWiPNja9hrVshUW+tMtZ7nJm0rIeymlDrbCtVVWiHKt5VSp+/F4RQA36V7Id5QUUoZSqkblVJfKqWCSqkvlFIXxY9XSq1QSi1VSs1QSq0HGoELrOM5OmlsoVIqpJS6xHqeEO6MOw/HWMdZr5T6XCn1k6TtKKXU7Ljz8IhS6jxr3Z57OO7KdMZY0nE7510p1dO+VmkeI+PWGaOUel8p1WiFL2+1DNamOBVTgf7p+IXWeXrKOv5vlVI3KKVuU2Yf1niWAUMs418QhCYQI00QDm9OBTphtnRZCoQxQ4UOSqliYCUwFLgWOAez92kJsAWYZA39KWbodHgL59ALeA4zxDUWeAt4USl1Qgu38wFwvlLqSqVU1ybG3QP8GngQOBt4BnhEKTU6adwJwOXADZjH/Czm8U5IGneu9feZPczvSUxj+FxgHfBUfM4Y8AvM9mRzMfscBoBb97BNMI/7FKXULGW2x2oOW9h9rezHUkxjdBOAUmoCpqH1LvBj4PfADODPe9j2KOALrfX2pOULgB8CP7e2czpme6EEtNafAdXWdgRBaAqttTzkIY/D9IHpPasGvNbzfwAbsVIdrGV/BuqBLhm2MQDQwMik5VOt5blJy78CbsuwLQNwAy9j9r61l/8O2LaHYykB/mPtU2P2hbwD6Bw35kjMfq0XJa37GPBe3PMVmEZS56RxfwE+T1r2MvB83PMFwPtpzsPFccs6YPY+vMx67sI0nO5L2vYL1ro9mzjufODVuOP+FtPQO6oF5300ELXPC2Ze2NfA/KRxF1vnpUMT83kF+FuG98j4uGVZwDbgqzTbWAEsbO3Phzzk0dYf4kkThMMUpZQP06vzjNY6ZC1ehFkAUB439FTgJa31lgM0j+5KqUeVUpsxDZcwppflqJZsR2u9CSgDTgNux2zofDXwUZzHahSmkfaMUsptP4DlwLFJVYWrtNbJ4dPFQF+l1CBr7h0xz8/iZkzxlbi5bgcqAXteJUBnTE9bPMnPU9Ba77KOawTwJ2A9ZtPrD5RSQ/a0vlLqKOAJYK7W+lFr8VGYzc6XJJ2nVwE/ptGVic6Yxlc8x1l/n4ubdwD4V4ZtbLO2IwhCE7hbewKCIBwwzsLMZ3pBKWVXP64Agpghz5XWsg7AewdiAlaS+d+BPOA3wJeYXrs/YFaNtghtJvYvtx5YuW0vANdgGmwdMb1WOzNsogtQYf2/Nc3rK4FvMMN0H2KGZyPA/zZjejVJz0OYBg/sNkiqksYkP0+L1lpbc1sJoJQ6FngDmMXucGwKSqk8zLl/ihlutbELNF7IsGpJE9PxY76H4ukM1GqtG5OWZzq+ILvPjSAIGRAjTRAOX+zcs7+leW2CUupqy+jZjmm8tBT7huxNWl4Y9/+RwGDgLK31S/ZCpVTWXuwvBa31K0qpDzHlKMD0rkUw881iaVapjF89zfa0UmoJppH2K+vvi1rr2n2cqu2xK0panvy8WWit/6OU+idQmmmMUkphSpQUAqO01uG4l3dYf2cAq9OsvrGJ3e8gVfLkOyBPKeVPMtQyHV9B3BwEQciAhDsF4TDEqrgcjRnePCXp8UvMYoJTrOHLgTOUUp0ybM4OlSZ7PmyPVP+4/R6PmUNlYxtjwbgxPTCNqBZhFTgkL/NjhhRtr9irmJ60dlrr99M8QsnbSMNTQG+r0OBk6/m+sgnTkBmTtPzHe1oxw3Er4AjSewNtfo35HhifJpS9FtiMmQuX7jwlFwUkr9sraZld7eocj2WI/zDDNnoCXzSxD0EQEE+aIByujAGygb9ord+Jf0GZemM3YXra/gXcCVwI/Fsp9UdMg6I/kKO1vhUz/BcALlJK7QTCWuv3MasCNwN3K6VmAe2B64Fdcbv7HNOYu90ak4dZRbh5L47pZaXU55h5T5swQ2xXYnqK5gFordcqpeZiVlbeimk8+IGjMRPtL93TTrTWq5RSX2JWhwaA5/dirsnbjCql5gBzlFJVwJuYBs0x1pB0Xj+bv1ph42WY+WiFmLp3gzDlVVJQpgDu74H5QEQpFZ+DuEZrvUspdQ3wuFIqH3gR0xjvjSmWO05r3ZBhPm8C5yqlDK11zDq+T5RSzwEPWCHW7zB/DDQkH5tSKgfT8zmriWMWBAHxpAnC4cr5wLpkAw3ACnstAX6ilPJpraswPVurgbswjZIZmMYZVvhqOmbS/utY+WuWV+pczJvwUsy8sMsxq0ntfQWBn2CGIJdiqvX/2dpOS7kV0/C8hd3G5U7gRK31yrhxP7X2cyFmztUCTCmON1qwr8WYIeDnmjBWWsqdmIn/V2AaXIXWc0g0bJO5H6jDzOl7BdMgzQPO0FovzbDOkZgVnBezO5fNfgwB0FovxjTmj8UMiT9tze0DdntP0/Espoc02Rs6FfO63I1ZVfw68FKaYzsd03h7uYl9CIKAdBwQBEFoNZRSfwV+qLXuscfBbQil1LNAhdb6p02McWN2uXhHa31R3PJFQH1zvJqC8H1Hwp2CIAgHAUthfyKmmG8Ms/p2GqaY7qHG/wDLlVK/1lpXg9k8HeiK2aEiH9P72gfTo4k1pgTTe5fS9UIQhFTESBMEQTg41AMnYubR5WCKyd6Aqfl2SKG1fk8pdT2m1pod3q7HNDqPxCze+Bg4R2v9btyq3TEFfr88mPMVhEMVCXcKgiAIgiC0QaRwQBAEQRAEoQ0iRpogCIIgCEIbRIw0QRAEQRCENogYaYIgCIIgCG0QMdIEQRAEQRDaIP8fwOxek3mY6A8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trasnforming y back to actual sizes\n",
    "y_test_gram = np.exp(y_test)\n",
    "pred_test_gram = np.exp(nn_test_preds.reshape(-1,))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# showing a scatter plot of the predicted vs actual serving sizes\n",
    "sns.scatterplot(y_test_gram, pred_test_gram)\n",
    "\n",
    "# setting the visuals\n",
    "plt.title(\"Underprediction of Serving Sizes\", size=20)\n",
    "plt.xlabel(\"Actual Serving Size (g)\", size=15)\n",
    "# plt.xticks(list(range(0, 600_001, 75_000)))\n",
    "plt.ylabel(\"Predicted Serving Size (g)\", size=15)\n",
    "# plt.yticks(list(range(0, 600_001, 75_000)))\n",
    "plt.plot(y_test_gram, y_test_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this comparison of predicted and actual serving sizes that our model has a clear tendency to underpredict servings as the real serving size increases. There could be several reasons behind why this is happening, though one clear and logical answer may be that some of those foods with larger serving sizes have larger portions of the food that are inedible. This could mean a food that has shells or pits, or something else that is counted in the total weight, but is not eaten. Unfortunately, these aspects were not accounted for in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing a neural network gave use the best results overall. It was able to increase the R<sup>2</sup> score above 90%, and still limit variance to less than 2%. It was also able to minimize MSE fra better than the other models that were made. However, this model was still not perfect, and was subject to significant underpredcition as the true serving size increased. \n",
    "\n",
    "As discussed above, this may be influence by factors unseen by the model, such as the percentage of the food item that is not typically eaten. In addition, this model only takes certain attributes into account, and there are many other factors that can have an impact on serving size, such as package size, sale price, or targeted demographic. Many of these factors can be proprietary information to the manufacturing company, and are unavailable to the public, making a deeper analysis difficult. This could, however, be adapted within a specific company to account for some of these features, or to be trained on a more specific set of foods products.\n",
    "\n",
    "In the end, this model should still be fairly useful to a project team that is working to develop a new food product. If the standard nutrition facts and category can be enterered, the resulting serving size should be fairly close to similar products on the market in that category, and can be useful in the development process. This way, researchers have a good idea of what their final servings size might be, and can have a head start on things like branding and marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main recommendations for this project would be to conduct further analysis of the vectorized ingredients. Utilizing the ingredients as features did not appear to help our models in the current scope, though with more time to optimize, they may be able to improve the model further. A more complex method of cleaning and vectorizing this feature may allow it to be more effectively be incorporated into the model.\n",
    "\n",
    "Another interesting step to take would be to perform PCA on the nutrient features. This would be able to definitively eliminate any possibility of multicolinearity in the X variables. It would also ensure that only the most valuable elements of each of the features were being effectively used to make the prediction.\n",
    "\n",
    "Further utilization of the neural network grid search function. The project timeline and available computing power were severe limits on fully fleshing out the neural network. Of course the possibilities are truly endless, but even within the scope of layers, nodes, and regularization that were searched through here, there is most likely further optimization that could be done.\n",
    "\n",
    "Another modeling option could be to use a support vector machine (SVM), as this may be a more fitting model for this type of data. Unfortunately, due to the large amount of data and features being used, there was once again a computing power issue preventing this type of model from being run.\n",
    "\n",
    "Overall, the computational issues in this project could possibly be overcome by utilizing cloud computing services, such as those available on Amazon Web Services, or perhaps even just using a more powerful home computer.\n",
    "\n",
    "The last recommendation would be to continually maintain the dataset, as the USDA periodically updates the information store in it. This could entail downloading the zip file when a new version comes out, or it could be linked to the API access to continually update."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
